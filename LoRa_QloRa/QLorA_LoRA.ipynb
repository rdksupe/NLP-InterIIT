{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyO63ghE1vIiBKaDIPhfLZ3a",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "c0f421016e734a54999882ef518f1583": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_25f1664868a34e31b75b7d7b24509782",
              "IPY_MODEL_a1dae23e801241a6ae06dc7aebfe52c3",
              "IPY_MODEL_eb663142b8ad4e599611f4be02bfda5c"
            ],
            "layout": "IPY_MODEL_e873388afb5b468f981dc624c82f0f28"
          }
        },
        "25f1664868a34e31b75b7d7b24509782": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1c6a91ef54824fc2a49797b1833acf40",
            "placeholder": "​",
            "style": "IPY_MODEL_357810ff8d8348d89c7aae875e3725e6",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "a1dae23e801241a6ae06dc7aebfe52c3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ac162ed7a53f401aa740e1445af230a3",
            "max": 3,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_16e2d63a49c842c7b458e1c33060e746",
            "value": 3
          }
        },
        "eb663142b8ad4e599611f4be02bfda5c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_22f2f234e53f4eafa29db55cbd97f767",
            "placeholder": "​",
            "style": "IPY_MODEL_597894e9c87d4ed5bac138666e96dc5a",
            "value": " 3/3 [00:58&lt;00:00, 16.13s/it]"
          }
        },
        "e873388afb5b468f981dc624c82f0f28": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1c6a91ef54824fc2a49797b1833acf40": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "357810ff8d8348d89c7aae875e3725e6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ac162ed7a53f401aa740e1445af230a3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "16e2d63a49c842c7b458e1c33060e746": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "22f2f234e53f4eafa29db55cbd97f767": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "597894e9c87d4ed5bac138666e96dc5a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d4975ca20cc647d9812c38d76116231a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_456c0bd7ac1e435bb2b1f7c4f6183828",
              "IPY_MODEL_6a7fbed796154a0ba378c39530e6998e",
              "IPY_MODEL_06d4a5a14a4a49e891dad9c0991db152"
            ],
            "layout": "IPY_MODEL_087a2a70430d4daeacee1e66924c9140"
          }
        },
        "456c0bd7ac1e435bb2b1f7c4f6183828": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d05962b3f65f429ca01882e37cd80841",
            "placeholder": "​",
            "style": "IPY_MODEL_c232382bb8b64ba9af4bd099e6a0c77b",
            "value": "Map: 100%"
          }
        },
        "6a7fbed796154a0ba378c39530e6998e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_64e7ee281b5b4fcb9b5df50ebaa57a0c",
            "max": 15011,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_301caf4995b040d483785b381644e783",
            "value": 15011
          }
        },
        "06d4a5a14a4a49e891dad9c0991db152": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_015fbb3df4fd4a87a6120a11efb10691",
            "placeholder": "​",
            "style": "IPY_MODEL_457cd9c49fab4f8daaee69ed935d54d3",
            "value": " 15011/15011 [00:08&lt;00:00, 1680.63 examples/s]"
          }
        },
        "087a2a70430d4daeacee1e66924c9140": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d05962b3f65f429ca01882e37cd80841": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c232382bb8b64ba9af4bd099e6a0c77b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "64e7ee281b5b4fcb9b5df50ebaa57a0c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "301caf4995b040d483785b381644e783": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "015fbb3df4fd4a87a6120a11efb10691": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "457cd9c49fab4f8daaee69ed935d54d3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f3931d2c6ba34cdaa88f448bcfe797f2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_44e6598a3fab45ed9c67942e7239111b",
              "IPY_MODEL_2d7ec7f76e264967a6e074ca0e451137",
              "IPY_MODEL_16afed569b174eea94cd9f40272267d6"
            ],
            "layout": "IPY_MODEL_074caf5acec642d48077c028b5e61403"
          }
        },
        "44e6598a3fab45ed9c67942e7239111b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e8bd76782bbf4eb48e15c0f552c0a74a",
            "placeholder": "​",
            "style": "IPY_MODEL_274501b3b9fc415d98a1a7453c898f50",
            "value": "Filter: 100%"
          }
        },
        "2d7ec7f76e264967a6e074ca0e451137": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a93dcf9c3f364279b9543ee33cad4d0b",
            "max": 15011,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2d76ebe0f9cf4454a4e26f903f1f959f",
            "value": 15011
          }
        },
        "16afed569b174eea94cd9f40272267d6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bdd8df0f51344bf3a4dd858e73fc0f1a",
            "placeholder": "​",
            "style": "IPY_MODEL_c266b8c78b324902aa1d1188da1f20d8",
            "value": " 15011/15011 [00:03&lt;00:00, 4886.79 examples/s]"
          }
        },
        "074caf5acec642d48077c028b5e61403": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e8bd76782bbf4eb48e15c0f552c0a74a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "274501b3b9fc415d98a1a7453c898f50": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a93dcf9c3f364279b9543ee33cad4d0b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2d76ebe0f9cf4454a4e26f903f1f959f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "bdd8df0f51344bf3a4dd858e73fc0f1a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c266b8c78b324902aa1d1188da1f20d8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rdksupe/NLP-InterIIT/blob/main/QLorA_LoRA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Qlora based finetuning on dolly dataset on Google Gemma-2b model .\n",
        "\n",
        "Adapted from https://github.com/ovh/ai-training-examples/blob/main/notebooks/natural-language-processing/llm/miniconda/llama2-fine-tuning/llama_2_finetuning.ipynb"
      ],
      "metadata": {
        "id": "kSBD55m9BAU-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "from functools import partial\n",
        "\n",
        "# Login to Hugging Face\n",
        "login(\"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\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YDAPeKQyBA3a",
        "outputId": "2b57b282-e448-4b09-ce55-3ed5f9434abe"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
            "Token is valid (permission: fineGrained).\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "cZIbFal3AuTd"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments,DataCollatorForLanguageModeling\n",
        "import bitsandbytes as bnb\n",
        "from datasets import load_dataset\n",
        "import torch\n",
        "from peft import get_peft_model, LoraConfig, TaskType\n",
        "import random\n",
        "import pandas as pd\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "b2LiSl1tGt8E"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install peft"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8tFfkyCEGhhC",
        "outputId": "0671ed69-de5d-4ec6-81b3-a12cee8f8775"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: peft in /usr/local/lib/python3.10/dist-packages (0.12.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from peft) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from peft) (24.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from peft) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from peft) (6.0.2)\n",
            "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.10/dist-packages (from peft) (2.4.0+cu121)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (from peft) (4.42.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from peft) (4.66.5)\n",
            "Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from peft) (0.32.1)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from peft) (0.4.4)\n",
            "Requirement already satisfied: huggingface-hub>=0.17.0 in /usr/local/lib/python3.10/dist-packages (from peft) (0.23.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.17.0->peft) (3.15.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.17.0->peft) (2024.6.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.17.0->peft) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.17.0->peft) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (1.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (3.1.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers->peft) (2024.5.15)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers->peft) (0.19.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.13.0->peft) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.17.0->peft) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.17.0->peft) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.17.0->peft) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.17.0->peft) (2024.7.4)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.13.0->peft) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_dataset(\"databricks/databricks-dolly-15k\", split=\"train\")"
      ],
      "metadata": {
        "id": "wBmrFHSDA64i"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Generate random indices\n",
        "nb_samples = 3\n",
        "random_indices = random.sample(range(len(dataset)), nb_samples)\n",
        "samples = []\n",
        "\n",
        "for idx in random_indices:\n",
        "    sample = dataset[idx]\n",
        "\n",
        "    sample_data = {\n",
        "        'instruction': sample['instruction'],\n",
        "        'context': sample['context'],\n",
        "        'response': sample['response'],\n",
        "        'category': sample['category']\n",
        "    }\n",
        "    samples.append(sample_data)\n",
        "\n",
        "# Create a DataFrame and display it\n",
        "df = pd.DataFrame(samples)\n",
        "display(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "id": "kCkLgLRVJrYj",
        "outputId": "59985b5c-b7fb-4292-a8f2-df4055717f02"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "                                         instruction  \\\n",
              "0  Extract the causes of income inequality in the...   \n",
              "1                   Do you really need mobile phone?   \n",
              "2  Q: Am I eligible for a booster dose of a COVID...   \n",
              "\n",
              "                                             context  \\\n",
              "0  According to CBO (and others), the precise rea...   \n",
              "1                                                      \n",
              "2                                                      \n",
              "\n",
              "                                            response                category  \n",
              "0  The text lists the following as causes of inco...  information_extraction  \n",
              "1      We do not really need a mobile phone to live.           brainstorming  \n",
              "2  A:  Individuals may receive a single booster d...              general_qa  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-f2275d8d-2ba8-4650-9eff-b1bc3c53186b\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>instruction</th>\n",
              "      <th>context</th>\n",
              "      <th>response</th>\n",
              "      <th>category</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Extract the causes of income inequality in the...</td>\n",
              "      <td>According to CBO (and others), the precise rea...</td>\n",
              "      <td>The text lists the following as causes of inco...</td>\n",
              "      <td>information_extraction</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Do you really need mobile phone?</td>\n",
              "      <td></td>\n",
              "      <td>We do not really need a mobile phone to live.</td>\n",
              "      <td>brainstorming</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Q: Am I eligible for a booster dose of a COVID...</td>\n",
              "      <td></td>\n",
              "      <td>A:  Individuals may receive a single booster d...</td>\n",
              "      <td>general_qa</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f2275d8d-2ba8-4650-9eff-b1bc3c53186b')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-f2275d8d-2ba8-4650-9eff-b1bc3c53186b button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-f2275d8d-2ba8-4650-9eff-b1bc3c53186b');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-b6d643eb-808b-45d1-af0f-6584deb66e38\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-b6d643eb-808b-45d1-af0f-6584deb66e38')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-b6d643eb-808b-45d1-af0f-6584deb66e38 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_4cf51806-5d42-4a66-9f47-a00ed91b051a\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_4cf51806-5d42-4a66-9f47-a00ed91b051a button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 3,\n  \"fields\": [\n    {\n      \"column\": \"instruction\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"Extract the causes of income inequality in the United States mentioned in the text. Separate them with a comma.\",\n          \"Do you really need mobile phone?\",\n          \"Q: Am I eligible for a booster dose of a COVID-19 vaccine, and if so, which one?\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"context\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"\",\n          \"According to CBO (and others), the precise reasons for the  rapid growth in income at the top are not well understood\\\",:\\u200axi\\u200a but involved multiple, possibly conflicting, factors.:\\u200axi\\u200a\\n\\nCauses include:\\n\\ndecline of labor unions \\u2013 Unions weakened in part due to globalization and automation may account for one-third to more than one-half of the rise of inequality among men. Pressure on employers to increase wages and on lawmakers to enact worker-friendly measures declined. Rewards from productivity gains went to executives, investors and creditors. A study by Kristal and Cohen reported that rising wage inequality was driven more by declining unions and the fall in the real value of the minimum wage, with twice as much impact as technology. An alternative theory states that passthrough income's contribution is incorrectly attributed to capital rather than labor.\\nglobalization \\u2013 Low skilled American workers lost ground in the face of competition from low-wage workers in Asia and other \\\"emerging\\\" economies.\\nskill-biased technological change \\u2013 Rapid progress in information technology increased the demand for skilled and educated workers.\\nsuperstars \\u2013 Modern communication technologies often turn competition into a \\\"winner take most\\\" tournament in which the winner is richly rewarded, while the runners-up get far less.\\nfinancialization \\u2013 In the 1990s stock market capitalization rose from 55% to 155% of Gross Domestic Product (GDP). Corporations began to shift executive compensation toward stock options, increasing incentives for managers to make decisions to increase share prices. Average annual CEO options increased from $500,000 to over $3 million. Stock comprised almost 50% of CEO compensation. Managers were incentivized to increase shareholder wealth rather than to improve long-term contracts with workers; between 2000 and 2007, nearly 75% of increased stock growth came at the cost of labor wages and salaries.\\nimmigration of less-educated workers \\u2013 Relatively high levels of immigration of low skilled workers since 1965 may have reduced wages for American-born high school dropouts;\\ncollege premium - Workers with college degrees traditionally earned more and faced a lower unemployment rate than others. Wealthy families are also more likely to send their children to schools which have large endowments, resulting in more grants and lower student debt. The cycle is completed when wealthier alums donate more and disproportionately increase the size of elite endowments. Elite colleges also have better access to financial expertise.\\nautomation - The Bureau of Labor Statistics (BLS) found that increased automation had led to \\\"an overall drop in the need for labor input. This would cause capital share to increase, relative to labor share, as machines replace some workers.\\\"\\nWe haven't achieved the minimalist state that libertarians advocate. What we've achieved is a state too constrained to provide the public goods \\u2013 investments in infrastructure, technology, and education \\u2013 that would make for a vibrant economy and too weak to engage in the redistribution that is needed to create a fair society. But we have a state that is still large enough and distorted enough that it can provide a bounty of gifts to the wealthy.\\n\\n\\u2014Joseph Stiglitz\\npolicy \\u2013 Krugman asserted that movement conservatives increased their influence over the Republican Party beginning in the 1970s. In the same era, it increased its political power. The result was less progressive tax laws, anti-labor policies, and slower expansion of the welfare state relative to other developed nations (e.g., the unique absence of universal healthcare). Further, variation in income inequality across developed countries indicate that policy has a significant influence on inequality; Japan, Sweden and France have income inequality around 1960 levels. The US was an early adopter of neoliberalism, whose focus on growth over equality spread to other countries over time.\\ncorporatism and corpocracy \\u2013 Excessive attention to the interests of corporations reduced scrutiny over compensation shifts.\\nfemale labor force participation \\u2013 High earning households are more likely to be dual earner households.\\nstock ownership is tilted towards households at higher income and education levels, resulting in disparate investment income.\\nHigher income households are disproportionately likely to prosper when economic times are good, and to suffer losses during downturns. More of their income comes from relatively volatile capital income. For example, in 2011 the top 1% of income earners derived 37% of their income from labor, versus 62% for the middle quintile. The top 1% derived 58% of their income from capital as opposed to 4% for the middle quintile. Government transfers represented only 1% of the income of the top 1% but 25% for the middle quintile; the dollar amounts of these transfers tend to rise in recessions.\\n\\nAccording to a 2018 report by the Organization of Economic Cooperation and Development (OECD), the US has higher income inequality and a larger percentage of low income workers than almost any other advanced nation because unemployed and at-risk workers get less support from the government and a weak collective bargaining system.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"response\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"The text lists the following as causes of income inequality in the United States: decline of labor unions, globalization, skill-biased technological change, superstars, financialization, immigration of less-educated workers, college premium, automation, public policy, corporatism and corpocracy, female labor force participation, and disproportionate stock ownership in households at higher income and education levels.\",\n          \"We do not really need a mobile phone to live.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"category\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"information_extraction\",\n          \"brainstorming\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_prompt_formats(sample):\n",
        "    \"\"\"\n",
        "    Format various fields of the sample ('instruction', 'context', 'response')\n",
        "    Then concatenate them using two newline characters\n",
        "    :param sample: Sample dictionnary\n",
        "    \"\"\"\n",
        "    INTRO_BLURB = \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\"\n",
        "    INSTRUCTION_KEY = \"### Instruction:\"\n",
        "    INPUT_KEY = \"### Input:\"\n",
        "    RESPONSE_KEY = \"### Response:\"\n",
        "    END_KEY = \"### End\"\n",
        "\n",
        "    blurb = f\"{INTRO_BLURB}\"\n",
        "    instruction = f\"{INSTRUCTION_KEY}\\n{sample['instruction']}\"\n",
        "    input_context = f\"{INPUT_KEY}\\n{sample['context']}\" if sample[\"context\"] else None\n",
        "    response = f\"{RESPONSE_KEY}\\n{sample['response']}\"\n",
        "    end = f\"{END_KEY}\"\n",
        "\n",
        "    parts = [part for part in [blurb, instruction, input_context, response, end] if part]\n",
        "\n",
        "    formatted_prompt = \"\\n\\n\".join(parts)\n",
        "\n",
        "    sample[\"text\"] = formatted_prompt\n",
        "\n",
        "    return sample\n",
        "\n",
        "\n",
        "print(create_prompt_formats(sample)[\"text\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RDlEMKa1J7VU",
        "outputId": "c91b1596-8034-4a14-94fb-f3cb31e1b75a"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "What links Brazil, Uruguay, Mozambique and Angola\n",
            "\n",
            "### Response:\n",
            "Colonies of Portugal\n",
            "\n",
            "### End\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# SOURCE https://github.com/databrickslabs/dolly/blob/master/training/trainer.py\n",
        "def get_max_length(model):\n",
        "    conf = model.config\n",
        "    max_length = None\n",
        "    for length_setting in [\"n_positions\", \"max_position_embeddings\", \"seq_length\"]:\n",
        "        max_length = getattr(model.config, length_setting, None)\n",
        "        if max_length:\n",
        "            print(f\"Found max lenth: {max_length}\")\n",
        "            break\n",
        "    if not max_length:\n",
        "        max_length = 1024\n",
        "        print(f\"Using default max length: {max_length}\")\n",
        "    return max_length\n",
        "\n",
        "\n",
        "def preprocess_batch(batch, tokenizer, max_length):\n",
        "    \"\"\"\n",
        "    Tokenizing a batch\n",
        "    \"\"\"\n",
        "    return tokenizer(\n",
        "        batch[\"text\"],\n",
        "        max_length=max_length,\n",
        "        truncation=True,\n",
        "    )\n",
        "\n",
        "\n",
        "# SOURCE https://github.com/databrickslabs/dolly/blob/master/training/trainer.py\n",
        "def preprocess_dataset(tokenizer: AutoTokenizer, max_length: int, seed, dataset: str):\n",
        "    \"\"\"Format & tokenize it so it is ready for training\n",
        "    :param tokenizer (AutoTokenizer): Model Tokenizer\n",
        "    :param max_length (int): Maximum number of tokens to emit from tokenizer\n",
        "    \"\"\"\n",
        "\n",
        "    # Add prompt to each sample\n",
        "    print(\"Preprocessing dataset...\")\n",
        "    dataset = dataset.map(create_prompt_formats)#, batched=True)\n",
        "\n",
        "    # Apply preprocessing to each batch of the dataset & and remove 'instruction', 'context', 'response', 'category' fields\n",
        "    _preprocessing_function = partial(preprocess_batch, max_length=max_length, tokenizer=tokenizer)\n",
        "    dataset = dataset.map(\n",
        "        _preprocessing_function,\n",
        "        batched=True,\n",
        "        remove_columns=[\"instruction\", \"context\", \"response\", \"text\", \"category\"],\n",
        "    )\n",
        "\n",
        "    # Filter out samples that have input_ids exceeding max_length\n",
        "    dataset = dataset.filter(lambda sample: len(sample[\"input_ids\"]) < max_length)\n",
        "\n",
        "    # Shuffle dataset\n",
        "    dataset = dataset.shuffle(seed=seed)\n",
        "\n",
        "    return dataset"
      ],
      "metadata": {
        "id": "6OkiQ9gXKRMI"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL  = \"google/gemma-2-2b\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
        "model = AutoModelForCausalLM.from_pretrained(MODEL,load_in_8bit=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104,
          "referenced_widgets": [
            "c0f421016e734a54999882ef518f1583",
            "25f1664868a34e31b75b7d7b24509782",
            "a1dae23e801241a6ae06dc7aebfe52c3",
            "eb663142b8ad4e599611f4be02bfda5c",
            "e873388afb5b468f981dc624c82f0f28",
            "1c6a91ef54824fc2a49797b1833acf40",
            "357810ff8d8348d89c7aae875e3725e6",
            "ac162ed7a53f401aa740e1445af230a3",
            "16e2d63a49c842c7b458e1c33060e746",
            "22f2f234e53f4eafa29db55cbd97f767",
            "597894e9c87d4ed5bac138666e96dc5a"
          ]
        },
        "id": "85JflG9VBdPf",
        "outputId": "23af62bc-3e14-4644-d501-088dbbd3a49c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
            "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c0f421016e734a54999882ef518f1583"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# SOURCE https://github.com/artidoro/qlora/blob/main/qlora.py\n",
        "def find_all_linear_names(model):\n",
        "    cls = bnb.nn.Linear8bitLt\n",
        "    lora_module_names = set()\n",
        "    for name, module in model.named_modules():\n",
        "        if isinstance(module, cls):\n",
        "            names = name.split('.')\n",
        "            lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n",
        "\n",
        "    if 'lm_head' in lora_module_names:  # needed for 16-bit\n",
        "        lora_module_names.remove('lm_head')\n",
        "    return list(lora_module_names)\n",
        "\n"
      ],
      "metadata": {
        "id": "5KrsONZCKsSF"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "target_modules = find_all_linear_names(model)"
      ],
      "metadata": {
        "id": "qVmkDfgSK9KM"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "peft_config = LoraConfig(\n",
        "    task_type=\"CAUSAL_LM\",\n",
        "    r=8,  # Rank of the adapter\n",
        "    lora_alpha=32,  # Scaling parameter\n",
        "    lora_dropout=0.1,  # Dropout for LoRA layers\n",
        "    target_modules=target_modules,\n",
        ")\n",
        "\n",
        "model= get_peft_model(model,peft_config)"
      ],
      "metadata": {
        "id": "biTKgoRYGz-a"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def print_trainable_parameters(model, use_4bit=False):\n",
        "    \"\"\"\n",
        "    Prints the number of trainable parameters in the model.\n",
        "    \"\"\"\n",
        "    trainable_params = 0\n",
        "    all_param = 0\n",
        "    for _, param in model.named_parameters():\n",
        "        num_params = param.numel()\n",
        "        # if using DS Zero 3 and the weights are initialized empty\n",
        "        if num_params == 0 and hasattr(param, \"ds_numel\"):\n",
        "            num_params = param.ds_numel\n",
        "\n",
        "        all_param += num_params\n",
        "        if param.requires_grad:\n",
        "            trainable_params += num_params\n",
        "    if use_4bit:\n",
        "        trainable_params /= 2\n",
        "    print(\n",
        "        f\"all params: {all_param:,d} || trainable params: {trainable_params:,d} || trainable%: {100 * trainable_params / all_param}\"\n",
        "    )\n",
        "print_trainable_parameters(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3XRhTVlFLKZq",
        "outputId": "9d2fc323-6a46-4bdb-ea4e-c083bc63ed6b"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "all params: 2,626,322,688 || trainable params: 11,980,800 || trainable%: 0.4561815672819562\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max_length = get_max_length(model)\n",
        "seed = 42\n",
        "dataset = preprocess_dataset(tokenizer, max_length, seed, dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 116,
          "referenced_widgets": [
            "d4975ca20cc647d9812c38d76116231a",
            "456c0bd7ac1e435bb2b1f7c4f6183828",
            "6a7fbed796154a0ba378c39530e6998e",
            "06d4a5a14a4a49e891dad9c0991db152",
            "087a2a70430d4daeacee1e66924c9140",
            "d05962b3f65f429ca01882e37cd80841",
            "c232382bb8b64ba9af4bd099e6a0c77b",
            "64e7ee281b5b4fcb9b5df50ebaa57a0c",
            "301caf4995b040d483785b381644e783",
            "015fbb3df4fd4a87a6120a11efb10691",
            "457cd9c49fab4f8daaee69ed935d54d3",
            "f3931d2c6ba34cdaa88f448bcfe797f2",
            "44e6598a3fab45ed9c67942e7239111b",
            "2d7ec7f76e264967a6e074ca0e451137",
            "16afed569b174eea94cd9f40272267d6",
            "074caf5acec642d48077c028b5e61403",
            "e8bd76782bbf4eb48e15c0f552c0a74a",
            "274501b3b9fc415d98a1a7453c898f50",
            "a93dcf9c3f364279b9543ee33cad4d0b",
            "2d76ebe0f9cf4454a4e26f903f1f959f",
            "bdd8df0f51344bf3a4dd858e73fc0f1a",
            "c266b8c78b324902aa1d1188da1f20d8"
          ]
        },
        "id": "we2wCXjwLh3K",
        "outputId": "770f53be-0a30-435a-ea45-c4059aeaad6a"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found max lenth: 8192\n",
            "Preprocessing dataset...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/15011 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d4975ca20cc647d9812c38d76116231a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Filter:   0%|          | 0/15011 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f3931d2c6ba34cdaa88f448bcfe797f2"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.config"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CP9ivbWNHRJN",
        "outputId": "1ceab408-c53b-4a8d-dd8d-ddd201af681c"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Gemma2Config {\n",
              "  \"_name_or_path\": \"google/gemma-2-2b\",\n",
              "  \"architectures\": [\n",
              "    \"Gemma2ForCausalLM\"\n",
              "  ],\n",
              "  \"attention_bias\": false,\n",
              "  \"attention_dropout\": 0.0,\n",
              "  \"attn_logit_softcapping\": 50.0,\n",
              "  \"bos_token_id\": 2,\n",
              "  \"cache_implementation\": \"hybrid\",\n",
              "  \"eos_token_id\": 1,\n",
              "  \"final_logit_softcapping\": 30.0,\n",
              "  \"head_dim\": 256,\n",
              "  \"hidden_act\": \"gelu_pytorch_tanh\",\n",
              "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
              "  \"hidden_size\": 2304,\n",
              "  \"initializer_range\": 0.02,\n",
              "  \"intermediate_size\": 9216,\n",
              "  \"max_position_embeddings\": 8192,\n",
              "  \"model_type\": \"gemma2\",\n",
              "  \"num_attention_heads\": 8,\n",
              "  \"num_hidden_layers\": 26,\n",
              "  \"num_key_value_heads\": 4,\n",
              "  \"pad_token_id\": 0,\n",
              "  \"quantization_config\": {\n",
              "    \"_load_in_4bit\": false,\n",
              "    \"_load_in_8bit\": true,\n",
              "    \"bnb_4bit_compute_dtype\": \"float32\",\n",
              "    \"bnb_4bit_quant_storage\": \"uint8\",\n",
              "    \"bnb_4bit_quant_type\": \"fp4\",\n",
              "    \"bnb_4bit_use_double_quant\": false,\n",
              "    \"llm_int8_enable_fp32_cpu_offload\": false,\n",
              "    \"llm_int8_has_fp16_weight\": false,\n",
              "    \"llm_int8_skip_modules\": null,\n",
              "    \"llm_int8_threshold\": 6.0,\n",
              "    \"load_in_4bit\": false,\n",
              "    \"load_in_8bit\": true,\n",
              "    \"quant_method\": \"bitsandbytes\"\n",
              "  },\n",
              "  \"query_pre_attn_scalar\": 256,\n",
              "  \"rms_norm_eps\": 1e-06,\n",
              "  \"rope_theta\": 10000.0,\n",
              "  \"sliding_window\": 4096,\n",
              "  \"torch_dtype\": \"float32\",\n",
              "  \"transformers_version\": \"4.42.4\",\n",
              "  \"use_cache\": true,\n",
              "  \"vocab_size\": 256000\n",
              "}"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Trainer(\n",
        "        model=model,\n",
        "        train_dataset=dataset,\n",
        "        args=TrainingArguments(\n",
        "            per_device_train_batch_size=1,\n",
        "            gradient_accumulation_steps=4,\n",
        "            warmup_steps=2,\n",
        "            max_steps=15,\n",
        "            learning_rate=2e-4,\n",
        "            fp16=True,\n",
        "            logging_steps=1,\n",
        "            output_dir=\"outputs\",\n",
        "            optim=\"paged_adamw_8bit\",\n",
        "        ),\n",
        "        data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False))\n",
        "dtypes = {}\n",
        "for _, p in model.named_parameters():\n",
        "  dtype = p.dtype\n",
        "  if dtype not in dtypes: dtypes[dtype] = 0\n",
        "  dtypes[dtype] += p.numel()\n",
        "  total = 0\n",
        "  for k, v in dtypes.items():\n",
        "    total+= v\n",
        "  for k, v in dtypes.items():\n",
        "    print(k, v, v/total)\n",
        "\n",
        "\n",
        "\n",
        "    # Launch training\n",
        "do_train = True\n",
        "print(\"Training...\")\n",
        "if do_train:\n",
        "  train_result = trainer.train()\n",
        "  metrics = train_result.metrics\n",
        "  trainer.log_metrics(\"train\", metrics)\n",
        "  trainer.save_metrics(\"train\", metrics)\n",
        "  trainer.save_state()\n",
        "  print(metrics)\n",
        "print(\"Saving last checkpoint of the model...\")\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "trainer.model.save_pretrained(output_dir)\n",
        "\n",
        "# Free memory for merging weights\n",
        "del model\n",
        "del trainer\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "\n",
        "output_dir = \"/results/gemma2/final_checkpoint/\"\n",
        "\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "7I7d3UTgEuYL",
        "outputId": "d88f7e10-887d-4fdd-81a0-57849447f4cc"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:482: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n",
            "max_steps is given, it will override any value given in num_train_epochs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.float16 589824000 1.0\n",
            "torch.float16 589824000 0.9920634920634921\n",
            "torch.int8 4718592 0.007936507936507936\n",
            "torch.float16 589824000 0.9920327370803237\n",
            "torch.int8 4718592 0.00793626189664259\n",
            "torch.float32 18432 3.100102303376012e-05\n",
            "torch.float16 589824000 0.9920054009182939\n",
            "torch.int8 4718592 0.00793604320734635\n",
            "torch.float32 34816 5.85558743597604e-05\n",
            "torch.float16 589824000 0.9919746495367341\n",
            "torch.int8 4718592 0.007935797196293873\n",
            "torch.float32 53248 8.955326697206627e-05\n",
            "torch.float16 589824000 0.991947316575853\n",
            "torch.int8 4718592 0.007935578532606824\n",
            "torch.float32 69632 0.00011710489154020487\n",
            "torch.float16 589824000 0.9919165687952691\n",
            "torch.int8 4718592 0.007935332550362153\n",
            "torch.float32 88064 0.0001480986543687381\n",
            "torch.float16 589824000 0.9918892390349744\n",
            "torch.int8 4718592 0.007935113912279796\n",
            "torch.float32 104448 0.00017564705274577673\n",
            "torch.float16 589824000 0.9879694141135548\n",
            "torch.int8 7077888 0.011855632969362657\n",
            "torch.float32 104448 0.00017495291708260864\n",
            "torch.float16 589824000 0.9879389124439139\n",
            "torch.int8 7077888 0.011855266949326967\n",
            "torch.float32 122880 0.00020582060675914873\n",
            "torch.float16 589824000 0.9879253567508233\n",
            "torch.int8 7077888 0.011855104281009879\n",
            "torch.float32 131072 0.0002195389681668496\n",
            "torch.float16 589824000 0.9878948578014537\n",
            "torch.int8 7077888 0.011854738293617445\n",
            "torch.float32 149504 0.0002504039049288407\n",
            "torch.float16 589824000 0.9878813033172917\n",
            "torch.int8 7077888 0.0118545756398075\n",
            "torch.float32 157696 0.0002641210429008037\n",
            "torch.float16 589824000 0.9839930300493704\n",
            "torch.int8 9437184 0.015743888480789926\n",
            "torch.float32 157696 0.00026308146983958865\n",
            "torch.float16 589824000 0.9839627734084061\n",
            "torch.int8 9437184 0.015743404374534496\n",
            "torch.float32 176128 0.00029382221705945456\n",
            "torch.float16 589824000 0.9839493266096796\n",
            "torch.int8 9437184 0.015743189225754874\n",
            "torch.float32 184320 0.0003074841645655249\n",
            "torch.float16 589824000 0.983919072656274\n",
            "torch.int8 9437184 0.015742705162500384\n",
            "torch.float32 202752 0.0003382221812255942\n",
            "torch.float16 589824000 0.9839056270519386\n",
            "torch.int8 9437184 0.015742490032831017\n",
            "torch.float32 210944 0.0003518829152303808\n",
            "torch.float16 589824000 0.9838753757857338\n",
            "torch.int8 9437184 0.01574200601257174\n",
            "torch.float32 229376 0.00038261820169445204\n",
            "torch.float16 589824000 0.9838619313756303\n",
            "torch.int8 9437184 0.015741790902010083\n",
            "torch.float32 237568 0.0003962777223596289\n",
            "torch.float16 589824000 0.9761785322071126\n",
            "torch.int8 14155776 0.023428284772970702\n",
            "torch.float32 237568 0.00039318301991675365\n",
            "torch.float16 589824000 0.976152062799116\n",
            "torch.int8 14155776 0.023427649507178783\n",
            "torch.float32 253952 0.00042028769370517494\n",
            "torch.float16 589824000 0.9761222864308834\n",
            "torch.int8 14155776 0.023426934874341204\n",
            "torch.float32 272384 0.00045077869477537323\n",
            "torch.float16 589824000 0.9760958200730039\n",
            "torch.int8 14155776 0.023426299681752093\n",
            "torch.float32 288768 0.0004778802452440748\n",
            "torch.float16 589824000 0.9760660471358562\n",
            "torch.int8 14155776 0.02342558513126055\n",
            "torch.float32 307200 0.0005083677328832584\n",
            "torch.float16 589824000 0.9429329142520381\n",
            "torch.int8 35389440 0.05657597485512229\n",
            "torch.float32 307200 0.0004911108928396031\n",
            "torch.float16 589824000 0.9429051299932228\n",
            "torch.int8 35389440 0.056574307799593375\n",
            "torch.float32 325632 0.0005205622071837585\n",
            "torch.float16 589824000 0.9427940093297323\n",
            "torch.int8 35389440 0.056567640559783944\n",
            "torch.float32 399360 0.000638350110483673\n",
            "torch.float16 589824000 0.9427662332560789\n",
            "torch.int8 35389440 0.056565973995364734\n",
            "torch.float32 417792 0.0006677927485563892\n",
            "torch.float16 589824000 0.9426551453260016\n",
            "torch.int8 35389440 0.056559308719560095\n",
            "torch.float32 491520 0.0007855459544383347\n",
            "torch.float16 589824000 0.911715544750038\n",
            "torch.int8 56623104 0.08752469229600365\n",
            "torch.float32 491520 0.000759762953958365\n",
            "torch.float16 589824000 0.9116895697015166\n",
            "torch.int8 56623104 0.0875221986913456\n",
            "torch.float32 509952 0.0007882316071377696\n",
            "torch.float16 589824000 0.9115856843064827\n",
            "torch.int8 56623104 0.08751222569342235\n",
            "torch.float32 583680 0.0009020900000949568\n",
            "torch.float16 589824000 0.9115597166568548\n",
            "torch.int8 56623104 0.08750973279905805\n",
            "torch.float32 602112 0.0009305505440872059\n",
            "torch.float16 589824000 0.9114558608510719\n",
            "torch.int8 56623104 0.08749976264170291\n",
            "torch.float32 675840 0.0010443765072251866\n",
            "torch.float16 589824000 0.882498942839808\n",
            "torch.int8 77856768 0.11648986045485467\n",
            "torch.float32 675840 0.0010111967053372801\n",
            "torch.float16 589824000 0.8824016030295788\n",
            "torch.int8 77856768 0.11647701159990441\n",
            "torch.float32 749568 0.0011213853705167565\n",
            "torch.float16 589824000 0.8823772714321167\n",
            "torch.int8 77856768 0.11647379982903941\n",
            "torch.float32 768000 0.0011489287388439019\n",
            "torch.float16 589824000 0.8822799584593186\n",
            "torch.int8 77856768 0.11646095451663006\n",
            "torch.float32 841728 0.0012590870240513192\n",
            "torch.float16 589824000 0.8822556335698268\n",
            "torch.int8 77856768 0.11645774363121715\n",
            "torch.float32 860160 0.0012866227989559976\n",
            "torch.float16 589826304 0.8822560393521417\n",
            "torch.int8 77856768 0.11645734228298942\n",
            "torch.float32 860160 0.0012866183648688855\n",
            "torch.float16 589828608 0.8822564451316597\n",
            "torch.int8 77856768 0.116456940937528\n",
            "torch.float32 860160 0.0012866139308123359\n",
            "torch.float16 589830912 0.8822568509083808\n",
            "torch.int8 77856768 0.11645653959483289\n",
            "torch.float32 860160 0.0012866094967863481\n",
            "torch.float16 589833216 0.882257256682305\n",
            "torch.int8 77856768 0.11645613825490403\n",
            "torch.float32 860160 0.0012866050627909221\n",
            "torch.float16 589833216 0.8760739723402217\n",
            "torch.int8 82575360 0.12264843974576001\n",
            "torch.float32 860160 0.0012775879140183334\n",
            "torch.float16 589833216 0.8760499888214118\n",
            "torch.int8 82575360 0.12264508210558975\n",
            "torch.float32 878592 0.0013049290729984623\n",
            "torch.float16 589833216 0.8760286712403976\n",
            "torch.int8 82575360 0.12264209769087925\n",
            "torch.float32 894976 0.0013292310687230712\n",
            "torch.float16 589833216 0.8760046902018277\n",
            "torch.int8 82575360 0.12263874039793717\n",
            "torch.float32 913408 0.0013565694002351184\n",
            "torch.float16 589833216 0.8759833748253003\n",
            "torch.int8 82575360 0.12263575629184999\n",
            "torch.float32 929792 0.0013808688828496998\n",
            "torch.float16 589833216 0.8759593962665856\n",
            "torch.int8 82575360 0.1226323993460822\n",
            "torch.float32 948224 0.0014082043873322435\n",
            "torch.float16 589833216 0.875938083094203\n",
            "torch.int8 82575360 0.12262941554857046\n",
            "torch.float32 964608 0.0014325013572266043\n",
            "torch.float16 589833216 0.8728797763280521\n",
            "torch.int8 84934656 0.12569272384242947\n",
            "torch.float32 964608 0.0014274998295183325\n",
            "torch.float16 589833216 0.8728559673775179\n",
            "torch.int8 84934656 0.12568929540712184\n",
            "torch.float32 983040 0.0014547372153602067\n",
            "torch.float16 589833216 0.8728453860386319\n",
            "torch.int8 84934656 0.1256877717181299\n",
            "torch.float32 991232 0.0014668422432382057\n",
            "torch.float16 589833216 0.8728215789641164\n",
            "torch.int8 84934656 0.12568434355296473\n",
            "torch.float32 1009664 0.0014940774829188758\n",
            "torch.float16 589833216 0.8728109984589671\n",
            "torch.int8 84934656 0.12568281998402903\n",
            "torch.float32 1017856 0.00150618155700382\n",
            "torch.float16 589833216 0.8697744510767066\n",
            "torch.int8 87293952 0.12872460743736136\n",
            "torch.float32 1017856 0.001500941485932071\n",
            "torch.float16 589833216 0.869750811226254\n",
            "torch.int8 87293952 0.12872110879416068\n",
            "torch.float32 1036288 0.0015280799795853346\n",
            "torch.float16 589833216 0.8697403050385262\n",
            "torch.int8 87293952 0.12871955390267215\n",
            "torch.float32 1044480 0.0015401410588016798\n",
            "torch.float16 589833216 0.869716667044142\n",
            "torch.int8 87293952 0.1287160555341653\n",
            "torch.float32 1062912 0.0015672774216927501\n",
            "torch.float16 589833216 0.8697061616812849\n",
            "torch.int8 87293952 0.1287145007647557\n",
            "torch.float32 1071104 0.0015793375539594416\n",
            "torch.float16 589833216 0.8696825255427504\n",
            "torch.int8 87293952 0.12871100267091032\n",
            "torch.float32 1089536 0.0016064717863392523\n",
            "torch.float16 589833216 0.8696720210046669\n",
            "torch.int8 87293952 0.12870944802356532\n",
            "torch.float32 1097728 0.0016185309717678073\n",
            "torch.float16 589833216 0.8636632845381198\n",
            "torch.int8 92012544 0.13472936724158355\n",
            "torch.float32 1097728 0.0016073482202966698\n",
            "torch.float16 589833216 0.8636425655181099\n",
            "torch.int8 92012544 0.13472613512496384\n",
            "torch.float32 1114112 0.0016312993569262003\n",
            "torch.float16 589833216 0.8636192578088103\n",
            "torch.int8 92012544 0.13472249917912474\n",
            "torch.float32 1132544 0.0016582430120649922\n",
            "torch.float16 589833216 0.863598540901097\n",
            "torch.int8 92012544 0.13471926739201814\n",
            "torch.float32 1148928 0.0016821917068848419\n",
            "torch.float16 589833216 0.8635752355679492\n",
            "torch.int8 92012544 0.13471563181685295\n",
            "torch.float32 1167360 0.0017091326151977872\n",
            "torch.float16 589833216 0.837537677701344\n",
            "torch.int8 113246208 0.1608047215448839\n",
            "torch.float32 1167360 0.0016576007537721323\n",
            "torch.float16 589833216 0.8375157577000083\n",
            "torch.int8 113246208 0.16080051297038642\n",
            "torch.float32 1185792 0.0016837293296052833\n",
            "torch.float16 589833216 0.8374280891672519\n",
            "torch.int8 113246208 0.16078368087509867\n",
            "torch.float32 1259520 0.0017882299576494807\n",
            "torch.float16 589833216 0.837406174901759\n",
            "torch.int8 113246208 0.16077947340186582\n",
            "torch.float32 1277952 0.0018143516963752219\n",
            "torch.float16 589833216 0.8373185293078711\n",
            "torch.int8 113246208 0.16076264571077203\n",
            "torch.float32 1351680 0.001918824981356871\n",
            "torch.float16 589833216 0.8128177665640782\n",
            "torch.int8 134479872 0.1853195551585605\n",
            "torch.float32 1351680 0.0018626782773612622\n",
            "torch.float16 589833216 0.8127971213885067\n",
            "torch.int8 134479872 0.18531484813207746\n",
            "torch.float32 1370112 0.0018880304794158113\n",
            "torch.float16 589833216 0.8127145511727034\n",
            "torch.int8 134479872 0.1852960224170261\n",
            "torch.float32 1443840 0.0019894264102705197\n",
            "torch.float16 589833216 0.8126939112399738\n",
            "torch.int8 134479872 0.18529131658589237\n",
            "torch.float32 1462272 0.0020147721741338807\n",
            "torch.float16 589833216 0.8126113619915439\n",
            "torch.int8 134479872 0.18527249565132745\n",
            "torch.float32 1536000 0.002116142357128649\n",
            "torch.float16 589833216 0.7895152369947901\n",
            "torch.int8 155713536 0.20842876586715792\n",
            "torch.float32 1536000 0.002055997138051984\n",
            "torch.float16 589833216 0.7894373291121025\n",
            "torch.int8 155713536 0.20840819850749337\n",
            "torch.float32 1609728 0.0021544723804041693\n",
            "torch.float16 589833216 0.78941785454381\n",
            "torch.int8 155713536 0.20840305730179548\n",
            "torch.float32 1628160 0.002179088154394563\n",
            "torch.float16 589833216 0.7893399658780242\n",
            "torch.int8 155713536 0.20838249501531378\n",
            "torch.float32 1701888 0.0022775391066620077\n",
            "torch.float16 589833216 0.7893204961130684\n",
            "torch.int8 155713536 0.20837735507767696\n",
            "torch.float32 1720320 0.002302148809254638\n",
            "torch.float16 589835520 0.7893211456854884\n",
            "torch.int8 155713536 0.2083767126033009\n",
            "torch.float32 1720320 0.0023021417112107107\n",
            "torch.float16 589837824 0.7893217952539029\n",
            "torch.int8 155713536 0.20837607013288661\n",
            "torch.float32 1720320 0.002302134613210553\n",
            "torch.float16 589840128 0.7893224448183118\n",
            "torch.int8 155713536 0.20837542766643405\n",
            "torch.float32 1720320 0.002302127515254164\n",
            "torch.float16 589842432 0.7893230943787153\n",
            "torch.int8 155713536 0.2083747852039432\n",
            "torch.float32 1720320 0.002302120417341544\n",
            "torch.float16 589842432 0.7843702765635852\n",
            "torch.int8 160432128 0.2133420482862862\n",
            "torch.float32 1720320 0.002287675150128682\n",
            "torch.float16 589842432 0.7843510514877694\n",
            "torch.int8 160432128 0.21333681922907238\n",
            "torch.float32 1738752 0.0023121292831582217\n",
            "torch.float16 589842432 0.7843339633226398\n",
            "torch.int8 160432128 0.213332171393402\n",
            "torch.float32 1755136 0.0023338652839581483\n",
            "torch.float16 589842432 0.7843147400268511\n",
            "torch.int8 160432128 0.21332694282034034\n",
            "torch.float32 1773568 0.0023583171528086034\n",
            "torch.float16 589842432 0.7842976534438577\n",
            "torch.int8 160432128 0.2133222954149976\n",
            "torch.float32 1789952 0.0023800511411446574\n",
            "torch.float16 589842432 0.7842784319278486\n",
            "torch.int8 160432128 0.2133170673260209\n",
            "torch.float32 1808384 0.0024045007461304694\n",
            "torch.float16 589842432 0.7842613469267719\n",
            "torch.int8 160432128 0.213312420350946\n",
            "torch.float32 1824768 0.002426232722282129\n",
            "torch.float16 589842432 0.7818088537085899\n",
            "torch.int8 162791424 0.2157725007329229\n",
            "torch.float32 1824768 0.002418645558487247\n",
            "torch.float16 589842432 0.7817897539882247\n",
            "torch.int8 162791424 0.21576722937483203\n",
            "torch.float32 1843200 0.0024430166369432977\n",
            "torch.float16 589842432 0.7817812655231663\n",
            "torch.int8 162791424 0.21576488663168666\n",
            "torch.float32 1851392 0.002453847845147\n",
            "torch.float16 589842432 0.7817621671507288\n",
            "torch.int8 162791424 0.21575961564561222\n",
            "torch.float32 1869824 0.002478217203658967\n",
            "torch.float16 589842432 0.7817536792847177\n",
            "torch.int8 162791424 0.21575727306779874\n",
            "torch.float32 1878016 0.0024890476474835376\n",
            "torch.float16 589842432 0.7793168202702629\n",
            "torch.int8 165150720 0.21820189086658404\n",
            "torch.float32 1878016 0.0024812888631529956\n",
            "torch.float16 589842432 0.779297842115944\n",
            "torch.int8 165150720 0.2181965771494284\n",
            "torch.float32 1896448 0.002505580734627613\n",
            "torch.float16 589842432 0.7792894076773843\n",
            "torch.int8 165150720 0.2181942155804307\n",
            "torch.float32 1904640 0.002516376742185027\n",
            "torch.float16 589842432 0.7792704308581447\n",
            "torch.int8 165150720 0.21818890223708562\n",
            "torch.float32 1923072 0.0025406669047696355\n",
            "torch.float16 589842432 0.7792619970129223\n",
            "torch.int8 165150720 0.21818654083421718\n",
            "torch.float32 1931264 0.0025514621528604517\n",
            "torch.float16 589842432 0.7792430215286215\n",
            "torch.int8 165150720 0.21818122786464322\n",
            "torch.float32 1949696 0.0025757506067353715\n",
            "torch.float16 589842432 0.7792345882766738\n",
            "torch.int8 165150720 0.21817886662788652\n",
            "torch.float32 1957888 0.002586545095439726\n",
            "torch.float16 589842432 0.7744071802510829\n",
            "torch.int8 169869312 0.22302229846548483\n",
            "torch.float32 1957888 0.0025705212834322374\n",
            "torch.float16 589842432 0.7743905226166053\n",
            "torch.int8 169869312 0.2230175012166693\n",
            "torch.float32 1974272 0.0025919761667253715\n",
            "torch.float16 589842432 0.7743717836343789\n",
            "torch.int8 169869312 0.22301210455843368\n",
            "torch.float32 1992704 0.0026161118071874514\n",
            "torch.float16 589842432 0.774355127522625\n",
            "torch.int8 169869312 0.22300730774814886\n",
            "torch.float32 2009088 0.002637564729226153\n",
            "torch.float16 589842432 0.7743363902533452\n",
            "torch.int8 169869312 0.22300191158322646\n",
            "torch.float32 2027520 0.002661698163428267\n",
            "torch.float16 589842432 0.7533369429600508\n",
            "torch.int8 191102976 0.24407354222086208\n",
            "torch.float32 2027520 0.0025895148190870783\n",
            "torch.float16 589842432 0.7533192090395481\n",
            "torch.int8 191102976 0.2440677966101695\n",
            "torch.float32 2045952 0.0026129943502824857\n",
            "torch.float16 589842432 0.7532482817060541\n",
            "torch.int8 191102976 0.24404481687223425\n",
            "torch.float32 2119680 0.002706901421711703\n",
            "torch.float16 589842432 0.7532305519595152\n",
            "torch.int8 191102976 0.2440390726138637\n",
            "torch.float32 2138112 0.0027303754266211604\n",
            "torch.float16 589842432 0.7531596413189297\n",
            "torch.int8 191102976 0.2440160982842618\n",
            "torch.float32 2211840 0.002824260396808586\n",
            "torch.float16 589842432 0.7332783391764625\n",
            "torch.int8 212336640 0.26397195297999587\n",
            "torch.float32 2211840 0.002749707843541624\n",
            "torch.float16 589842432 0.7332615370514642\n",
            "torch.int8 212336640 0.26396590440401446\n",
            "torch.float32 2230272 0.0027725585445213326\n",
            "torch.float16 589842432 0.7331943362507446\n",
            "torch.int8 212336640 0.26394171287174084\n",
            "torch.float32 2304000 0.002863950877514549\n",
            "torch.float16 589842432 0.7331775379751185\n",
            "torch.int8 212336640 0.26393566568149013\n",
            "torch.float32 2322432 0.0028867963433912985\n",
            "torch.float16 589842432 0.7331103525692424\n",
            "torch.int8 212336640 0.2639114796911869\n",
            "torch.float32 2396160 0.002978167739570686\n",
            "torch.float16 589842432 0.7142602057897909\n",
            "torch.int8 233570304 0.2828382027989197\n",
            "torch.float32 2396160 0.0029015914112894225\n",
            "torch.float16 589842432 0.7141964425201419\n",
            "torch.int8 233570304 0.28281295333318457\n",
            "torch.float32 2469888 0.002990604146673511\n",
            "torch.float16 589842432 0.7141805034815212\n",
            "torch.int8 233570304 0.2828066416711302\n",
            "torch.float32 2488320 0.003012854847348688\n",
            "torch.float16 589842432 0.7141167544407748\n",
            "torch.int8 233570304 0.28278139783986433\n",
            "torch.float32 2562048 0.0031018477193608856\n",
            "torch.float16 589842432 0.7141008189587843\n",
            "torch.int8 233570304 0.2827750875861915\n",
            "torch.float32 2580480 0.0031240934550242115\n",
            "torch.float16 589844736 0.7141016164349172\n",
            "torch.int8 233570304 0.2827742988242842\n",
            "torch.float32 2580480 0.003124084740798594\n",
            "torch.float16 589847040 0.7141024139066012\n",
            "torch.int8 233570304 0.28277351006677715\n",
            "torch.float32 2580480 0.0031240760266215907\n",
            "torch.float16 589849344 0.7141032113738365\n",
            "torch.int8 233570304 0.2827727213136703\n",
            "torch.float32 2580480 0.003124067312493201\n",
            "torch.float16 589851648 0.7141040088366228\n",
            "torch.int8 233570304 0.2827719325649637\n",
            "torch.float32 2580480 0.0031240585984134246\n",
            "torch.float16 589851648 0.7100478150412142\n",
            "torch.int8 238288896 0.28684587137642975\n",
            "torch.float32 2580480 0.003106313582356139\n",
            "torch.float16 589851648 0.7100320608823953\n",
            "torch.int8 238288896 0.28683950699459737\n",
            "torch.float32 2598912 0.0031284321230072884\n",
            "torch.float16 589851648 0.710018057772546\n",
            "torch.int8 238288896 0.2868338500033897\n",
            "torch.float32 2615296 0.003148092224064293\n",
            "torch.float16 589851648 0.7100023049341615\n",
            "torch.int8 238288896 0.2868274861549877\n",
            "torch.float32 2633728 0.0031702089108508165\n",
            "torch.float16 589851648 0.7099883029979577\n",
            "torch.int8 238288896 0.2868218296379106\n",
            "torch.float32 2650112 0.003189867364131741\n",
            "torch.float16 589851648 0.7099725514798413\n",
            "torch.int8 238288896 0.286815466322872\n",
            "torch.float32 2668544 0.003211982197286701\n",
            "torch.float16 589851648 0.7099585507171354\n",
            "torch.int8 238288896 0.28680981027986585\n",
            "torch.float32 2684928 0.0032316390029986947\n",
            "torch.float16 589851648 0.7079481894705016\n",
            "torch.int8 240648192 0.2888293224294758\n",
            "torch.float32 2684928 0.003222488100022491\n",
            "torch.float16 589851648 0.7079325283436262\n",
            "torch.int8 240648192 0.2888229329892156\n",
            "torch.float32 2703360 0.0032445386671582635\n",
            "torch.float16 589851648 0.7079255680652048\n",
            "torch.int8 240648192 0.28882009332872877\n",
            "torch.float32 2711552 0.0032543386060664902\n",
            "torch.float16 589851648 0.707909907939157\n",
            "torch.int8 240648192 0.28881370429678715\n",
            "torch.float32 2729984 0.0032763877640558385\n",
            "torch.float16 589851648 0.7079029481055267\n",
            "torch.int8 240648192 0.2888108648177665\n",
            "torch.float32 2738176 0.0032861870767067836\n",
            "torch.float16 589851648 0.7059041951537537\n",
            "torch.int8 243007488 0.29081889626758395\n",
            "torch.float32 2738176 0.0032769085786623496\n",
            "torch.float16 589851648 0.7058886243292228\n",
            "torch.int8 243007488 0.2908124813885747\n",
            "torch.float32 2756608 0.0032988942822025145\n",
            "torch.float16 589851648 0.7058817041832559\n",
            "torch.int8 243007488 0.2908096304220754\n",
            "torch.float32 2764800 0.0033086653946686373\n",
            "torch.float16 589851648 0.7058661343509093\n",
            "torch.int8 243007488 0.2908032159518269\n",
            "torch.float32 2783232 0.003330649697263794\n",
            "torch.float16 589851648 0.7058592146458922\n",
            "torch.int8 243007488 0.29080036516699037\n",
            "torch.float32 2791424 0.0033404201871174476\n",
            "torch.float16 589851648 0.705843645805635\n",
            "torch.int8 243007488 0.2907939511054635\n",
            "torch.float32 2809856 0.0033624030889014963\n",
            "torch.float16 589851648 0.7058367265415254\n",
            "torch.int8 243007488 0.2907911005022724\n",
            "torch.float32 2818048 0.0033721729562021883\n",
            "torch.float16 589851648 0.701873646732846\n",
            "torch.int8 247726080 0.2947731141380701\n",
            "torch.float32 2818048 0.0033532391290838665\n",
            "torch.float16 589851648 0.701859963568352\n",
            "torch.int8 247726080 0.2947673674783573\n",
            "torch.float32 2834432 0.003372668953290728\n",
            "torch.float16 589851648 0.7018445706459957\n",
            "torch.int8 247726080 0.29476090275400163\n",
            "torch.float32 2852864 0.0033945266000026805\n",
            "torch.float16 589851648 0.7018308886151552\n",
            "torch.int8 247726080 0.2947551565704009\n",
            "torch.float32 2869248 0.00341395481444388\n",
            "torch.float16 589851648 0.7018154969680799\n",
            "torch.int8 247726080 0.29474869238163814\n",
            "torch.float32 2887680 0.0034358106502819924\n",
            "torch.float16 589851648 0.6845216238088278\n",
            "torch.int8 268959744 0.31212722949294297\n",
            "torch.float32 2887680 0.0033511466982292398\n",
            "torch.float16 589851648 0.6845069820004492\n",
            "torch.int8 268959744 0.31212055314127635\n",
            "torch.float32 2906112 0.0033724648582744814\n",
            "torch.float16 589851648 0.6844484210301179\n",
            "torch.int8 268959744 0.31209385059048733\n",
            "torch.float32 2979840 0.0034577283793947905\n",
            "torch.float16 589851648 0.6844337823531299\n",
            "torch.int8 268959744 0.3120871756666679\n",
            "torch.float32 2998272 0.003479041980202255\n",
            "torch.float16 589851648 0.6843752339063519\n",
            "torch.int8 268959744 0.31206047882635146\n",
            "torch.float32 3072000 0.003564287267296595\n",
            "torch.float16 589851648 0.6679201177154028\n",
            "torch.int8 290193408 0.3286012947302877\n",
            "torch.float32 3072000 0.003478587554309448\n",
            "torch.float16 589851648 0.6679061774947039\n",
            "torch.int8 290193408 0.32859443646318515\n",
            "torch.float32 3090432 0.003499386042110902\n",
            "torch.float16 589851648 0.6678504224303746\n",
            "torch.int8 290193408 0.328567006257326\n",
            "torch.float32 3164160 0.00358257131229935\n",
            "torch.float16 589851648 0.6678364851187267\n",
            "torch.int8 290193408 0.32856014942140943\n",
            "torch.float32 3182592 0.003603365459863865\n",
            "torch.float16 589851648 0.6677807416887801\n",
            "torch.int8 290193408 0.3285327249393983\n",
            "torch.float32 3256320 0.003686533371821669\n",
            "torch.float16 589851648 0.6521048073494201\n",
            "torch.int8 311427072 0.3442951994430199\n",
            "torch.float32 3256320 0.003599993207559986\n",
            "torch.float16 589851648 0.6520516590880534\n",
            "torch.int8 311427072 0.34426713847637613\n",
            "torch.float32 3330048 0.0036812024355704675\n",
            "torch.float16 589851648 0.6520383733763477\n",
            "torch.int8 311427072 0.3442601239494015\n",
            "torch.float32 3348480 0.003701502674250785\n",
            "torch.float16 589851648 0.6519852359430762\n",
            "torch.int8 311427072 0.34423206869972395\n",
            "torch.float32 3422208 0.0037826953571998547\n",
            "torch.float16 589851648 0.6519719529379807\n",
            "torch.int8 311427072 0.34422505560177247\n",
            "torch.float32 3440640 0.003802991460246855\n",
            "torch.float16 589853952 0.6519728392399808\n",
            "torch.int8 311427072 0.3442241789846208\n",
            "torch.float32 3440640 0.0038029817753982734\n",
            "torch.float16 589856256 0.6519737255374669\n",
            "torch.int8 311427072 0.34422330237193405\n",
            "torch.float32 3440640 0.003802972090599019\n",
            "torch.float16 589858560 0.6519746118304388\n",
            "torch.int8 311427072 0.3442224257637121\n",
            "torch.float32 3440640 0.003802962405849092\n",
            "torch.float16 589860864 0.6519754981188965\n",
            "torch.int8 311427072 0.3442215491599549\n",
            "torch.float32 3440640 0.0038029527211484917\n",
            "torch.float16 589860864 0.6485927767494021\n",
            "torch.int8 316145664 0.3476240018375647\n",
            "torch.float32 3440640 0.0037832214130331977\n",
            "torch.float16 589860864 0.6485796318189495\n",
            "torch.int8 316145664 0.3476169565951697\n",
            "torch.float32 3459072 0.003803411585880763\n",
            "torch.float16 589860864 0.6485679478836164\n",
            "torch.int8 316145664 0.34761069439721853\n",
            "torch.float32 3475456 0.003821357719165111\n",
            "torch.float16 589860864 0.6485548039595399\n",
            "torch.int8 316145664 0.3476036496942075\n",
            "torch.float32 3493888 0.003841546346252578\n",
            "torch.float16 589860864 0.648543120918712\n",
            "torch.int8 316145664 0.3475973879756811\n",
            "torch.float32 3510272 0.0038594911056068454\n",
            "torch.float16 589860864 0.6485299780008962\n",
            "torch.int8 316145664 0.3475903438119921\n",
            "torch.float32 3528704 0.0038796781871117225\n",
            "torch.float16 589860864 0.6485182958544707\n",
            "torch.int8 316145664 0.3475840825728355\n",
            "torch.float32 3545088 0.003897621572693682\n",
            "torch.float16 589860864 0.6468404492565168\n",
            "torch.int8 318504960 0.3492720130977005\n",
            "torch.float32 3545088 0.0038875376457826617\n",
            "torch.float16 589860864 0.6468273752577042\n",
            "torch.int8 318504960 0.34926495357956155\n",
            "torch.float32 3563520 0.003907671162734292\n",
            "torch.float16 589860864 0.6468215647612076\n",
            "torch.int8 318504960 0.3492618161075453\n",
            "torch.float32 3571712 0.003916619131247165\n",
            "torch.float16 589860864 0.6468084915257662\n",
            "torch.int8 318504960 0.3492547570016012\n",
            "torch.float32 3590144 0.003936751472632503\n",
            "torch.float16 589860864 0.6468026813685309\n",
            "torch.int8 318504960 0.3492516197127747\n",
            "torch.float32 3598336 0.003945698918694349\n",
            "torch.float16 589860864 0.6451336888813229\n",
            "torch.int8 320864256 0.35093079357684115\n",
            "torch.float32 3598336 0.003935517541835873\n",
            "torch.float16 589860864 0.6451206837851883\n",
            "torch.int8 320864256 0.35092371924668947\n",
            "torch.float32 3616768 0.003955596968122279\n",
            "torch.float16 589860864 0.6451149039107646\n",
            "torch.int8 320864256 0.35092057519150655\n",
            "torch.float32 3624960 0.0039645208977288\n",
            "torch.float16 589860864 0.6451018995719747\n",
            "torch.int8 320864256 0.3509135012733247\n",
            "torch.float32 3643392 0.00398459915470055\n",
            "torch.float16 589860864 0.6450961200341342\n",
            "torch.int8 320864256 0.35091035740123144\n",
            "torch.float32 3651584 0.003993522564634367\n",
            "torch.float16 589860864 0.645083116452623\n",
            "torch.int8 320864256 0.35090328389498343\n",
            "torch.float32 3670016 0.0040135996523936015\n",
            "torch.float16 589860864 0.645077337251336\n",
            "torch.int8 320864256 0.3509001402059639\n",
            "torch.float32 3678208 0.004022522542700107\n",
            "torch.float16 589860864 0.6417656364892266\n",
            "torch.int8 325582848 0.3542324918113149\n",
            "torch.float32 3678208 0.004001871699458544\n",
            "torch.float16 589860864 0.6417541967655828\n",
            "torch.int8 325582848 0.35422617747850926\n",
            "torch.float32 3694592 0.0040196257559080035\n",
            "torch.float16 589860864 0.6417413275639641\n",
            "torch.int8 325582848 0.35421907412317544\n",
            "torch.float32 3713024 0.004039598312860539\n",
            "torch.float16 589860864 0.6417298887069283\n",
            "torch.int8 325582848 0.35421276026870757\n",
            "torch.float32 3729408 0.004057351024364159\n",
            "torch.float16 589860864 0.6417170204801882\n",
            "torch.int8 325582848 0.3542056574514732\n",
            "torch.float32 3747840 0.004077322068338591\n",
            "torch.float16 589860864 0.6272278262687503\n",
            "torch.int8 346816512 0.36878691266398367\n",
            "torch.float32 3747840 0.0039852610672659805\n",
            "torch.float16 589860864 0.6272155330672189\n",
            "torch.int8 346816512 0.36877968471323014\n",
            "torch.float32 3766272 0.004004782219550916\n",
            "torch.float16 589860864 0.6271663650794688\n",
            "torch.int8 346816512 0.36875077574324366\n",
            "torch.float32 3840000 0.004082859177287544\n",
            "torch.float16 589860864 0.6271540742869834\n",
            "torch.int8 346816512 0.3687435492089207\n",
            "torch.float32 3858432 0.004102376504095844\n",
            "torch.float16 589860864 0.6271049159340013\n",
            "torch.int8 346816512 0.3687146459038238\n",
            "torch.float32 3932160 0.004180438162174873\n",
            "torch.float16 589860864 0.6132609389971255\n",
            "torch.int8 368050176 0.38265091025231557\n",
            "torch.float32 3932160 0.004088150750558927\n",
            "torch.float16 589860864 0.6132491871762279\n",
            "torch.int8 368050176 0.3826435775743678\n",
            "torch.float32 3950592 0.004107235249404355\n",
            "torch.float16 589860864 0.6132021843962572\n",
            "torch.int8 368050176 0.38261424967265995\n",
            "torch.float32 4024320 0.004183565931082937\n",
            "torch.float16 589860864 0.61319043482704\n",
            "torch.int8 368050176 0.382606918399673\n",
            "torch.float32 4042752 0.0042026467732870065\n",
            "torch.float16 589860864 0.6131434410524971\n",
            "torch.int8 368050176 0.3825775961170009\n",
            "torch.float32 4116480 0.0042789628305019796\n",
            "torch.float16 589860864 0.5999025219325652\n",
            "torch.int8 389283840 0.39591092004299033\n",
            "torch.float32 4116480 0.0041865580244445\n",
            "torch.float16 589860864 0.5998575427371788\n",
            "torch.int8 389283840 0.3958812356293112\n",
            "torch.float32 4190208 0.004261221633509947\n",
            "torch.float16 589860864 0.5998462989921962\n",
            "torch.int8 389283840 0.3958738152213982\n",
            "torch.float32 4208640 0.004279885786405584\n",
            "torch.float16 589860864 0.5998013282270106\n",
            "torch.int8 389283840 0.3958441363713038\n",
            "torch.float32 4282368 0.004354535401685586\n",
            "torch.float16 589860864 0.5997900865892819\n",
            "torch.int8 389283840 0.3958367173540914\n",
            "torch.float32 4300800 0.004373196056626641\n",
            "torch.float16 589863168 0.5997910241923077\n",
            "torch.int8 389283840 0.39583578999649366\n",
            "torch.float32 4300800 0.004373185811198636\n",
            "torch.float16 589865472 0.5997919617909403\n",
            "torch.int8 389283840 0.395834862643241\n",
            "torch.float32 4300800 0.004373175565818635\n",
            "torch.float16 589867776 0.5997928993851798\n",
            "torch.int8 389283840 0.3958339352943335\n",
            "torch.float32 4300800 0.00437316532048664\n",
            "torch.float16 589870080 0.5997938369750262\n",
            "torch.int8 389283840 0.39583300794977117\n",
            "torch.float32 4300800 0.004373155075202649\n",
            "torch.float16 589870080 0.5969297822930882\n",
            "torch.int8 394002432 0.3987179447323508\n",
            "torch.float32 4300800 0.004352272974560965\n",
            "torch.float16 589870080 0.5969186482092285\n",
            "torch.int8 394002432 0.39871050774534705\n",
            "torch.float32 4319232 0.004370844045424448\n",
            "torch.float16 589870080 0.596908751594482\n",
            "torch.int8 394002432 0.39870389732313555\n",
            "torch.float32 4335616 0.004387351082382516\n",
            "torch.float16 589870080 0.5968976182951408\n",
            "torch.int8 394002432 0.3986964608601493\n",
            "torch.float32 4354048 0.0044059208447099416\n",
            "torch.float16 589870080 0.5968877223777072\n",
            "torch.int8 394002432 0.3986898509037065\n",
            "torch.float32 4370432 0.004422426718586316\n",
            "torch.float16 589870080 0.5968765898628017\n",
            "torch.int8 394002432 0.3986824149646824\n",
            "torch.float32 4388864 0.00444099517251598\n",
            "torch.float16 589870080 0.5968666946426073\n",
            "torch.int8 394002432 0.3986758054739591\n",
            "torch.float32 4405248 0.004457499883433581\n",
            "torch.float16 589870080 0.5954452000632611\n",
            "torch.int8 396361728 0.4001079160115731\n",
            "torch.float32 4405248 0.004446883925165828\n",
            "torch.float16 589870080 0.595434121291619\n",
            "torch.int8 396361728 0.4001004716586196\n",
            "torch.float32 4423680 0.00446540704976138\n",
            "torch.float16 589870080 0.5954291975254408\n",
            "torch.int8 396361728 0.40009716314622534\n",
            "torch.float32 4431872 0.00447363932833391\n",
            "torch.float16 589870080 0.5954181193492671\n",
            "torch.int8 396361728 0.40008971919339553\n",
            "torch.float32 4450304 0.0044921614573373864\n",
            "torch.float16 589870080 0.5954131958477309\n",
            "torch.int8 396361728 0.4000864108588268\n",
            "torch.float32 4458496 0.004500393293442388\n",
            "torch.float16 589870080 0.5939986079245185\n",
            "torch.int8 398721024 0.4015116908561264\n",
            "torch.float32 4458496 0.004489701219355005\n",
            "torch.float16 589870080 0.5939875829171792\n",
            "torch.int8 398721024 0.40150423853337774\n",
            "torch.float32 4476928 0.004508178549443026\n",
            "torch.float16 589870080 0.5939826830452846\n",
            "torch.int8 398721024 0.4015009264787312\n",
            "torch.float32 4485120 0.004516390475984248\n",
            "torch.float16 589870080 0.5939716586290844\n",
            "torch.int8 398721024 0.4014934745555614\n",
            "torch.float32 4503552 0.004534866815354205\n",
            "torch.float16 589870080 0.5939667590199077\n",
            "torch.int8 398721024 0.40149016267849835\n",
            "torch.float32 4511744 0.004543078301593995\n",
            "torch.float16 589870080 0.5939557351947992\n",
            "torch.int8 398721024 0.40148271115487527\n",
            "torch.float32 4530176 0.004561553650325567\n",
            "torch.float16 589870080 0.5939508358483193\n",
            "torch.int8 398721024 0.40147939945538136\n",
            "torch.float32 4538368 0.004569764696299336\n",
            "torch.float16 589870080 0.5911421801385587\n",
            "torch.int8 403439616 0.4043096645222333\n",
            "torch.float32 4538368 0.004548155339208034\n",
            "torch.float16 589870080 0.5911324741374091\n",
            "torch.int8 403439616 0.40430302613607094\n",
            "torch.float32 4554752 0.004564499726519969\n",
            "torch.float16 589870080 0.5911215552670871\n",
            "torch.int8 403439616 0.4042955582122022\n",
            "torch.float32 4573184 0.004582886520710727\n",
            "torch.float16 589870080 0.5911118499432023\n",
            "torch.int8 403439616 0.4042889202892528\n",
            "torch.float32 4589568 0.004599229767544954\n",
            "torch.float16 589870080 0.5911009318347633\n",
            "torch.int8 403439616 0.4042814528864713\n",
            "torch.float32 4608000 0.004617615278765435\n",
            "torch.float16 589870080 0.5787855495772483\n",
            "torch.int8 424673280 0.4166930415517475\n",
            "torch.float32 4608000 0.004521408871004205\n",
            "torch.float16 589870080 0.5787750820620868\n",
            "torch.int8 424673280 0.41668550552958306\n",
            "torch.float32 4626432 0.004539412408330093\n",
            "torch.float16 589870080 0.5787332157873322\n",
            "torch.int8 424673280 0.41665536416655363\n",
            "torch.float32 4700160 0.004611420046114201\n",
            "torch.float16 589870080 0.5787227501650135\n",
            "torch.int8 424673280 0.4166478295071295\n",
            "torch.float32 4718592 0.004629420327856995\n",
            "torch.float16 589870080 0.5786808914606031\n",
            "torch.int8 424673280 0.41661769359432216\n",
            "torch.float32 4792320 0.004701414945074815\n",
            "torch.float16 589870080 0.56687243709536\n",
            "torch.int8 445906944 0.4285220841562674\n",
            "torch.float32 4792320 0.004605478748372583\n",
            "torch.float16 589870080 0.5668623960464436\n",
            "torch.int8 445906944 0.4285144937162898\n",
            "torch.float32 4810752 0.0046231102372665195\n",
            "torch.float16 589870080 0.5668222354076816\n",
            "torch.int8 445906944 0.42848413464518814\n",
            "torch.float32 4884480 0.0046936299471302435\n",
            "torch.float16 589870080 0.5668121961371224\n",
            "torch.int8 445906944 0.4284765455495435\n",
            "torch.float32 4902912 0.004711258313334101\n",
            "torch.float16 589870080 0.566772042610844\n",
            "torch.int8 445906944 0.4284461918550594\n",
            "torch.float32 4976640 0.004781765534096645\n",
            "torch.float16 589870080 0.5554398479602197\n",
            "torch.int8 467140608 0.4398739944286793\n",
            "torch.float32 4976640 0.0046861576111009865\n",
            "torch.float16 589870080 0.555401289471629\n",
            "torch.int8 467140608 0.43984345849133555\n",
            "torch.float32 5050368 0.00475525203703543\n",
            "torch.float16 589870080 0.5553916506859419\n",
            "torch.int8 467140608 0.4398358251694246\n",
            "torch.float32 5068800 0.004772524144633514\n",
            "torch.float16 589870080 0.5553530988885129\n",
            "torch.int8 467140608 0.439805294531067\n",
            "torch.float32 5142528 0.004841606580420127\n",
            "torch.float16 589870080 0.5553434617753985\n",
            "torch.int8 467140608 0.43979766253373015\n",
            "torch.float32 5160960 0.004858875690871387\n",
            "torch.float16 589872384 0.5553444262960047\n",
            "torch.int8 467140608 0.4397967085526872\n",
            "torch.float32 5160960 0.0048588651513080975\n",
            "torch.float16 589874688 0.5553453908124265\n",
            "torch.int8 467140608 0.43979575457578296\n",
            "torch.float32 5160960 0.0048588546117905316\n",
            "torch.float16 589876992 0.555346355324664\n",
            "torch.int8 467140608 0.43979480060301723\n",
            "torch.float32 5160960 0.004858844072318688\n",
            "torch.float16 589879296 0.5553473198327173\n",
            "torch.int8 467140608 0.43979384663439014\n",
            "torch.float32 5160960 0.004858833532892568\n",
            "torch.float16 589879296 0.5528911770295251\n",
            "torch.int8 471859200 0.4422714786724945\n",
            "torch.float32 5160960 0.004837344297980409\n",
            "torch.float16 589879296 0.5528816253196047\n",
            "torch.int8 471859200 0.4422638380208693\n",
            "torch.float32 5179392 0.004854536659525948\n",
            "torch.float16 589879296 0.5528731351878441\n",
            "torch.int8 471859200 0.44225704655216097\n",
            "torch.float32 5195776 0.004869818259994932\n",
            "torch.float16 589879296 0.5528635841012874\n",
            "torch.int8 471859200 0.44224940639918\n",
            "torch.float32 5214208 0.004887009499532606\n",
            "torch.float16 589879296 0.5528550945236007\n",
            "torch.int8 471859200 0.44224261537368925\n",
            "torch.float32 5230592 0.004902290102710079\n",
            "torch.float16 589879296 0.5528455440603467\n",
            "torch.int8 471859200 0.44223497571930365\n",
            "torch.float32 5249024 0.004919480220349719\n",
            "torch.float16 589879296 0.5528370550366796\n",
            "torch.int8 471859200 0.4422281851369871\n",
            "torch.float32 5265408 0.004934759826333307\n",
            "torch.float16 589879296 0.551617351335823\n",
            "torch.int8 474218496 0.4434587762137317\n",
            "torch.float32 5265408 0.004923872450445274\n",
            "torch.float16 589879296 0.5516078435878726\n",
            "torch.int8 474218496 0.4434511327009589\n",
            "torch.float32 5283840 0.004941023711168438\n",
            "torch.float16 589879296 0.5516036180273209\n",
            "torch.int8 474218496 0.4434477356687471\n",
            "torch.float32 5292032 0.004948646303931914\n",
            "torch.float16 589879296 0.5515941107527788\n",
            "torch.int8 474218496 0.44344009253655886\n",
            "torch.float32 5310464 0.004965796710662387\n",
            "torch.float16 589879296 0.5515898854026229\n",
            "torch.int8 474218496 0.4434366956734894\n",
            "torch.float32 5318656 0.004973418923887731\n",
            "torch.float16 589879296 0.5503756726183197\n",
            "torch.int8 476577792 0.4446618564265623\n",
            "torch.float32 5318656 0.004962470955118014\n",
            "torch.float16 589879296 0.5503662076252969\n",
            "torch.int8 476577792 0.4446542094291059\n",
            "torch.float32 5337088 0.004979582945597197\n",
            "torch.float16 589879296 0.5503620010662265\n",
            "torch.int8 476577792 0.44465081084799396\n",
            "torch.float32 5345280 0.004987188085779635\n",
            "torch.float16 589879296 0.5503525365434222\n",
            "torch.int8 476577792 0.44464316423043854\n",
            "torch.float32 5363712 0.005004299226139295\n",
            "torch.float16 589879296 0.55034833019333\n",
            "torch.int8 476577792 0.44463976581816517\n",
            "torch.float32 5371904 0.005011903988504912\n",
            "torch.float16 589879296 0.5503388661407093\n",
            "torch.int8 476577792 0.4446321195804824\n",
            "torch.float32 5390336 0.005029014278808399\n",
            "torch.float16 589879296 0.5503346599995796\n",
            "torch.int8 476577792 0.444628721337035\n",
            "torch.float32 5398528 0.005036618663385349\n",
            "torch.float16 589879296 0.547922559928168\n",
            "torch.int8 481296384 0.4470628967548143\n",
            "torch.float32 5398528 0.005014543317017678\n",
            "torch.float16 589879296 0.5479142214164918\n",
            "torch.int8 481296384 0.4470560931671229\n",
            "torch.float32 5414912 0.005029685416385285\n",
            "torch.float16 589879296 0.5479048408942186\n",
            "torch.int8 481296384 0.44704843937849065\n",
            "torch.float32 5433344 0.005046719727290713\n",
            "torch.float16 589879296 0.5478965029218408\n",
            "torch.int8 481296384 0.44704163623082543\n",
            "torch.float32 5449728 0.00506186084733382\n",
            "torch.float16 589879296 0.5478871230062488\n",
            "torch.int8 481296384 0.4470339829371986\n",
            "torch.float32 5468160 0.005078894056552629\n",
            "torch.float16 589879296 0.5372906301940221\n",
            "torch.int8 502530048 0.4577287048592263\n",
            "torch.float32 5468160 0.0049806649467516555\n",
            "torch.float16 589879296 0.5372816099029626\n",
            "torch.int8 502530048 0.4577210202916718\n",
            "torch.float32 5486592 0.004997369805365597\n",
            "torch.float16 589879296 0.5372455317672602\n",
            "torch.int8 502530048 0.4576902846015243\n",
            "torch.float32 5560320 0.005064183631215516\n",
            "torch.float16 589879296 0.5372365129903922\n",
            "torch.int8 502530048 0.45768260132394006\n",
            "torch.float32 5578752 0.0050808856856677615\n",
            "torch.float16 589879296 0.5372004409106932\n",
            "torch.int8 502530048 0.4576518707930238\n",
            "torch.float32 5652480 0.005147688296283033\n",
            "torch.float16 589879296 0.5270094468566284\n",
            "torch.int8 523763712 0.4679405193849937\n",
            "torch.float32 5652480 0.005050033758377841\n",
            "torch.float16 589879296 0.5270007684707432\n",
            "torch.int8 523763712 0.46793281370073553\n",
            "torch.float32 5670912 0.005066417828521243\n",
            "torch.float16 589879296 0.5269660577851937\n",
            "torch.int8 523763712 0.4679019935013612\n",
            "torch.float32 5744640 0.005131948713445157\n",
            "torch.float16 589879296 0.5269573808282335\n",
            "torch.int8 523763712 0.46789428908586955\n",
            "torch.float32 5763072 0.005148330085896978\n",
            "torch.float16 589879296 0.5269226758576784\n",
            "torch.int8 523763712 0.46786347396093453\n",
            "torch.float32 5836800 0.005213850181387104\n",
            "torch.float16 589879296 0.517114341575866\n",
            "torch.int8 544997376 0.4777688607854015\n",
            "torch.float32 5836800 0.005116797638732542\n",
            "torch.float16 589879296 0.5170809209640501\n",
            "torch.int8 544997376 0.47773798303487275\n",
            "torch.float32 5910528 0.005181096001077151\n",
            "torch.float16 589879296 0.5170725664860673\n",
            "torch.int8 544997376 0.47773026422085546\n",
            "torch.float32 5928960 0.005197169293077263\n",
            "torch.float16 589879296 0.5170391512736281\n",
            "torch.int8 544997376 0.47769939145888307\n",
            "torch.float32 6002688 0.005261457267488825\n",
            "torch.float16 589879296 0.5170307981453259\n",
            "torch.int8 544997376 0.47769167389185374\n",
            "torch.float32 6021120 0.005277527962820354\n",
            "torch.float16 589881600 0.5170317734814179\n",
            "torch.int8 544997376 0.47769070921350854\n",
            "torch.float32 6021120 0.0052775173050734845\n",
            "torch.float16 589883904 0.5170327488135708\n",
            "torch.int8 544997376 0.4776897445390596\n",
            "torch.float32 6021120 0.005277506647369661\n",
            "torch.float16 589886208 0.5170337241417843\n",
            "torch.int8 544997376 0.4776887798685069\n",
            "torch.float32 6021120 0.005277495989708883\n",
            "torch.float16 589888512 0.5170346994660585\n",
            "torch.int8 544997376 0.47768781520185033\n",
            "torch.float32 6021120 0.00527748533209115\n",
            "torch.float16 589888512 0.5149051417845412\n",
            "torch.int8 549715968 0.47983910974056443\n",
            "torch.float32 6021120 0.005255748474894416\n",
            "torch.float16 589888512 0.514896857595641\n",
            "torch.int8 549715968 0.4798313897208867\n",
            "torch.float32 6039552 0.005271752683472278\n",
            "torch.float16 589888512 0.5148894940959539\n",
            "torch.int8 549715968 0.4798245276897129\n",
            "torch.float32 6055936 0.005285978214333278\n",
            "torch.float16 589888512 0.514881210410546\n",
            "torch.int8 549715968 0.47981680813923866\n",
            "torch.float32 6074368 0.00530198145021527\n",
            "torch.float16 589888512 0.5148738473583871\n",
            "torch.int8 549715968 0.47980994652511566\n",
            "torch.float32 6090752 0.005316206116497132\n",
            "torch.float16 589888512 0.5148655641764258\n",
            "torch.int8 549715968 0.47980222744380224\n",
            "torch.float32 6109184 0.005332208379771929\n",
            "torch.float16 589888512 0.5148582015717545\n",
            "torch.int8 549715968 0.479795366246692\n",
            "torch.float32 6125568 0.005346432181553468\n",
            "torch.float16 589888512 0.5138001803455463\n",
            "torch.int8 552075264 0.4808643742624964\n",
            "torch.float32 6125568 0.005335445391957231\n",
            "torch.float16 589888512 0.5137919316731463\n",
            "torch.int8 552075264 0.4808566543494955\n",
            "torch.float32 6144000 0.005351413977358168\n",
            "torch.float16 589888512 0.5137882656815378\n",
            "torch.int8 552075264 0.4808532233566148\n",
            "torch.float32 6152192 0.005358510961847366\n",
            "torch.float16 589888512 0.5137800173916921\n",
            "torch.int8 552075264 0.48084550380164554\n",
            "torch.float32 6170624 0.005374478806662356\n",
            "torch.float16 589888512 0.5137763515701018\n",
            "torch.int8 552075264 0.48084207296788445\n",
            "torch.float32 6178816 0.005381575462013694\n",
            "torch.float16 589888512 0.5127227674745024\n",
            "torch.int8 554434560 0.48190669288149834\n",
            "torch.float32 6178816 0.005370539643999263\n",
            "torch.float16 589888512 0.5127145533596487\n",
            "torch.int8 554434560 0.4818989724579571\n",
            "torch.float32 6197248 0.0053864741823942755\n",
            "torch.float16 589888512 0.5127109027264158\n",
            "torch.int8 554434560 0.48189554123800804\n",
            "torch.float32 6205440 0.00539355603557611\n",
            "torch.float16 589888512 0.5127026889917149\n",
            "torch.int8 554434560 0.4818878211717713\n",
            "torch.float32 6223872 0.00540948983651379\n",
            "torch.float16 589888512 0.512699038527433\n",
            "torch.int8 554434560 0.48188439011061873\n",
            "torch.float32 6232064 0.005416571361948186\n",
            "torch.float16 589888512 0.5126908251728585\n",
            "torch.int8 554434560 0.48187667040166177\n",
            "torch.float32 6250496 0.005432504425479727\n",
            "torch.float16 589888512 0.5126871748775159\n",
            "torch.int8 554434560 0.48187323949929467\n",
            "torch.float32 6258688 0.005439585623189438\n",
            "torch.float16 589888512 0.5105932071939678\n",
            "torch.int8 559153152 0.48398942407662315\n",
            "torch.float32 6258688 0.005417368729408991\n",
            "torch.float16 589888512 0.5105859662713308\n",
            "torch.int8 559153152 0.4839825604326743\n",
            "torch.float32 6275072 0.005431473295994909\n",
            "torch.float16 589888512 0.5105778204788448\n",
            "torch.int8 559153152 0.48397483906592204\n",
            "torch.float32 6293504 0.00544734045523316\n",
            "torch.float16 589888512 0.5105705799926081\n",
            "torch.int8 559153152 0.4839679758356355\n",
            "torch.float32 6309888 0.005461444171756304\n",
            "torch.float16 589888512 0.5105624346910504\n",
            "torch.int8 559153152 0.4839602549342324\n",
            "torch.float32 6328320 0.005477310374717161\n",
            "torch.float16 589888512 0.5013485322799203\n",
            "torch.int8 580386816 0.49327300402862595\n",
            "torch.float32 6328320 0.005378463691453726\n",
            "torch.float16 589888512 0.5013406785635027\n",
            "torch.int8 580386816 0.49326527681683485\n",
            "torch.float32 6346752 0.005394044619662415\n",
            "torch.float16 589888512 0.5013092661582763\n",
            "torch.int8 580386816 0.49323437039048246\n",
            "torch.float32 6420480 0.0054563634512412576\n",
            "torch.float16 589888512 0.5013014136720229\n",
            "torch.int8 580386816 0.49322664438904046\n",
            "torch.float32 6438912 0.0054719419389366785\n",
            "torch.float16 589888512 0.5012700061868749\n",
            "torch.int8 580386816 0.4931957428035158\n",
            "torch.float32 6512640 0.005534251009609235\n",
            "torch.float16 589888512 0.4923855264743667\n",
            "torch.int8 601620480 0.5021783112510609\n",
            "torch.float32 6512640 0.0054361622745723506\n",
            "torch.float16 589888512 0.4923779510599071\n",
            "torch.int8 601620480 0.5021705851733519\n",
            "torch.float32 6531072 0.005451463766740943\n",
            "torch.float16 589888512 0.49234765173289985\n",
            "torch.int8 601620480 0.5021396832397035\n",
            "torch.float32 6604800 0.005512665027396663\n",
            "torch.float16 589888512 0.492340077483802\n",
            "torch.int8 601620480 0.5021319583505335\n",
            "torch.float32 6623232 0.005527964165664574\n",
            "torch.float16 589888512 0.49230978281770377\n",
            "torch.int8 601620480 0.5021010611704924\n",
            "torch.float32 6696960 0.005589156011803887\n",
            "torch.float16 589888512 0.48373736941149204\n",
            "torch.int8 622854144 0.5107707965426638\n",
            "torch.float32 6696960 0.005491834045844218\n",
            "torch.float16 589888512 0.48370812414825043\n",
            "torch.int8 622854144 0.510739916922139\n",
            "torch.float32 6770688 0.005551958929610532\n",
            "torch.float16 589888512 0.483700813384956\n",
            "torch.int8 622854144 0.510732197600401\n",
            "torch.float32 6789120 0.0055669890146429445\n",
            "torch.float16 589888512 0.4836715725415421\n",
            "torch.int8 622854144 0.510701322646704\n",
            "torch.float32 6862848 0.005627104811753949\n",
            "torch.float16 589888512 0.4836642628830794\n",
            "torch.int8 622854144 0.5106936044915406\n",
            "torch.float32 6881280 0.005642132625380026\n",
            "torch.float16 589890816 0.4836652382946435\n",
            "torch.int8 622854144 0.5106926397385481\n",
            "torch.float32 6881280 0.005642121966808454\n",
            "torch.float16 589893120 0.4836662137025222\n",
            "torch.int8 622854144 0.5106916749892007\n",
            "torch.float32 6881280 0.005642111308277154\n",
            "torch.float16 589895424 0.4836671891067157\n",
            "torch.int8 622854144 0.5106907102434982\n",
            "torch.float32 6881280 0.005642100649786123\n",
            "torch.float16 589897728 0.4836681645072239\n",
            "torch.int8 622854144 0.5106897455014408\n",
            "torch.float32 6881280 0.005642089991335362\n",
            "torch.float16 589897728 0.48180413095405367\n",
            "torch.int8 627572736 0.5125755233946888\n",
            "torch.float32 6881280 0.005620345651257552\n",
            "torch.float16 589897728 0.4817968777442681\n",
            "torch.int8 627572736 0.5125678069440333\n",
            "torch.float32 6899712 0.005635315311698674\n",
            "torch.float16 589897728 0.4817904306300128\n",
            "torch.int8 627572736 0.5125609480718246\n",
            "torch.float32 6916096 0.005648621298162567\n",
            "torch.float16 589897728 0.481783177832715\n",
            "torch.int8 627572736 0.5125532320600013\n",
            "torch.float32 6934528 0.005663590107283718\n",
            "torch.float16 589897728 0.4817767310850999\n",
            "torch.int8 627572736 0.512546373577849\n",
            "torch.float32 6950912 0.0056768953370510255\n",
            "torch.float16 589897728 0.4817694787002547\n",
            "torch.int8 627572736 0.5125386580048205\n",
            "torch.float32 6969344 0.005691863294924825\n",
            "torch.float16 589897728 0.48176303231924855\n",
            "torch.int8 627572736 0.5125317999126914\n",
            "torch.float32 6985728 0.005705167768060093\n",
            "torch.float16 589897728 0.4808365510299133\n",
            "torch.int8 629932032 0.513469252843342\n",
            "torch.float32 6985728 0.005694196126744692\n",
            "torch.float16 589897728 0.48082932692307695\n",
            "torch.int8 629932032 0.5134615384615384\n",
            "torch.float32 7004160 0.005709134615384615\n",
            "torch.float16 589897728 0.4808261162786039\n",
            "torch.int8 629932032 0.5134581099218087\n",
            "torch.float32 7012352 0.005715773799587343\n",
            "torch.float16 589897728 0.48081889248530596\n",
            "torch.int8 629932032 0.5134503958748224\n",
            "torch.float32 7030784 0.005730711639871597\n",
            "torch.float16 589897728 0.4808156819801788\n",
            "torch.int8 629932032 0.5134469674838955\n",
            "torch.float32 7038976 0.00573735053592563\n",
            "torch.float16 589897728 0.47989283732833066\n",
            "torch.int8 632291328 0.5143808240129688\n",
            "torch.float32 7038976 0.0057263386587005525\n",
            "torch.float16 589897728 0.4798856415502368\n",
            "torch.int8 632291328 0.5143731111029659\n",
            "torch.float32 7057408 0.00574124734679733\n",
            "torch.float16 589897728 0.47988244349590653\n",
            "torch.int8 632291328 0.5143696832172097\n",
            "torch.float32 7065600 0.005747873286883853\n",
            "torch.float16 589897728 0.47987524802950865\n",
            "torch.int8 632291328 0.5143619706413031\n",
            "torch.float32 7084032 0.005762781329188262\n",
            "torch.float16 589897728 0.4798720501137055\n",
            "torch.int8 632291328 0.5143585429040293\n",
            "torch.float32 7092224 0.005769406982265279\n",
            "torch.float16 589897728 0.47986485495898334\n",
            "torch.int8 632291328 0.5143508306621973\n",
            "torch.float32 7110656 0.00578431437881928\n",
            "torch.float16 589897728 0.4798616571816982\n",
            "torch.int8 632291328 0.5143474030733963\n",
            "torch.float32 7118848 0.005790939744905439\n",
            "torch.float16 589897728 0.47802679270834714\n",
            "torch.int8 637009920 0.5162044105736933\n",
            "torch.float32 7118848 0.0057687967179596115\n",
            "torch.float16 589897728 0.4780204460966543\n",
            "torch.int8 637009920 0.5161975570897503\n",
            "torch.float32 7135232 0.005781996813595327\n",
            "torch.float16 589897728 0.47801330635993555\n",
            "torch.int8 637009920 0.5161898471378381\n",
            "torch.float32 7153664 0.005796846502226301\n",
            "torch.float16 589897728 0.47800696010634325\n",
            "torch.int8 637009920 0.5161829940405956\n",
            "torch.float32 7170048 0.0058100458530611025\n",
            "torch.float16 589897728 0.47799982077247066\n",
            "torch.int8 637009920 0.5161752845237029\n",
            "torch.float32 7188480 0.005824894703826508\n",
            "torch.float16 589897728 0.4699145449740111\n",
            "torch.int8 658243584 0.5243590873050833\n",
            "torch.float32 7188480 0.005726367720905647\n",
            "torch.float16 589897728 0.4699076453228009\n",
            "torch.int8 658243584 0.5243513882567137\n",
            "torch.float32 7206912 0.005740966420485412\n",
            "torch.float16 589897728 0.46988004874396205\n",
            "torch.int8 658243584 0.5243205943239712\n",
            "torch.float32 7280640 0.005799356932066773\n",
            "torch.float16 589897728 0.46987315010570824\n",
            "torch.int8 658243584 0.5243128964059197\n",
            "torch.float32 7299072 0.005813953488372093\n",
            "torch.float16 589897728 0.46984555757824886\n",
            "torch.int8 658243584 0.5242821069939515\n",
            "torch.float32 7372800 0.005872335427799636\n",
            "torch.float16 589897728 0.4620315297106889\n",
            "torch.int8 679477248 0.5321937980019634\n",
            "torch.float32 7372800 0.005774672287347693\n",
            "torch.float16 589897728 0.46202485960530687\n",
            "torch.int8 679477248 0.5321861150009384\n",
            "torch.float32 7391232 0.005789025393754782\n",
            "torch.float16 589897728 0.4619981811095232\n",
            "torch.int8 679477248 0.5321553852150188\n",
            "torch.float32 7464960 0.005846433675457971\n",
            "torch.float16 589897728 0.46199151196697175\n",
            "torch.int8 679477248 0.5321477033230361\n",
            "torch.float32 7483392 0.005860784709992205\n",
            "torch.float16 589897728 0.4619648373220936\n",
            "torch.int8 679477248 0.5321169779728052\n",
            "torch.float32 7557120 0.005918184705101187\n",
            "torch.float16 589897728 0.4544086326849354\n",
            "torch.int8 700710912 0.5397699843816556\n",
            "torch.float32 7557120 0.005821382933409058\n",
            "torch.float16 589897728 0.4543828264758497\n",
            "torch.int8 700710912 0.5397393304370048\n",
            "torch.float32 7630848 0.005877843087145413\n",
            "torch.float16 589897728 0.45437637538155745\n",
            "torch.int8 700710912 0.5397316674948535\n",
            "torch.float32 7649280 0.005891957123589125\n",
            "torch.float16 589897728 0.45435057283607094\n",
            "torch.int8 700710912 0.5397010179020145\n",
            "torch.float32 7723008 0.005948409261914565\n",
            "torch.float16 589897728 0.4543441226575809\n",
            "torch.int8 700710912 0.5396933560477002\n",
            "torch.float32 7741440 0.00596252129471891\n",
            "torch.float16 589900032 0.4543450909549067\n",
            "torch.int8 700710912 0.5396923983312069\n",
            "torch.float32 7741440 0.005962510713886439\n",
            "torch.float16 589902336 0.45434605924879595\n",
            "torch.int8 700710912 0.5396914406181125\n",
            "torch.float32 7741440 0.005962500133091521\n",
            "torch.float16 589904640 0.4543470275392486\n",
            "torch.int8 700710912 0.5396904829084173\n",
            "torch.float32 7741440 0.005962489552334155\n",
            "torch.float16 589906944 0.45434799582626473\n",
            "torch.int8 700710912 0.539689525202121\n",
            "torch.float32 7741440 0.005962478971614341\n",
            "torch.float16 589906944 0.45270275048976966\n",
            "torch.int8 705429504 0.54135636134745\n",
            "torch.float32 7741440 0.005940888162780335\n",
            "torch.float16 589906944 0.45269634711269846\n",
            "torch.int8 705429504 0.5413487039852894\n",
            "torch.float32 7759872 0.005954948902012094\n",
            "torch.float16 589906944 0.452690655374042\n",
            "torch.int8 705429504 0.541341897623001\n",
            "torch.float32 7776256 0.005967447002957006\n",
            "torch.float16 589906944 0.4526842523391291\n",
            "torch.int8 705429504 0.5413342406700042\n",
            "torch.float32 7794688 0.00598150699086665\n",
            "torch.float16 589906944 0.4526785609046012\n",
            "torch.int8 705429504 0.5413274346714023\n",
            "torch.float32 7811072 0.005994004423996448\n",
            "torch.float16 589906944 0.4526721582118192\n",
            "torch.int8 705429504 0.5413197781275366\n",
            "torch.float32 7829504 0.0060080636606441975\n",
            "torch.float16 589906944 0.4526664670813954\n",
            "torch.int8 705429504 0.5413129724925921\n",
            "torch.float32 7845888 0.0060205604260124045\n",
            "torch.float16 589906944 0.4518484355771199\n",
            "torch.int8 707788800 0.5421418839901078\n",
            "torch.float32 7845888 0.006009680432772288\n",
            "torch.float16 589906944 0.45184205634525293\n",
            "torch.int8 707788800 0.5421342299882114\n",
            "torch.float32 7864320 0.0060237136665356825\n",
            "torch.float16 589906944 0.45183922118890735\n",
            "torch.int8 707788800 0.5421308282789621\n",
            "torch.float32 7872512 0.006029950532130585\n",
            "torch.float16 589906944 0.45183284221721476\n",
            "torch.int8 707788800 0.5421231745892311\n",
            "torch.float32 7890944 0.00604398319355413\n",
            "torch.float16 589906944 0.45183000717649874\n",
            "torch.int8 707788800 0.5421197730187177\n",
            "torch.float32 7899136 0.006050219804783548\n",
            "torch.float16 589906944 0.4510149933570761\n",
            "torch.int8 710148096 0.5429457002628202\n",
            "torch.float32 7899136 0.006039306380103641\n",
            "torch.float16 589906944 0.451008637636546\n",
            "torch.int8 710148096 0.5429380490512535\n",
            "torch.float32 7917568 0.006053313312200495\n",
            "torch.float16 589906944 0.4510058129293641\n",
            "torch.int8 710148096 0.5429346485819976\n",
            "torch.float32 7925760 0.006059538488638366\n",
            "torch.float16 589906944 0.45099945746757136\n",
            "torch.int8 710148096 0.5429269976819069\n",
            "torch.float32 7944192 0.006073544850521747\n",
            "torch.float16 589906944 0.4509966328753804\n",
            "torch.int8 710148096 0.5429235973510805\n",
            "torch.float32 7952384 0.006079769773539145\n",
            "torch.float16 589906944 0.45099027767230926\n",
            "torch.int8 710148096 0.5429159467624469\n",
            "torch.float32 7970816 0.006093775565243873\n",
            "torch.float16 589906944 0.45098745319510214\n",
            "torch.int8 710148096 0.5429125465700416\n",
            "torch.float32 7979008 0.006100000234856272\n",
            "torch.float16 589906944 0.449366412243513\n",
            "torch.int8 714866688 0.5445555135200489\n",
            "torch.float32 7979008 0.006078074236438024\n",
            "torch.float16 589906944 0.4493608039307302\n",
            "torch.int8 714866688 0.5445487172006886\n",
            "torch.float32 7995392 0.006090478868581226\n",
            "torch.float16 589906944 0.4493544947461777\n",
            "torch.int8 714866688 0.5445410715441815\n",
            "torch.float32 8013824 0.006104433709640809\n",
            "torch.float16 589906944 0.4493488867308615\n",
            "torch.int8 714866688 0.5445342755853\n",
            "torch.float32 8030208 0.006116837683838585\n",
            "torch.float16 589906944 0.4493425778809456\n",
            "torch.int8 714866688 0.5445266303343154\n",
            "torch.float32 8048640 0.006130791784739008\n",
            "torch.float16 589906944 0.44219056194057516\n",
            "torch.int8 736100352 0.5517762277698077\n",
            "torch.float32 8048640 0.006033210289617122\n",
            "torch.float16 589906944 0.44218445249373084\n",
            "torch.int8 736100352 0.5517686042522031\n",
            "torch.float32 8067072 0.006046943254066029\n",
            "torch.float16 589906944 0.4421600163944618\n",
            "torch.int8 736100352 0.5517381122882478\n",
            "torch.float32 8140800 0.006101871317290401\n",
            "torch.float16 589906944 0.44215390779163666\n",
            "torch.int8 736100352 0.551730489823831\n",
            "torch.float32 8159232 0.006115602384532315\n",
            "torch.float16 589906944 0.44212947506809447\n",
            "torch.int8 736100352 0.5517000020721905\n",
            "torch.float32 8232960 0.006170522859715004\n",
            "torch.float16 589906944 0.43520346300724105\n",
            "torch.int8 757334016 0.5587226761249674\n",
            "torch.float32 8232960 0.0060738608677915395\n",
            "torch.float16 589906944 0.4351975451057359\n",
            "torch.int8 757334016 0.5587150786078356\n",
            "torch.float32 8251392 0.006087376286428505\n",
            "torch.float16 589906944 0.43517387510906147\n",
            "torch.int8 757334016 0.5586846906054185\n",
            "torch.float32 8325120 0.006141434285520039\n",
            "torch.float16 589906944 0.4351679580121966\n",
            "torch.int8 757334016 0.5586770941212996\n",
            "torch.float32 8343552 0.006154947866503804\n",
            "torch.float16 589906944 0.4351442912337552\n",
            "torch.int8 757334016 0.5586467102505127\n",
            "torch.float32 8417280 0.006208998515732107\n",
            "torch.float16 589906944 0.4284337370902284\n",
            "torch.int8 778567680 0.5654530161287084\n",
            "torch.float32 8417280 0.006113246781063214\n",
            "torch.float16 589906944 0.4284107971113109\n",
            "torch.int8 778567680 0.5654227396141721\n",
            "torch.float32 8491008 0.006166463274516934\n",
            "torch.float16 589906944 0.4284050625004183\n",
            "torch.int8 778567680 0.5654151709921313\n",
            "torch.float32 8509440 0.006179766507450299\n",
            "torch.float16 589906944 0.42838212559201\n",
            "torch.int8 778567680 0.5653848985300974\n",
            "torch.float32 8583168 0.006232975877892567\n",
            "torch.float16 589906944 0.42837639174866765\n",
            "torch.int8 778567680 0.5653773309210806\n",
            "torch.float32 8601600 0.006246277330251837\n",
            "torch.float16 589909248 0.42837734813624223\n",
            "torch.int8 778567680 0.5653763849841975\n",
            "torch.float32 8601600 0.006246266879560263\n",
            "torch.float16 589911552 0.42837830452061654\n",
            "torch.int8 778567680 0.5653754390504798\n",
            "torch.float32 8601600 0.0062462564289036595\n",
            "torch.float16 589913856 0.4283792609017906\n",
            "torch.int8 778567680 0.5653744931199274\n",
            "torch.float32 8601600 0.006246245978282026\n",
            "torch.float16 589916160 0.42838021727976444\n",
            "torch.int8 778567680 0.5653735471925402\n",
            "torch.float32 8601600 0.006246235527695362\n",
            "torch.float16 589916160 0.4269173821603091\n",
            "torch.int8 783286272 0.5668577119913919\n",
            "torch.float32 8601600 0.006224905848299044\n",
            "torch.float16 589916160 0.4269116875494651\n",
            "torch.int8 783286272 0.5668501507296382\n",
            "torch.float32 8620032 0.00623816172089673\n",
            "torch.float16 589916160 0.4269066258006954\n",
            "torch.int8 783286272 0.5668434297774207\n",
            "torch.float32 8636416 0.006249944421883846\n",
            "torch.float16 589916160 0.426900931476802\n",
            "torch.int8 783286272 0.5668358688966779\n",
            "torch.float32 8654848 0.006263199626520041\n",
            "torch.float16 589916160 0.42689586998308987\n",
            "torch.int8 783286272 0.5668291482831241\n",
            "torch.float32 8671232 0.006274981733786049\n",
            "torch.float16 589916160 0.4268901759461254\n",
            "torch.int8 783286272 0.5668215877833633\n",
            "torch.float32 8689664 0.006288236270511239\n",
            "torch.float16 589916160 0.42688511470745155\n",
            "torch.int8 783286272 0.5668148675084475\n",
            "torch.float32 8706048 0.006300017784101014\n",
            "torch.float16 589916160 0.4261575471363136\n",
            "torch.int8 785645568 0.5675531726023506\n",
            "torch.float32 8706048 0.006289280261335795\n",
            "torch.float16 589916160 0.42615187277803174\n",
            "torch.int8 785645568 0.5675456155379105\n",
            "torch.float32 8724480 0.0063025116840577525\n",
            "torch.float16 589916160 0.42614935088952177\n",
            "torch.int8 785645568 0.5675422569072013\n",
            "torch.float32 8732672 0.006308392203276991\n",
            "torch.float16 589916160 0.42614367674950515\n",
            "torch.int8 785645568 0.567534700133445\n",
            "torch.float32 8751104 0.00632162311704989\n",
            "torch.float16 589916160 0.4261411549579991\n",
            "torch.int8 785645568 0.5675313416319249\n",
            "torch.float32 8759296 0.006327503410076072\n",
            "torch.float16 589916160 0.42541611897975157\n",
            "torch.int8 788004864 0.5682671432158206\n",
            "torch.float32 8759296 0.006316737804427772\n",
            "torch.float16 589916160 0.4254104643486403\n",
            "torch.int8 788004864 0.5682595898088758\n",
            "torch.float32 8777728 0.006329945842483891\n",
            "torch.float16 589916160 0.42540795122750896\n",
            "torch.int8 788004864 0.5682562328035764\n",
            "torch.float32 8785920 0.006335815968914626\n",
            "torch.float16 589916160 0.4254022968135257\n",
            "torch.int8 788004864 0.5682486796866693\n",
            "torch.float32 8804352 0.006349023499805055\n",
            "torch.float16 589916160 0.42539978378889287\n",
            "torch.int8 788004864 0.5682453228102718\n",
            "torch.float32 8812544 0.006354893400835307\n",
            "torch.float16 589916160 0.4253941295920251\n",
            "torch.int8 788004864 0.5682377699833856\n",
            "torch.float32 8830976 0.006368100424589256\n",
            "torch.float16 589916160 0.4253916166638853\n",
            "torch.int8 788004864 0.5682344132358826\n",
            "torch.float32 8839168 0.006373970100232009\n",
            "torch.float16 589916160 0.4239490869592734\n",
            "torch.int8 792723456 0.5696985574736582\n",
            "torch.float32 8839168 0.006352355567068423\n",
            "torch.float16 589916160 0.4239440952194525\n",
            "torch.int8 792723456 0.5696918496234404\n",
            "torch.float32 8855552 0.0063640551571070925\n",
            "torch.float16 589916160 0.42393847965266024\n",
            "torch.int8 792723456 0.5696843034807565\n",
            "torch.float32 8873984 0.006377216866583266\n",
            "torch.float16 589916160 0.4239334881626237\n",
            "torch.int8 792723456 0.5696775959661966\n",
            "torch.float32 8890368 0.0063889158711796755\n",
            "torch.float16 589916160 0.4239278728768285\n",
            "torch.int8 792723456 0.5696700502011135\n",
            "torch.float32 8908800 0.006402076922058025\n",
            "torch.float16 589916160 0.4175563721759551\n",
            "torch.int8 813957120 0.5761377720759312\n",
            "torch.float32 8908800 0.0063058557481136795\n",
            "torch.float16 589916160 0.41755092455685633\n",
            "torch.int8 813957120 0.5761302555360343\n",
            "torch.float32 8927232 0.006318819907109433\n",
            "torch.float16 589916160 0.4175291355018264\n",
            "torch.int8 813957120 0.5761001913376239\n",
            "torch.float32 9000960 0.006370673160549661\n",
            "torch.float16 589916160 0.4175236885933824\n",
            "torch.int8 813957120 0.5760926757782774\n",
            "torch.float32 9019392 0.006383635628340211\n",
            "torch.float16 589916160 0.41750190238069357\n",
            "torch.int8 813957120 0.5760626155016849\n",
            "torch.float32 9093120 0.0064354821176214804\n",
            "torch.float16 589916160 0.4113206900540202\n",
            "torch.int8 835190784 0.5823391066310816\n",
            "torch.float32 9093120 0.006340203314898193\n",
            "torch.float16 589916160 0.4113154039257349\n",
            "torch.int8 835190784 0.5823316226428027\n",
            "torch.float32 9111552 0.006352973431462426\n",
            "torch.float16 589916160 0.4112942607712276\n",
            "torch.int8 835190784 0.5823016886132125\n",
            "torch.float32 9185280 0.006404050615559915\n",
            "torch.float16 589916160 0.41128897532223313\n",
            "torch.int8 835190784 0.5822942055866592\n",
            "torch.float32 9203712 0.006416819091107694\n",
            "torch.float16 589916160 0.4112678348846274\n",
            "torch.int8 835190784 0.5822642754036006\n",
            "torch.float32 9277440 0.006467889711771988\n",
            "torch.float16 589916160 0.40526851175728207\n",
            "torch.int8 856424448 0.5883579481421763\n",
            "torch.float32 9277440 0.00637354010054154\n",
            "torch.float16 589916160 0.4052479856834748\n",
            "torch.int8 856424448 0.5883281489391337\n",
            "torch.float32 9351168 0.006423865377391539\n",
            "torch.float16 589916160 0.40524285448989583\n",
            "torch.int8 856424448 0.5883206996100147\n",
            "torch.float32 9369600 0.006436445900089477\n",
            "torch.float16 589916160 0.40522233101492333\n",
            "torch.int8 856424448 0.5882909041798906\n",
            "torch.float32 9443328 0.006486764805186036\n",
            "torch.float16 589916160 0.4052172004709914\n",
            "torch.int8 856424448 0.5882834557939117\n",
            "torch.float32 9461760 0.006499343735096878\n",
            "torch.float16 589918464 0.4052181417920329\n",
            "torch.int8 856424448 0.588282524758926\n",
            "torch.float32 9461760 0.006499333449041164\n",
            "torch.float16 589920768 0.40521908311009497\n",
            "torch.int8 856424448 0.588281593726887\n",
            "torch.float32 9461760 0.0064993231630180075\n",
            "torch.float16 589923072 0.40522002442517746\n",
            "torch.int8 856424448 0.5882806626977951\n",
            "torch.float32 9461760 0.006499312877027408\n",
            "torch.float16 589925376 0.40522096573728045\n",
            "torch.int8 856424448 0.5882797316716502\n",
            "torch.float32 9461760 0.006499302591069367\n",
            "torch.float16 589925376 0.4039118025042435\n",
            "torch.int8 861143040 0.5896098924559297\n",
            "torch.float32 9461760 0.006478305039826853\n",
            "torch.float16 589925376 0.403906705171431\n",
            "torch.int8 861143040 0.5896024516289156\n",
            "torch.float32 9480192 0.006490843199653373\n",
            "torch.float16 589925376 0.4039021743169372\n",
            "torch.int8 861143040 0.5895958377181205\n",
            "torch.float32 9496576 0.006501987964942268\n",
            "torch.float16 589925376 0.40389707722713414\n",
            "torch.int8 861143040 0.5895883972458392\n",
            "torch.float32 9515008 0.006514525527026658\n",
            "torch.float16 589925376 0.40389254658864093\n",
            "torch.int8 861143040 0.5895817836503509\n",
            "torch.float32 9531392 0.006525669761008212\n",
            "torch.float16 589925376 0.4038874497418299\n",
            "torch.int8 861143040 0.5895743435327769\n",
            "torch.float32 9549824 0.006538206725393215\n",
            "torch.float16 589925376 0.40388291931932185\n",
            "torch.int8 861143040 0.5895677302525727\n",
            "torch.float32 9566208 0.006549350428105421\n",
            "torch.float16 589925376 0.4032315978456014\n",
            "torch.int8 863502336 0.5902296135311348\n",
            "torch.float32 9566208 0.006538788623263725\n",
            "torch.float16 589925376 0.4032265176664756\n",
            "torch.int8 863502336 0.590222177426975\n",
            "torch.float32 9584640 0.006551304906549415\n",
            "torch.float16 589925376 0.40322425985017474\n",
            "torch.int8 863502336 0.590218872551936\n",
            "torch.float32 9592832 0.006556867597889315\n",
            "torch.float16 589925376 0.4032191798559439\n",
            "torch.int8 863502336 0.5902114367184159\n",
            "torch.float32 9611264 0.006569383425640193\n",
            "torch.float16 589925376 0.40321692212181637\n",
            "torch.int8 863502336 0.590208131963658\n",
            "torch.float32 9619456 0.006574945914525703\n",
            "torch.float16 589925376 0.40256774519045657\n",
            "torch.int8 865861632 0.5908678945202196\n",
            "torch.float32 9619456 0.006564360289323796\n",
            "torch.float16 589925376 0.4025626817247685\n",
            "torch.int8 865861632 0.5908604626299456\n",
            "torch.float32 9637888 0.0065768556452858295\n",
            "torch.float16 589925376 0.40256043133645963\n",
            "torch.int8 865861632 0.5908571596276118\n",
            "torch.float32 9646080 0.006582409035928633\n",
            "torch.float16 589925376 0.4025553680547549\n",
            "torch.int8 865861632 0.5908497280073789\n",
            "torch.float32 9664512 0.0065949039378661945\n",
            "torch.float16 589925376 0.4025531177482142\n",
            "torch.int8 865861632 0.59084642512506\n",
            "torch.float32 9672704 0.006600457126725842\n",
            "torch.float16 589925376 0.4025480546504827\n",
            "torch.int8 865861632 0.5908389937748536\n",
            "torch.float32 9691136 0.006612951574663675\n",
            "torch.float16 589925376 0.4025458044257057\n",
            "torch.int8 865861632 0.5908356910125432\n",
            "torch.float32 9699328 0.0066185045617511646\n",
            "torch.float16 589925376 0.4012538420766333\n",
            "torch.int8 870580224 0.5921488953137287\n",
            "torch.float32 9699328 0.006597262609638049\n",
            "torch.float16 589925376 0.4012493705437457\n",
            "torch.int8 870580224 0.5921422964653638\n",
            "torch.float32 9715712 0.006608332990890558\n",
            "torch.float16 589925376 0.40124434018837135\n",
            "torch.int8 870580224 0.5921348729367502\n",
            "torch.float32 9734144 0.006620786874878551\n",
            "torch.float16 589925376 0.4012398688672563\n",
            "torch.int8 870580224 0.5921282744009077\n",
            "torch.float32 9750528 0.006631856731836047\n",
            "torch.float16 589925376 0.4012348387501175\n",
            "torch.int8 870580224 0.5921208512238694\n",
            "torch.float32 9768960 0.0066443100260131\n",
            "torch.float16 589925376 0.3955227108422568\n",
            "torch.int8 891813888 0.5979275699923998\n",
            "torch.float32 9768960 0.006549719165343335\n",
            "torch.float16 589925376 0.39551782304854766\n",
            "torch.int8 891813888 0.5979201809183087\n",
            "torch.float32 9787392 0.006561996033143641\n",
            "torch.float16 589925376 0.3954982730816996\n",
            "torch.int8 891813888 0.5978906264481091\n",
            "torch.float32 9861120 0.006611100470191351\n",
            "torch.float16 589925376 0.39549338589196237\n",
            "torch.int8 891813888 0.5978832382870665\n",
            "torch.float32 9879552 0.00662337582097114\n",
            "torch.float16 589925376 0.3954738383407781\n",
            "torch.int8 891813888 0.5978536874687228\n",
            "torch.float32 9953280 0.0066724741904991385\n",
            "torch.float16 589925376 0.389923429761883\n",
            "torch.int8 913047552 0.6034977430968002\n",
            "torch.float32 9953280 0.006578827141316862\n",
            "torch.float16 589925376 0.3899186793774556\n",
            "torch.int8 913047552 0.6034903907653885\n",
            "torch.float32 9971712 0.00659092985715591\n",
            "torch.float16 589925376 0.38989967899715544\n",
            "torch.int8 913047552 0.6034609832311038\n",
            "torch.float32 10045440 0.006639337771740785\n",
            "torch.float16 589925376 0.3898949291914116\n",
            "torch.int8 913047552 0.6034536317953403\n",
            "torch.float32 10063872 0.006651439013248058\n",
            "torch.float16 589925376 0.3898759311256342\n",
            "torch.int8 913047552 0.6034242278433212\n",
            "torch.float32 10137600 0.006699841031044627\n",
            "torch.float16 589925376 0.38448046994660245\n",
            "torch.int8 934281216 0.6089124075753662\n",
            "torch.float32 10137600 0.006607122478031318\n",
            "torch.float16 589925376 0.38446199585573143\n",
            "torch.int8 934281216 0.6088831496441335\n",
            "torch.float32 10211328 0.006654854500135139\n",
            "torch.float16 589925376 0.3844573776104074\n",
            "torch.int8 934281216 0.6088758356006414\n",
            "torch.float32 10229760 0.006666786788951152\n",
            "torch.float16 589925376 0.3844389057385664\n",
            "torch.int8 934281216 0.6088465811837482\n",
            "torch.float32 10303488 0.006714513077685355\n",
            "torch.float16 589925376 0.38443428804794993\n",
            "torch.int8 934281216 0.608839268018762\n",
            "torch.float32 10321920 0.00672644393328809\n",
            "torch.float16 589927680 0.38443521228054633\n",
            "torch.int8 934281216 0.6088383538854915\n",
            "torch.float32 10321920 0.0067264338339621844\n",
            "torch.float16 589929984 0.3844361365103674\n",
            "torch.int8 934281216 0.608837439754966\n",
            "torch.float32 10321920 0.006726423734666607\n",
            "torch.float16 589932288 0.3844370607374131\n",
            "torch.int8 934281216 0.6088365256271855\n",
            "torch.float32 10321920 0.006726413635401355\n",
            "torch.float16 589934592 0.3844379849616835\n",
            "torch.int8 934281216 0.60883561150215\n",
            "torch.float32 10321920 0.00672640353616643\n",
            "torch.float16 589934592 0.3832594898814513\n",
            "torch.int8 938999808 0.6100347263800743\n",
            "torch.float32 10321920 0.006705783738474434\n",
            "torch.float16 589934592 0.3832549005520231\n",
            "torch.int8 938999808 0.6100274215372825\n",
            "torch.float32 10340352 0.006717677910694399\n",
            "torch.float16 589934592 0.38325082124035564\n",
            "torch.int8 938999808 0.6100209284905542\n",
            "torch.float32 10356736 0.006728250269090096\n",
            "torch.float16 589934592 0.3832462321185286\n",
            "torch.int8 938999808 0.6100136239782016\n",
            "torch.float32 10375168 0.006740143903269755\n",
            "torch.float16 589934592 0.3832421529913893\n",
            "torch.int8 938999808 0.610007131225187\n",
            "torch.float32 10391552 0.006750715783423626\n",
            "torch.float16 589934592 0.3832375640771494\n",
            "torch.int8 938999808 0.6099998270432512\n",
            "torch.float32 10409984 0.006762608879599485\n",
            "torch.float16 589934592 0.38323348513452565\n",
            "torch.int8 938999808 0.6099933345839303\n",
            "torch.float32 10426368 0.006773180281543982\n",
            "torch.float16 589934592 0.3826470224882537\n",
            "torch.int8 941359104 0.6105901622358337\n",
            "torch.float32 10426368 0.006762815275912502\n",
            "torch.float16 589934592 0.3826424478149633\n",
            "torch.int8 941359104 0.6105828624259766\n",
            "torch.float32 10444800 0.006774689759060151\n",
            "torch.float16 589934592 0.38264041466194476\n",
            "torch.int8 941359104 0.610579618122066\n",
            "torch.float32 10452992 0.006779967215989245\n",
            "torch.float16 589934592 0.3826358401466496\n",
            "torch.int8 941359104 0.6105723185643219\n",
            "torch.float32 10471424 0.006791841289028513\n",
            "torch.float16 589934592 0.38263380706384936\n",
            "torch.int8 941359104 0.6105690743724587\n",
            "torch.float32 10479616 0.0067971185636919364\n",
            "torch.float16 589934592 0.38204917695773444\n",
            "torch.int8 943718400 0.6111640898655253\n",
            "torch.float32 10479616 0.006786733176740219\n",
            "torch.float16 589934592 0.38204461656807875\n",
            "torch.int8 943718400 0.6111567946099365\n",
            "torch.float32 10498048 0.006798588821984668\n",
            "torch.float16 589934592 0.382042589763178\n",
            "torch.int8 943718400 0.6111535523300229\n",
            "torch.float32 10506240 0.006803857906799084\n",
            "torch.float16 589934592 0.38203802953077814\n",
            "torch.int8 943718400 0.6111462573259964\n",
            "torch.float32 10524672 0.006815713143225467\n",
            "torch.float16 589934592 0.3820360027957671\n",
            "torch.int8 943718400 0.6111430151578853\n",
            "torch.float32 10532864 0.006820982046347664\n",
            "torch.float16 589934592 0.38203144272061496\n",
            "torch.int8 943718400 0.6111357204054079\n",
            "torch.float32 10551296 0.00683283687397713\n",
            "torch.float16 589934592 0.38202941605548996\n",
            "torch.int8 943718400 0.6111324783490936\n",
            "torch.float32 10559488 0.006838105595416507\n",
            "torch.float16 589934592 0.380865619306267\n",
            "torch.int8 948436992 0.6123171063870297\n",
            "torch.float32 10559488 0.006817274306703301\n",
            "torch.float16 589934592 0.3808615906981418\n",
            "torch.int8 948436992 0.6123106295995621\n",
            "torch.float32 10575872 0.006827779702296112\n",
            "torch.float16 589934592 0.38085705861587055\n",
            "torch.int8 948436992 0.6123033433774366\n",
            "torch.float32 10594304 0.006839598006692837\n",
            "torch.float16 589934592 0.38085303018884387\n",
            "torch.int8 948436992 0.6122968668811207\n",
            "torch.float32 10610688 0.0068501029300353415\n",
            "torch.float16 589934592 0.3808484983103016\n",
            "torch.int8 948436992 0.61228958098653\n",
            "torch.float32 10629120 0.0068619207031683825\n",
            "torch.float16 589934592 0.37569843331820385\n",
            "torch.int8 969670656 0.6175324370431816\n",
            "torch.float32 10629120 0.006769129638614558\n",
            "torch.float16 589934592 0.37569402328083734\n",
            "torch.int8 969670656 0.6175251883008902\n",
            "torch.float32 10647552 0.006780788418272523\n",
            "torch.float16 589934592 0.37567638416664384\n",
            "torch.int8 969670656 0.6174961950333937\n",
            "torch.float32 10721280 0.006827420799962439\n",
            "torch.float16 589934592 0.37567197464689545\n",
            "torch.int8 969670656 0.6174889471419069\n",
            "torch.float32 10739712 0.006839078211197621\n",
            "torch.float16 589934592 0.37565433760299216\n",
            "torch.int8 969670656 0.6174599572773296\n",
            "torch.float32 10813440 0.00688570511967825\n",
            "torch.float16 589934592 0.37064286762037846\n",
            "torch.int8 990904320 0.6225632869859259\n",
            "torch.float32 10813440 0.00679384539369562\n",
            "torch.float16 589934592 0.37063857547064205\n",
            "torch.int8 990904320 0.6225560775261425\n",
            "torch.float32 10831872 0.006805347003215459\n",
            "torch.float16 589934592 0.37062140786573666\n",
            "torch.int8 990904320 0.6225272413566832\n",
            "torch.float32 10905600 0.0068513507775800645\n",
            "torch.float16 589934592 0.37061711621300314\n",
            "torch.int8 990904320 0.6225200327317081\n",
            "torch.float32 10924032 0.006862851055288796\n",
            "torch.float16 589934592 0.37059995059593653\n",
            "torch.int8 990904320 0.6224911999011918\n",
            "torch.float32 10997760 0.006908849502871611\n",
            "torch.float16 589934592 0.36572155524407335\n",
            "torch.int8 1012137984 0.6274605399475898\n",
            "torch.float32 10997760 0.006817904808336888\n",
            "torch.float16 589934592 0.3657048401103003\n",
            "torch.int8 1012137984 0.6274318621551211\n",
            "torch.float32 11071488 0.006863297734578528\n",
            "torch.float16 589934592 0.36570066156559095\n",
            "torch.int8 1012137984 0.6274246931165948\n",
            "torch.float32 11089920 0.006874645317814281\n",
            "torch.float16 589934592 0.3656839483415914\n",
            "torch.int8 1012137984 0.6273960186006833\n",
            "torch.float32 11163648 0.006920033057725339\n",
            "torch.float16 589934592 0.3656797702742846\n",
            "torch.int8 1012137984 0.6273888503812258\n",
            "torch.float32 11182080 0.006931379344489554\n",
            "torch.float16 589936896 0.3656806761893654\n",
            "torch.int8 1012137984 0.6273879543653108\n",
            "torch.float32 11182080 0.006931369445323825\n",
            "torch.float16 589939200 0.36568158210185864\n",
            "torch.int8 1012137984 0.627387058351955\n",
            "torch.float32 11182080 0.006931359546186372\n",
            "torch.float16 589941504 0.36568248801176423\n",
            "torch.int8 1012137984 0.6273861623411585\n",
            "torch.float32 11182080 0.006931349647077194\n",
            "torch.float16 589943808 0.36568339391908233\n",
            "torch.int8 1012137984 0.6273852663329214\n",
            "torch.float32 11182080 0.006931339747996291\n",
            "torch.float16 589943808 0.3646169356752682\n",
            "torch.int8 1016856576 0.6284719387416056\n",
            "torch.float32 11182080 0.006911125583126221\n",
            "torch.float16 589943808 0.3646127820191269\n",
            "torch.int8 1016856576 0.628464779292681\n",
            "torch.float32 11200512 0.006922438688192173\n",
            "torch.float16 589943808 0.3646090899597809\n",
            "torch.int8 1016856576 0.6284584154750189\n",
            "torch.float32 11216896 0.006932494565200194\n",
            "torch.float16 589943808 0.36460493648239084\n",
            "torch.int8 1016856576 0.6284512563341988\n",
            "torch.float32 11235328 0.006943807183410301\n",
            "torch.float16 589943808 0.3646012445819297\n",
            "torch.int8 1016856576 0.6284448927903987\n",
            "torch.float32 11251712 0.006953862627671538\n",
            "torch.float16 589943808 0.36459709128327933\n",
            "torch.int8 1016856576 0.6284377339576634\n",
            "torch.float32 11270144 0.006965174759057227\n",
            "torch.float16 589943808 0.36459339954169284\n",
            "torch.int8 1016856576 0.6284313706877075\n",
            "torch.float32 11286528 0.006975229770599616\n",
            "torch.float16 589943808 0.3640625681293828\n",
            "torch.int8 1019215872 0.6289723577174121\n",
            "torch.float32 11286528 0.006965074153205091\n",
            "torch.float16 589943808 0.3640584270940957\n",
            "torch.int8 1019215872 0.6289652034616442\n",
            "torch.float32 11304960 0.006976369444260135\n",
            "torch.float16 589943808 0.3640565866642064\n",
            "torch.int8 1019215872 0.6289620238446553\n",
            "torch.float32 11313152 0.00698138949113835\n",
            "torch.float16 589943808 0.36405244576498996\n",
            "torch.int8 1019215872 0.6289548698239696\n",
            "torch.float32 11331584 0.006992684411040429\n",
            "torch.float16 589943808 0.36405060539557504\n",
            "torch.int8 1019215872 0.6289516903114591\n",
            "torch.float32 11339776 0.006997704292965835\n",
            "torch.float16 589943808 0.36352135222671556\n",
            "torch.int8 1021575168 0.629491116673597\n",
            "torch.float32 11339776 0.006987531099687473\n",
            "torch.float16 589943808 0.36351722349434423\n",
            "torch.int8 1021575168 0.6294839671613746\n",
            "torch.float32 11358208 0.006998809344281224\n",
            "torch.float16 589943808 0.3635153885322829\n",
            "torch.int8 1021575168 0.6294807896525157\n",
            "torch.float32 11366400 0.007003821815201322\n",
            "torch.float16 589943808 0.3635112599353763\n",
            "torch.int8 1021575168 0.6294736403748705\n",
            "torch.float32 11384832 0.007015099689753146\n",
            "torch.float16 589943808 0.36350942503352\n",
            "torch.int8 1021575168 0.6294704629702658\n",
            "torch.float32 11393024 0.007020111996214213\n",
            "torch.float16 589943808 0.3635052965720714\n",
            "torch.int8 1021575168 0.6294633139271862\n",
            "torch.float32 11411456 0.007031389500742322\n",
            "torch.float16 589943808 0.36350346173041725\n",
            "torch.int8 1021575168 0.6294601366268303\n",
            "torch.float32 11419648 0.00703640164275245\n",
            "torch.float16 589943808 0.36244966212836793\n",
            "torch.int8 1026293760 0.6305343348166006\n",
            "torch.float32 11419648 0.007016003055031459\n",
            "torch.float16 589943808 0.36244601374624497\n",
            "torch.int8 1026293760 0.6305279879209198\n",
            "torch.float32 11436032 0.007025998332835281\n",
            "torch.float16 589943808 0.3624419094041495\n",
            "torch.int8 1026293760 0.6305208478160075\n",
            "torch.float32 11454464 0.007037242779843011\n",
            "torch.float16 589943808 0.36243826117810035\n",
            "torch.int8 1026293760 0.63051450119184\n",
            "torch.float32 11470848 0.007047237630059658\n",
            "torch.float16 589943808 0.3624341570115823\n",
            "torch.int8 1026293760 0.6305073613923704\n",
            "torch.float32 11489280 0.00705848159604725\n",
            "torch.float16 589943808 0.35776709248450456\n",
            "torch.int8 1047527424 0.6352653179847645\n",
            "torch.float32 11489280 0.006967589530730982\n",
            "torch.float16 589943808 0.3577630934261341\n",
            "torch.int8 1047527424 0.6352582170994658\n",
            "torch.float32 11507712 0.006978689474399983\n",
            "torch.float16 589943808 0.357747098086628\n",
            "torch.int8 1047527424 0.6352298151456499\n",
            "torch.float32 11581440 0.007023086767722116\n",
            "torch.float16 589943808 0.35774309947523036\n",
            "torch.int8 1047527424 0.6352227150540138\n",
            "torch.float32 11599872 0.007034185470755783\n",
            "torch.float16 589943808 0.3577271059234657\n",
            "torch.int8 1047527424 0.6351943162745818\n",
            "torch.float32 11673600 0.007078577801952569\n",
            "torch.float16 589943808 0.3531797190952343\n",
            "torch.int8 1068761088 0.6398316851895105\n",
            "torch.float32 11673600 0.006988595715255185\n",
            "torch.float16 589943808 0.3531758219325461\n",
            "torch.int8 1068761088 0.6398246249648275\n",
            "torch.float32 11692032 0.006999553102626396\n",
            "torch.float16 589943808 0.3531602341418202\n",
            "torch.int8 1068761088 0.6397963856241483\n",
            "torch.float32 11765760 0.00704338023403148\n",
            "torch.float16 589943808 0.35315633740913127\n",
            "torch.int8 1068761088 0.6397893261784658\n",
            "torch.float32 11784192 0.007054336412402833\n",
            "torch.float16 589943808 0.35314075133826034\n",
            "torch.int8 1068761088 0.639761089953531\n",
            "torch.float32 11857920 0.007098158708208671\n",
            "torch.float16 589943808 0.348708497135542\n",
            "torch.int8 1089994752 0.6442824328373115\n",
            "torch.float32 11857920 0.007009070027146527\n",
            "torch.float16 589943808 0.34869330118259284\n",
            "torch.int8 1089994752 0.6442543564192839\n",
            "torch.float32 11931648 0.007052342398123249\n",
            "torch.float16 589943808 0.3486895024012928\n",
            "torch.int8 1089994752 0.6442473376971194\n",
            "torch.float32 11950080 0.007063159901587849\n",
            "torch.float16 589943808 0.3486743081037604\n",
            "torch.int8 1089994752 0.6442192643376806\n",
            "torch.float32 12023808 0.007106427558559033\n",
            "torch.float16 589943808 0.3486705097362807\n",
            "torch.int8 1089994752 0.6442122463801008\n",
            "torch.float32 12042240 0.007117243883618537\n",
            "torch.float16 589946112 0.34867139666189007\n",
            "torch.int8 1089994752 0.6442113691461543\n",
            "torch.float32 12042240 0.007117234191955619\n",
            "torch.float16 589948416 0.34867228358508395\n",
            "torch.int8 1089994752 0.6442104919145969\n",
            "torch.float32 12042240 0.007117224500319095\n",
            "torch.float16 589950720 0.3486731705058624\n",
            "torch.int8 1089994752 0.6442096146854286\n",
            "torch.float32 12042240 0.007117214808708965\n",
            "torch.float16 589953024 0.34867405742422536\n",
            "torch.int8 1089994752 0.6442087374586494\n",
            "torch.float32 12042240 0.00711720511712523\n",
            "torch.float16 589953024 0.3477043855488001\n",
            "torch.int8 1094713344 0.6451982024717824\n",
            "torch.float32 12042240 0.007097411979417505\n",
            "torch.float16 589953024 0.34770060834298955\n",
            "torch.int8 1094713344 0.6451911935110081\n",
            "torch.float32 12060672 0.0071081981460023175\n",
            "torch.float16 589953024 0.34769725089560743\n",
            "torch.int8 1094713344 0.6451849634514923\n",
            "torch.float32 12077056 0.007117785652900223\n",
            "torch.float16 589953024 0.3476934738448057\n",
            "torch.int8 1094713344 0.6451779547783516\n",
            "torch.float32 12095488 0.007128571376842644\n",
            "torch.float16 589953024 0.347690116535205\n",
            "torch.int8 1094713344 0.6451717249745023\n",
            "torch.float32 12111872 0.007138158490292756\n",
            "torch.float16 589953024 0.3476863396394026\n",
            "torch.int8 1094713344 0.6451647165889773\n",
            "torch.float32 12130304 0.007148943771620033\n",
            "torch.float16 589953024 0.3476829824675748\n",
            "torch.int8 1094713344 0.6451584870407786\n",
            "torch.float32 12146688 0.00715853049164657\n",
            "torch.float16 589953024 0.34720022563078995\n",
            "torch.int8 1097072640 0.6456511834768837\n",
            "torch.float32 12146688 0.007148590892326384\n",
            "torch.float16 589953024 0.3471964593706271\n",
            "torch.int8 1097072640 0.6456441797650428\n",
            "torch.float32 12165120 0.007159360864330112\n",
            "torch.float16 589953024 0.3471947855034489\n",
            "torch.int8 1097072640 0.6456410670529971\n",
            "torch.float32 12173312 0.007164147443554015\n",
            "torch.float16 589953024 0.34719101936130825\n",
            "torch.int8 1097072640 0.6456340635606295\n",
            "torch.float32 12191744 0.007174917078062327\n",
            "torch.float16 589953024 0.3471893455465831\n",
            "torch.int8 1097072640 0.6456309509461251\n",
            "torch.float32 12199936 0.007179703507291792\n",
            "torch.float16 589953024 0.3467079576145932\n",
            "torch.int8 1099431936 0.6461222937418457\n",
            "torch.float32 12199936 0.007169748643561067\n",
            "torch.float16 589953024 0.34670420202657243\n",
            "torch.int8 1099431936 0.6461152948567811\n",
            "torch.float32 12218368 0.007180503116646466\n",
            "torch.float16 589953024 0.34670253290245706\n",
            "torch.int8 1099431936 0.6461121842898666\n",
            "torch.float32 12226560 0.007185282807676338\n",
            "torch.float16 589953024 0.3466987774319572\n",
            "torch.int8 1099431936 0.646105185623813\n",
            "torch.float32 12244992 0.007196036944229811\n",
            "torch.float16 589953024 0.3466971083600721\n",
            "torch.int8 1099431936 0.6461020751542345\n",
            "torch.float32 12253184 0.007200816485693447\n",
            "torch.float16 589953024 0.3466933530070876\n",
            "torch.int8 1099431936 0.6460950767071816\n",
            "torch.float32 12271616 0.007211570285730791\n",
            "torch.float16 589953024 0.3466916839874304\n",
            "torch.int8 1099431936 0.6460919663349344\n",
            "torch.float32 12279808 0.007216349677635213\n",
            "torch.float16 589953024 0.3457329916766183\n",
            "torch.int8 1104150528 0.6470706137218778\n",
            "torch.float32 12279808 0.007196394601503849\n",
            "torch.float16 589953024 0.3457296721207197\n",
            "torch.int8 1104150528 0.6470644008722964\n",
            "torch.float32 12296192 0.007205927006983893\n",
            "torch.float16 589953024 0.3457259376965286\n",
            "torch.int8 1104150528 0.647057411559123\n",
            "torch.float32 12314624 0.007216650744348377\n",
            "torch.float16 589953024 0.3457226182760855\n",
            "torch.int8 1104150528 0.647051198963059\n",
            "torch.float32 12331008 0.007226182760855476\n",
            "torch.float16 589953024 0.34571888400427736\n",
            "torch.int8 1104150528 0.6470442099350838\n",
            "torch.float32 12349440 0.007236906060638792\n",
            "torch.float16 589953024 0.3414699198788047\n",
            "torch.int8 1125384192 0.651382117290602\n",
            "torch.float32 12349440 0.007147962830593281\n",
            "torch.float16 589953024 0.3414662769112595\n",
            "torch.int8 1125384192 0.6513751680286769\n",
            "torch.float32 12367872 0.007158555060063584\n",
            "torch.float16 589953024 0.3414517058183448\n",
            "torch.int8 1125384192 0.6513473724636754\n",
            "torch.float32 12441600 0.007200921717979901\n",
            "torch.float16 589953024 0.3414480632394201\n",
            "torch.int8 1125384192 0.651340423943076\n",
            "torch.float32 12460032 0.00721151281750392\n",
            "torch.float16 589953024 0.341433493700863\n",
            "torch.int8 1125384192 0.6513126313431403\n",
            "torch.float32 12533760 0.0072538749559967145\n",
            "torch.float16 589953024 0.3372885821170768\n",
            "torch.int8 1146617856 0.6555456030349334\n",
            "torch.float32 12533760 0.0071658148479898836\n",
            "torch.float16 589953024 0.3372850278199292\n",
            "torch.int8 1146617856 0.6555386949924128\n",
            "torch.float32 12552192 0.007176277187658068\n",
            "torch.float16 589953024 0.33727081138040044\n",
            "torch.int8 1146617856 0.6555110642781876\n",
            "torch.float32 12625920 0.007218124341412013\n",
            "torch.float16 589953024 0.3372672574577718\n",
            "torch.int8 1146617856 0.6555041569635726\n",
            "torch.float32 12644352 0.007228585578655652\n",
            "torch.float16 589953024 0.3372530425162004\n",
            "torch.int8 1146617856 0.6554765291607397\n",
            "torch.float32 12718080 0.007270428323059902\n",
            "torch.float16 589953024 0.3332084075080421\n",
            "torch.int8 1167851520 0.659608357537712\n",
            "torch.float32 12718080 0.007183234954245917\n",
            "torch.float16 589953024 0.33319453264071786\n",
            "torch.int8 1167851520 0.6595808913086476\n",
            "torch.float32 12791808 0.007224576050634493\n",
            "torch.float16 589953024 0.33319106410443255\n",
            "torch.int8 1167851520 0.6595740251087839\n",
            "torch.float32 12810240 0.007234910786783536\n",
            "torch.float16 589953024 0.33317719068140655\n",
            "torch.int8 1167851520 0.6595465617388045\n",
            "torch.float32 12883968 0.007276247579788895\n",
            "torch.float16 589953024 0.33317372250616756\n",
            "torch.int8 1167851520 0.6595396962536563\n",
            "torch.float32 12902400 0.007286581240176127\n",
            "torch.float16 589955328 0.3331745901628679\n",
            "torch.int8 1167851520 0.6595388380780626\n",
            "torch.float32 12902400 0.007286571759069505\n",
            "torch.float16 589957632 0.3331754578173103\n",
            "torch.int8 1167851520 0.6595379799047022\n",
            "torch.float32 12902400 0.007286562277987556\n",
            "torch.float16 589959936 0.3331763254694947\n",
            "torch.int8 1167851520 0.659537121733575\n",
            "torch.float32 12902400 0.0072865527969302795\n",
            "torch.float16 589962240 0.33317719311942123\n",
            "torch.int8 1167851520 0.6595362635646811\n",
            "torch.float32 12902400 0.007286543315897676\n",
            "torch.float16 589962240 0.332291704516551\n",
            "torch.int8 1172570112 0.6604411176919444\n",
            "torch.float32 12902400 0.007267177791504669\n",
            "torch.float16 589962240 0.33228825480537144\n",
            "torch.int8 1172570112 0.6604342612730925\n",
            "torch.float32 12920832 0.007277483921536058\n",
            "torch.float16 589962240 0.33228518845556465\n",
            "torch.int8 1172570112 0.6604281667980693\n",
            "torch.float32 12937216 0.00728664474636605\n",
            "torch.float16 589962240 0.33228173887967705\n",
            "torch.int8 1172570112 0.6604213106481148\n",
            "torch.float32 12955648 0.007296950472208204\n",
            "torch.float16 589962240 0.33227867265012623\n",
            "torch.int8 1172570112 0.6604152164121043\n",
            "torch.float32 12972032 0.007306110937769445\n",
            "torch.float16 589962240 0.3322752232095225\n",
            "torch.int8 1172570112 0.6604083605310314\n",
            "torch.float32 12990464 0.007316416259446142\n",
            "torch.float16 589962240 0.3322721571002206\n",
            "torch.int8 1172570112 0.6604022665340197\n",
            "torch.float32 13006848 0.007325576365759765\n",
            "torch.float16 589962240 0.3318312273323392\n",
            "torch.int8 1174929408 0.660852917443494\n",
            "torch.float32 13006848 0.007315855224166858\n",
            "torch.float16 589962240 0.3318277871754422\n",
            "torch.int8 1174929408 0.660846066256702\n",
            "torch.float32 13025280 0.007326146567855841\n",
            "torch.float16 589962240 0.33182625823971706\n",
            "torch.int8 1174929408 0.6608430213303922\n",
            "torch.float32 13033472 0.00733072042989077\n",
            "torch.float16 589962240 0.3318228181858498\n",
            "torch.int8 1174929408 0.6608361703487873\n",
            "torch.float32 13051904 0.007341011465362878\n",
            "torch.float16 589962240 0.33182128929591465\n",
            "torch.int8 1174929408 0.6608331255136697\n",
            "torch.float32 13060096 0.007345585190415607\n",
            "torch.float16 589962240 0.33138155454004264\n",
            "torch.int8 1177288704 0.6612825947537797\n",
            "torch.float32 13060096 0.007335850706177725\n",
            "torch.float16 589962240 0.33137812370046527\n",
            "torch.int8 1177288704 0.6612757484025968\n",
            "torch.float32 13078528 0.007346127896937944\n",
            "torch.float16 589962240 0.33137659890567817\n",
            "torch.int8 1177288704 0.6612727056253527\n",
            "torch.float32 13086720 0.007350695468969194\n",
            "torch.float16 589962240 0.33137316816871226\n",
            "torch.int8 1177288704 0.6612658594789342\n",
            "torch.float32 13105152 0.007360972352353492\n",
            "torch.float16 589962240 0.3313716434195292\n",
            "torch.int8 1177288704 0.6612628167926945\n",
            "torch.float32 13113344 0.007365539787776287\n",
            "torch.float16 589962240 0.33136821278517015\n",
            "torch.int8 1177288704 0.6612559708510314\n",
            "torch.float32 13131776 0.0073758163637984535\n",
            "torch.float16 589962240 0.33136668808158914\n",
            "torch.int8 1177288704 0.661252928255792\n",
            "torch.float32 13139968 0.007380383662618921\n",
            "torch.float16 589962240 0.3304907842785201\n",
            "torch.int8 1182007296 0.6621483406768082\n",
            "torch.float32 13139968 0.007360875044671769\n",
            "torch.float16 589962240 0.3304877510128854\n",
            "torch.int8 1182007296 0.6621422634368294\n",
            "torch.float32 13156352 0.007369985550285179\n",
            "torch.float16 589962240 0.33048433865559956\n",
            "torch.int8 1182007296 0.6621354266751945\n",
            "torch.float32 13174784 0.0073802346692059045\n",
            "torch.float16 589962240 0.33048130550827987\n",
            "torch.int8 1182007296 0.6621293496722634\n",
            "torch.float32 13191168 0.007389344819456658\n",
            "torch.float16 589962240 0.3304778932840945\n",
            "torch.int8 1182007296 0.6621225131772994\n",
            "torch.float32 13209600 0.007399593538606089\n",
            "torch.float16 589962240 0.32659325708940945\n",
            "torch.int8 1203240960 0.6660941286509927\n",
            "torch.float32 13209600 0.007312614259597806\n",
            "torch.float16 589962240 0.32658992468084164\n",
            "torch.int8 1203240960 0.6660873321304489\n",
            "torch.float32 13228032 0.007322743188709439\n",
            "torch.float16 589962240 0.3265765957265902\n",
            "torch.int8 1203240960 0.6660601474351889\n",
            "torch.float32 13301760 0.007363256838220915\n",
            "torch.float16 589962240 0.32657326365802186\n",
            "torch.int8 1203240960 0.6660533516080814\n",
            "torch.float32 13320192 0.007373384733896654\n",
            "torch.float16 589962240 0.3265599360636643\n",
            "torch.int8 1203240960 0.6660261696863549\n",
            "torch.float32 13393920 0.00741389424998087\n",
            "torch.float16 589962240 0.32276633327563\n",
            "torch.int8 1224474624 0.6699058986852034\n",
            "torch.float32 13393920 0.007327768039166584\n",
            "torch.float16 589962240 0.3227630785054865\n",
            "torch.int8 1224474624 0.6698991433656637\n",
            "torch.float32 13412352 0.007337778128849771\n",
            "torch.float16 589962240 0.32275006008130724\n",
            "torch.int8 1224474624 0.6698721234498602\n",
            "torch.float32 13486080 0.007377816468832506\n",
            "torch.float16 589962240 0.3227468056393511\n",
            "torch.int8 1224474624 0.6698653688114777\n",
            "torch.float32 13504512 0.007387825549171222\n",
            "torch.float16 589962240 0.3227337885278217\n",
            "torch.int8 1224474624 0.6698383516200969\n",
            "torch.float32 13578240 0.007427859852081397\n",
            "torch.float16 589962240 0.3190280562620334\n",
            "torch.int8 1245708288 0.6736293729411316\n",
            "torch.float32 13578240 0.007342570796835053\n",
            "torch.float16 589962240 0.31901533738764404\n",
            "torch.int8 1245708288 0.6736025169727887\n",
            "torch.float32 13651968 0.007382145639567237\n",
            "torch.float16 589962240 0.3190121578275047\n",
            "torch.int8 1245708288 0.6735958033152879\n",
            "torch.float32 13670400 0.007392038857207404\n",
            "torch.float16 589962240 0.31899944022072246\n",
            "torch.int8 1245708288 0.673568950023504\n",
            "torch.float32 13744128 0.00743160975577345\n",
            "torch.float16 589962240 0.3189962609774612\n",
            "torch.int8 1245708288 0.673562237035093\n",
            "torch.float32 13762560 0.007441501987445787\n",
            "torch.float16 589964544 0.3189971093621204\n",
            "torch.int8 1245708288 0.6735613979209499\n",
            "torch.float32 13762560 0.007441492716929687\n",
            "torch.float16 589966848 0.31899795774466577\n",
            "torch.int8 1245708288 0.6735605588088975\n",
            "torch.float32 13762560 0.007441483446436684\n",
            "torch.float16 589969152 0.31899880612509735\n",
            "torch.int8 1245708288 0.6735597196989359\n",
            "torch.float32 13762560 0.007441474175966779\n",
            "torch.float16 589971456 0.3189996545034151\n",
            "torch.int8 1245708288 0.673558880591065\n",
            "torch.float32 13762560 0.0074414649055199724\n",
            "torch.float16 589971456 0.31818784296052804\n",
            "torch.int8 1250426880 0.6743896296689023\n",
            "torch.float32 13762560 0.007422527370569679\n",
            "torch.float16 589971456 0.3181846799232571\n",
            "torch.int8 1250426880 0.6743829257058787\n",
            "torch.float32 13780992 0.007432394370864219\n",
            "torch.float16 589971456 0.31818186838736473\n",
            "torch.int8 1250426880 0.6743769667395283\n",
            "torch.float32 13797376 0.007441164873106987\n",
            "torch.float16 589971456 0.318178705468876\n",
            "torch.int8 1250426880 0.67437026302826\n",
            "torch.float32 13815808 0.007451031502863997\n",
            "torch.float16 589971456 0.3181758940385648\n",
            "torch.int8 1250426880 0.6743643042856858\n",
            "torch.float32 13832192 0.007459801675749349\n",
            "torch.float16 589971456 0.31817273123885154\n",
            "torch.int8 1250426880 0.6743576008261587\n",
            "torch.float32 13850624 0.0074696679349897\n",
            "torch.float16 589971456 0.3181699199141156\n",
            "torch.int8 1250426880 0.6743516423073482\n",
            "torch.float32 13867008 0.007478437778536188\n",
            "torch.float16 589971456 0.3177656071611761\n",
            "torch.int8 1252786176 0.6747654582457766\n",
            "torch.float32 13867008 0.007468934593047306\n",
            "torch.float16 589971456 0.31776245251300517\n",
            "torch.int8 1252786176 0.674758759447761\n",
            "torch.float32 13885440 0.007478788039233854\n",
            "torch.float16 589971456 0.3177610504672567\n",
            "torch.int8 1252786176 0.6747557822468916\n",
            "torch.float32 13893632 0.007483167285851695\n",
            "torch.float16 589971456 0.3177578959095587\n",
            "torch.int8 1252786176 0.6747490836409924\n",
            "torch.float32 13912064 0.007493020449448861\n",
            "torch.float16 589971456 0.3177564939040196\n",
            "torch.int8 1252786176 0.6747461065255063\n",
            "torch.float32 13920256 0.007497399570474122\n",
            "torch.float16 589971456 0.31735323052042835\n",
            "torch.int8 1255145472 0.675158884826265\n",
            "torch.float32 13920256 0.007487884653306644\n",
            "torch.float16 589971456 0.31735008405472037\n",
            "torch.int8 1255145472 0.6751521908207399\n",
            "torch.float32 13938688 0.007497725124539792\n",
            "torch.float16 589971456 0.317348685645544\n",
            "torch.int8 1255145472 0.6751492157497803\n",
            "torch.float32 13946880 0.007502098604675757\n",
            "torch.float16 589971456 0.3173455392699572\n",
            "torch.int8 1255145472 0.6751425219359849\n",
            "torch.float32 13965312 0.007511938794057868\n",
            "torch.float16 589971456 0.3173441409008338\n",
            "torch.int8 1255145472 0.6751395469502368\n",
            "torch.float32 13973504 0.007516312148929396\n",
            "torch.float16 589971456 0.3173409946153643\n",
            "torch.int8 1255145472 0.6751328533281632\n",
            "torch.float32 13991936 0.007526152056472579\n",
            "torch.float16 589971456 0.31733959628629216\n",
            "torch.int8 1255145472 0.6751298784276228\n",
            "torch.float32 14000128 0.007530525286085052\n",
            "torch.float16 589971456 0.31653620137219307\n",
            "torch.int8 1259864064 0.6759523380464928\n",
            "torch.float32 14000128 0.007511460581314088\n",
            "torch.float16 589971456 0.3165334188932011\n",
            "torch.int8 1259864064 0.6759463961568383\n",
            "torch.float32 14016512 0.007520184949960663\n",
            "torch.float16 589971456 0.31653028866280697\n",
            "torch.int8 1259864064 0.6759397116558418\n",
            "torch.float32 14034944 0.007529999681351246\n",
            "torch.float16 589971456 0.31652750628776366\n",
            "torch.int8 1259864064 0.6759337699881661\n",
            "torch.float32 14051328 0.0075387237240701855\n",
            "torch.float16 589971456 0.3165243761743086\n",
            "torch.int8 1259864064 0.675927085736889\n",
            "torch.float32 14069760 0.007548538088802453\n",
            "torch.float16 589971456 0.31295913644867124\n",
            "torch.int8 1281097728 0.6795773500290067\n",
            "torch.float32 14069760 0.007463513522322098\n",
            "torch.float16 589971456 0.3129560765116726\n",
            "torch.int8 1281097728 0.6795707055069762\n",
            "torch.float32 14088192 0.007473217981351176\n",
            "torch.float16 589971456 0.31294383736202136\n",
            "torch.int8 1281097728 0.6795441287181309\n",
            "torch.float32 14161920 0.007512033919847738\n",
            "torch.float16 589971456 0.3129407777241856\n",
            "torch.int8 1281097728 0.6795374848457196\n",
            "torch.float32 14180352 0.007521737430094771\n",
            "torch.float16 589971456 0.31292853977109786\n",
            "torch.int8 1281097728 0.6795109106551607\n",
            "torch.float32 14254080 0.007560549573741429\n",
            "torch.float16 589971456 0.3094433989017505\n",
            "torch.int8 1302331392 0.6830802547113872\n",
            "torch.float32 14254080 0.007476346386862255\n",
            "torch.float16 589971456 0.3094404073280376\n",
            "torch.int8 1302331392 0.6830736509675652\n",
            "torch.float32 14272512 0.007485941704397145\n",
            "torch.float16 589971456 0.30942844161159033\n",
            "torch.int8 1302331392 0.6830472372690742\n",
            "torch.float32 14346240 0.007524321119335411\n",
            "torch.float16 589971456 0.3094254503270712\n",
            "torch.int8 1302331392 0.6830406341636323\n",
            "torch.float32 14364672 0.007533915509296555\n",
            "torch.float16 589971456 0.30941348576731476\n",
            "torch.int8 1302331392 0.6830142230184764\n",
            "torch.float32 14438400 0.007572291214208841\n",
            "torch.float16 589971456 0.30600577441538823\n",
            "torch.int8 1323565056 0.6865053314552674\n",
            "torch.float32 14438400 0.007488894129344355\n",
            "torch.float16 589971456 0.30599407284662694\n",
            "torch.int8 1323565056 0.6864790796979063\n",
            "torch.float32 14512128 0.007526847455466684\n",
            "torch.float16 589971456 0.30599114759426793\n",
            "torch.int8 1323565056 0.6864725170722692\n",
            "torch.float32 14530560 0.007536335333462922\n",
            "torch.float16 589971456 0.30597944714410896\n",
            "torch.int8 1323565056 0.6864462678244244\n",
            "torch.float32 14604288 0.007574285031466582\n",
            "torch.float16 589971456 0.3059765221713805\n",
            "torch.int8 1323565056 0.6864397058261213\n",
            "torch.float32 14622720 0.007583772002498184\n",
            "torch.float16 589973760 0.30597735147435284\n",
            "torch.int8 1323565056 0.6864388855851546\n",
            "torch.float32 14622720 0.0075837629404925545\n",
            "torch.float16 589976064 0.3059781807753432\n",
            "torch.int8 1323565056 0.6864380653461482\n",
            "torch.float32 14622720 0.007583753878508581\n",
            "torch.float16 589978368 0.30597901007435174\n",
            "torch.int8 1323565056 0.686437245109102\n",
            "torch.float32 14622720 0.007583744816546265\n",
            "torch.float16 589980672 0.3059798393713784\n",
            "torch.int8 1323565056 0.686436424874016\n",
            "torch.float32 14622720 0.007583735754605606\n",
            "torch.float16 589980672 0.3052328769300455\n",
            "torch.int8 1328283648 0.6872019008415515\n",
            "torch.float32 14622720 0.00756522222840296\n",
            "torch.float16 589980672 0.3052299662587949\n",
            "torch.int8 1328283648 0.6871953477505598\n",
            "torch.float32 14641152 0.007574685990645279\n",
            "torch.float16 589980672 0.30522737904206304\n",
            "torch.int8 1328283648 0.6871895228857094\n",
            "torch.float32 14657536 0.007583098072227499\n",
            "torch.float16 589980672 0.3052244684756656\n",
            "torch.int8 1328283648 0.6871829700307845\n",
            "torch.float32 14675968 0.007592561493549872\n",
            "torch.float16 589980672 0.30522188135213413\n",
            "torch.int8 1328283648 0.6871771453757656\n",
            "torch.float32 14692352 0.007600973272100328\n",
            "torch.float16 589980672 0.3052189708905842\n",
            "torch.int8 1328283648 0.6871705927568946\n",
            "torch.float32 14710784 0.007610436352521175\n",
            "torch.float16 589980672 0.305216383860248\n",
            "torch.int8 1328283648 0.6871647683116957\n",
            "torch.float32 14727168 0.0076188478280562405\n",
            "torch.float16 589980672 0.30484430878242275\n",
            "torch.int8 1330642944 0.6875461311720532\n",
            "torch.float32 14727168 0.007609560045524026\n",
            "torch.float16 589980672 0.3048414055171166\n",
            "torch.int8 1330642944 0.6875395831448422\n",
            "torch.float32 14745600 0.007619011338041247\n",
            "torch.float16 589980672 0.30484011519473103\n",
            "torch.int8 1330642944 0.6875366729505608\n",
            "torch.float32 14753792 0.007623211854708185\n",
            "torch.float16 589980672 0.3048372120093014\n",
            "torch.int8 1330642944 0.6875301251035034\n",
            "torch.float32 14772224 0.007632662887195211\n",
            "torch.float16 589980672 0.30483592172241586\n",
            "torch.int8 1330642944 0.6875272149892886\n",
            "torch.float32 14780416 0.007636863288295557\n",
            "torch.float16 589980672 0.30446477310979436\n",
            "torch.int8 1333002240 0.6879076617554815\n",
            "torch.float32 14780416 0.007627565134724234\n",
            "torch.float16 589980672 0.30446187706917005\n",
            "torch.int8 1333002240 0.6879011184417383\n",
            "torch.float32 14798848 0.007637004489091692\n",
            "torch.float16 589980672 0.30446058995768804\n",
            "torch.int8 1333002240 0.6878982103422528\n",
            "torch.float32 14807040 0.007641199700059132\n",
            "torch.float16 589980672 0.30445769399664235\n",
            "torch.int8 1333002240 0.6878916672083095\n",
            "torch.float32 14825472 0.007650638795048169\n",
            "torch.float16 589980672 0.30445640692052783\n",
            "torch.int8 1333002240 0.6878887591887335\n",
            "torch.float32 14833664 0.007654833890738687\n",
            "torch.float16 589980672 0.30445351103905743\n",
            "torch.int8 1333002240 0.6878822162345826\n",
            "torch.float32 14852096 0.007664272726359994\n",
            "torch.float16 589980672 0.30445222399830907\n",
            "torch.int8 1333002240 0.6878793082949126\n",
            "torch.float32 14860288 0.00766846770677834\n",
            "torch.float16 589980672 0.3037126923991306\n",
            "torch.int8 1337720832 0.6886374670340474\n",
            "torch.float32 14860288 0.00764984056682198\n",
            "torch.float16 589980672 0.3037101308400574\n",
            "torch.int8 1337720832 0.6886316589608387\n",
            "torch.float32 14876672 0.007658210199103977\n",
            "torch.float16 589980672 0.3037072491377479\n",
            "torch.int8 1337720832 0.6886251249955853\n",
            "torch.float32 14895104 0.007667625866666807\n",
            "torch.float16 589980672 0.303704687670492\n",
            "torch.int8 1337720832 0.6886193171305631\n",
            "torch.float32 14911488 0.007675995198944906\n",
            "torch.float16 589980672 0.30370180607147435\n",
            "torch.int8 1337720832 0.6886127833995133\n",
            "torch.float32 14929920 0.007685410529012425\n",
            "torch.float16 589980672 0.3004181272965325\n",
            "torch.int8 1358954496 0.6919795582210584\n",
            "torch.float32 14929920 0.007602314482409089\n",
            "torch.float16 589980672 0.3004153077266008\n",
            "torch.int8 1358954496 0.6919730636570544\n",
            "torch.float32 14948352 0.007611628616344823\n",
            "torch.float16 589980672 0.3004040299761148\n",
            "torch.int8 1358954496 0.6919470866200851\n",
            "torch.float32 15022080 0.007648883403800041\n",
            "torch.float16 589980672 0.30040121067079606\n",
            "torch.int8 1358954496 0.6919405926655874\n",
            "torch.float32 15040512 0.007658196663616527\n",
            "torch.float16 589980672 0.30038993397868735\n",
            "torch.int8 1358954496 0.6919146180664718\n",
            "torch.float32 15114240 0.007695447954840859\n",
            "torch.float16 589980672 0.29717710301415395\n",
            "torch.int8 1380188160 0.6952097559616929\n",
            "torch.float32 15114240 0.007613141024153154\n",
            "torch.float16 589980672 0.2971743439529475\n",
            "torch.int8 1380188160 0.6952033014729434\n",
            "torch.float32 15132672 0.007622354574109062\n",
            "torch.float16 589980672 0.29716330822041803\n",
            "torch.int8 1380188160 0.6951774847164005\n",
            "torch.float32 15206400 0.007659207063181495\n",
            "torch.float16 589980672 0.29716054941535264\n",
            "torch.int8 1380188160 0.6951710308268617\n",
            "torch.float32 15224832 0.007668419757785628\n",
            "torch.float16 589980672 0.2971495147073158\n",
            "torch.int8 1380188160 0.695145216466995\n",
            "torch.float32 15298560 0.007705268825689181\n",
            "torch.float16 589980672 0.2940052631337231\n",
            "torch.int8 1401421824 0.6983710004087424\n",
            "torch.float32 15298560 0.00762373645753441\n",
            "torch.float16 589980672 0.29399446151302644\n",
            "torch.int8 1401421824 0.6983453425733298\n",
            "torch.float32 15372288 0.007660195913643691\n",
            "torch.float16 589980672 0.29399176123186577\n",
            "torch.int8 1401421824 0.6983389284090544\n",
            "torch.float32 15390720 0.007669310359079866\n",
            "torch.float16 589980672 0.2939809606032357\n",
            "torch.int8 1401421824 0.6983132729301659\n",
            "torch.float32 15464448 0.0077057664665983955\n",
            "torch.float16 589980672 0.29397826057007453\n",
            "torch.int8 1401421824 0.6983068593549809\n",
            "torch.float32 15482880 0.007714880074944549\n",
            "torch.float16 589982976 0.2939790711157288\n",
            "torch.int8 1401421824 0.6983060576663662\n",
            "torch.float32 15482880 0.00771487121790493\n",
            "torch.float16 589985280 0.293979881659522\n",
            "torch.int8 1401421824 0.6983052559795924\n",
            "torch.float32 15482880 0.007714862360885648\n",
            "torch.float16 589987584 0.2939806922014541\n",
            "torch.int8 1401421824 0.6983044542946591\n",
            "torch.float32 15482880 0.007714853503886702\n",
            "torch.float16 589989888 0.2939815027415251\n",
            "torch.int8 1401421824 0.6983036526115668\n",
            "torch.float32 15482880 0.007714844646908092\n",
            "torch.float16 589989888 0.29329191749819034\n",
            "torch.int8 1406140416 0.699011334377892\n",
            "torch.float32 15482880 0.007696748123917645\n",
            "torch.float16 589989888 0.29328923014898567\n",
            "torch.int8 1406140416 0.6990049295387492\n",
            "torch.float32 15501312 0.007705840312265205\n",
            "torch.float16 589989888 0.2932868414354798\n",
            "torch.int8 1406140416 0.6989992364469331\n",
            "torch.float32 15517696 0.007713922117587172\n",
            "torch.float16 589989888 0.2932841541792949\n",
            "torch.int8 1406140416 0.6989928318294869\n",
            "torch.float32 15536128 0.007723013991218202\n",
            "torch.float16 589989888 0.293281765548471\n",
            "torch.int8 1406140416 0.6989871389347295\n",
            "torch.float32 15552512 0.007731095516799404\n",
            "torch.float16 589989888 0.2932790783853011\n",
            "torch.int8 1406140416 0.6989807345389687\n",
            "torch.float32 15570944 0.007740187075730243\n",
            "torch.float16 589989888 0.29327668983715505\n",
            "torch.int8 1406140416 0.6989750418412598\n",
            "torch.float32 15587328 0.007748268321585204\n",
            "torch.float16 589989888 0.2929331446072749\n",
            "torch.int8 1408499712 0.6993276634168365\n",
            "torch.float32 15587328 0.007739191975888619\n",
            "torch.float16 589989888 0.2929304638286839\n",
            "torch.int8 1408499712 0.6993212635175328\n",
            "torch.float32 15605760 0.007748272653783384\n",
            "torch.float16 589989888 0.2929292723872819\n",
            "torch.int8 1408499712 0.6993184191554417\n",
            "torch.float32 15613952 0.007752308457276381\n",
            "torch.float16 589989888 0.29292659167956336\n",
            "torch.int8 1408499712 0.6993120194253338\n",
            "torch.float32 15632384 0.007761388895102791\n",
            "torch.float16 589989888 0.29292540026965963\n",
            "torch.int8 1408499712 0.6993091751384395\n",
            "torch.float32 15640576 0.007765424591900856\n",
            "torch.float16 589989888 0.2925826770693896\n",
            "torch.int8 1410859008 0.6996609838982586\n",
            "torch.float32 15640576 0.007756339032351765\n",
            "torch.float16 589989888 0.2925800027015416\n",
            "torch.int8 1410859008 0.6996545886090412\n",
            "torch.float32 15659008 0.007765408689417168\n",
            "torch.float16 589989888 0.2925788141093023\n",
            "torch.int8 1410859008 0.6996517462958054\n",
            "torch.float32 15667200 0.007769439594892279\n",
            "torch.float16 589989888 0.2925761398120727\n",
            "torch.int8 1410859008 0.6996453511754598\n",
            "torch.float32 15685632 0.007778509012467552\n",
            "torch.float16 589989888 0.29257495125121874\n",
            "torch.int8 1410859008 0.6996425089372765\n",
            "torch.float32 15693824 0.007782539811504712\n",
            "torch.float16 589989888 0.2925722770246047\n",
            "torch.int8 1410859008 0.699636113985796\n",
            "torch.float32 15712256 0.007791608989599339\n",
            "torch.float16 589989888 0.29257108849513486\n",
            "torch.int8 1410859008 0.6996332718226624\n",
            "torch.float32 15720448 0.007795639682202766\n",
            "torch.float16 589989888 0.29188809644189156\n",
            "torch.int8 1415577600 0.7003344623929918\n",
            "torch.float32 15720448 0.007777441165116616\n",
            "torch.float16 589989888 0.29188573049702876\n",
            "torch.int8 1415577600 0.7003287857218847\n",
            "torch.float32 15736832 0.007785483781086462\n",
            "torch.float16 589989888 0.291883068854904\n",
            "torch.int8 1415577600 0.7003223995768886\n",
            "torch.float32 15755264 0.00779453156820747\n",
            "torch.float16 589989888 0.2918807029915439\n",
            "torch.int8 1415577600 0.7003167231013332\n",
            "torch.float32 15771648 0.007802573907122926\n",
            "torch.float16 589989888 0.29187804144110735\n",
            "torch.int8 1415577600 0.7003103371763268\n",
            "torch.float32 15790080 0.007811621382565799\n",
            "torch.float16 589989888 0.28884383657238766\n",
            "torch.int8 1436811264 0.7034257473988804\n",
            "torch.float32 15790080 0.007730416028731881\n",
            "torch.float16 589989888 0.2888412301176697\n",
            "torch.int8 1436811264 0.7034193998604327\n",
            "torch.float32 15808512 0.0077393700218976345\n",
            "torch.float16 589989888 0.28883080476918077\n",
            "torch.int8 1436811264 0.7033940108521722\n",
            "torch.float32 15882240 0.007775184378646967\n",
            "torch.float16 589989888 0.2888281985496479\n",
            "torch.int8 1436811264 0.7033876638864743\n",
            "torch.float32 15900672 0.007784137563877752\n",
            "torch.float16 589989888 0.28881777414183585\n",
            "torch.int8 1436811264 0.7033622771690583\n",
            "torch.float32 15974400 0.00781994868910591\n",
            "torch.float16 589989888 0.28584653761546\n",
            "torch.int8 1458044928 0.706413962058588\n",
            "torch.float32 15974400 0.007739500325952033\n",
            "torch.float16 589989888 0.28584398497359664\n",
            "torch.int8 1458044928 0.7064076537021271\n",
            "torch.float32 15992832 0.007748361324276214\n",
            "torch.float16 589989888 0.2858337748620347\n",
            "torch.int8 1458044928 0.7063824214029302\n",
            "torch.float32 16066560 0.007783803735035154\n",
            "torch.float16 589989888 0.28583122244811093\n",
            "torch.int8 1458044928 0.7063761136097775\n",
            "torch.float32 16084992 0.00779266394211164\n",
            "torch.float16 589989888 0.2858210132482461\n",
            "torch.int8 1458044928 0.7063508835636622\n",
            "torch.float32 16158720 0.007828103188091759\n",
            "torch.float16 589989888 0.2829108050379318\n",
            "torch.int8 1479278592 0.7093407969359947\n",
            "torch.float32 16158720 0.007748398026073506\n",
            "torch.float16 589989888 0.2829008033891414\n",
            "torch.int8 1479278592 0.7093157198537579\n",
            "torch.float32 16232448 0.007783476757100729\n",
            "torch.float16 589989888 0.28289830308743813\n",
            "torch.int8 1479278592 0.7093094508602404\n",
            "torch.float32 16250880 0.007792246052321471\n",
            "torch.float16 589989888 0.28288830232256695\n",
            "torch.int8 1479278592 0.7092843759942495\n",
            "torch.float32 16324608 0.007827321683183484\n",
            "torch.float16 589989888 0.2828858022418288\n",
            "torch.int8 1479278592 0.7092781075547568\n",
            "torch.float32 16343040 0.007836090203414296\n",
            "torch.float16 589992192 0.282886594445588\n",
            "torch.int8 1479278592 0.7092773240076107\n",
            "torch.float32 16343040 0.007836081546801255\n",
            "torch.float16 589994496 0.28288738664759694\n",
            "torch.int8 1479278592 0.7092765404621957\n",
            "torch.float32 16343040 0.00783607289020734\n",
            "torch.float16 589996800 0.2828881788478555\n",
            "torch.int8 1479278592 0.709275756918512\n",
            "torch.float32 16343040 0.00783606423363255\n",
            "torch.float16 589999104 0.28288897104636385\n",
            "torch.int8 1479278592 0.7092749733765593\n",
            "torch.float32 16343040 0.007836055577076886\n",
            "torch.float16 589999104 0.28225039642941646\n",
            "torch.int8 1483997184 0.7099312365805519\n",
            "torch.float32 16343040 0.007818366990031581\n",
            "torch.float16 589999104 0.2822479076500283\n",
            "torch.int8 1483997184 0.7099249766700223\n",
            "torch.float32 16361472 0.007827115679949445\n",
            "torch.float16 589999104 0.2822456954385291\n",
            "torch.int8 1483997184 0.7099194123977837\n",
            "torch.float32 16377856 0.00783489216368723\n",
            "torch.float16 589999104 0.28224320674204306\n",
            "torch.int8 1483997184 0.7099131526957738\n",
            "torch.float32 16396288 0.00784364056218309\n",
            "torch.float16 589999104 0.28224099460423285\n",
            "torch.int8 1483997184 0.7099075886088816\n",
            "torch.float32 16412672 0.007851416786885567\n",
            "torch.float16 589999104 0.2822385059906448\n",
            "torch.int8 1483997184 0.7099013291153812\n",
            "torch.float32 16431104 0.007860164893973988\n",
            "torch.float16 589999104 0.28223629392651983\n",
            "torch.int8 1483997184 0.709895765213826\n",
            "torch.float32 16447488 0.007867940859654098\n",
            "torch.float16 589999104 0.281918118285331\n",
            "torch.int8 1486356480 0.7102228106821128\n",
            "torch.float32 16447488 0.007859071032556283\n",
            "torch.float16 589999104 0.28191563536227887\n",
            "torch.int8 1486356480 0.710216555573007\n",
            "torch.float32 16465920 0.007867809064714197\n",
            "torch.float16 589999104 0.28191453185496085\n",
            "torch.int8 1486356480 0.710213775559882\n",
            "torch.float32 16474112 0.007871692585157202\n",
            "torch.float16 589999104 0.2819120489950812\n",
            "torch.int8 1486356480 0.7102075206099235\n",
            "torch.float32 16492544 0.007880430394995197\n",
            "torch.float16 589999104 0.28191094551583934\n",
            "torch.int8 1486356480 0.7102047406675295\n",
            "torch.float32 16500736 0.007884313816631234\n",
            "torch.float16 589999104 0.28159350259443655\n",
            "torch.int8 1488715776 0.7105310616394337\n",
            "torch.float32 16500736 0.007875435766129762\n",
            "torch.float16 589999104 0.2815910253860098\n",
            "torch.int8 1488715776 0.7105248110210168\n",
            "torch.float32 16519168 0.007884163592973456\n",
            "torch.float16 589999104 0.2815899244184768\n",
            "torch.int8 1488715776 0.7105220330036874\n",
            "torch.float32 16527360 0.007888042577835774\n",
            "torch.float16 589999104 0.2815874472730045\n",
            "torch.int8 1488715776 0.7105157825441206\n",
            "torch.float32 16545792 0.007896770182874888\n",
            "torch.float16 589999104 0.2815863463334508\n",
            "torch.int8 1488715776 0.71051300459739\n",
            "torch.float32 16553984 0.00790064906915927\n",
            "torch.float16 589999104 0.28158386925093065\n",
            "torch.int8 1488715776 0.7105067542966671\n",
            "torch.float32 16572416 0.007909376452402257\n",
            "torch.float16 589999104 0.2815827683393551\n",
            "torch.int8 1488715776 0.7105039764205324\n",
            "torch.float32 16580608 0.007913255240112462\n",
            "torch.float16 589999104 0.2809500705823839\n",
            "torch.int8 1493434368 0.7111544547358464\n",
            "torch.float32 16580608 0.007895474681769753\n",
            "torch.float16 589999104 0.28094787866848775\n",
            "torch.int8 1493434368 0.7111489064570066\n",
            "torch.float32 16596992 0.007903214874505744\n",
            "torch.float16 589999104 0.2809454128062359\n",
            "torch.int8 1493434368 0.7111426647467927\n",
            "torch.float32 16615424 0.007911922446971444\n",
            "torch.float16 589999104 0.28094322096501684\n",
            "torch.int8 1493434368 0.7111371166519167\n",
            "torch.float32 16631808 0.007919662383066493\n",
            "torch.float16 589999104 0.28094075518452477\n",
            "torch.int8 1493434368 0.7111308751486569\n",
            "torch.float32 16650240 0.00792836966681831\n",
            "torch.float16 589999104 0.2781286339682535\n",
            "torch.int8 1514668032 0.7140223566433466\n",
            "torch.float32 16650240 0.007849009388399976\n",
            "torch.float16 589999104 0.27812621734616005\n",
            "torch.int8 1514668032 0.7140161525996359\n",
            "torch.float32 16668672 0.007857630054204034\n",
            "torch.float16 589999104 0.27811655127772555\n",
            "torch.int8 1514668032 0.7139913375028781\n",
            "torch.float32 16742400 0.007892111219396347\n",
            "torch.float16 589999104 0.27811413486559633\n",
            "torch.int8 1514668032 0.7139851339981957\n",
            "torch.float32 16760832 0.007900731136207968\n",
            "torch.float16 589999104 0.27810446963696417\n",
            "torch.int8 1514668032 0.71396032105741\n",
            "torch.float32 16834560 0.007935209305625746\n",
            "torch.float16 589999104 0.27534856380957845\n",
            "torch.int8 1535901696 0.7167948616855793\n",
            "torch.float32 16834560 0.00785657450484226\n",
            "torch.float16 589999104 0.2753461952571357\n",
            "torch.int8 1535901696 0.7167886958055142\n",
            "torch.float32 16852992 0.007865108937350093\n",
            "torch.float16 589999104 0.2753367214548374\n",
            "torch.int8 1535901696 0.7167640333459969\n",
            "torch.float32 16926720 0.007899245199165633\n",
            "torch.float16 589999104 0.27533435310612564\n",
            "torch.int8 1535901696 0.7167578679962898\n",
            "torch.float32 16945152 0.007907778897584514\n",
            "torch.float16 589999104 0.2753248801186986\n",
            "torch.int8 1535901696 0.7167332076580677\n",
            "torch.float32 17018880 0.007941912223233677\n",
            "torch.float16 589999104 0.27262352071110907\n",
            "torch.int8 1557135360 0.7195124894070353\n",
            "torch.float32 17018880 0.007863989881855618\n",
            "torch.float16 589999104 0.2726142333367873\n",
            "torch.int8 1557135360 0.7194879780156451\n",
            "torch.float32 17092608 0.007897788647567568\n",
            "torch.float16 589999104 0.27261191159207787\n",
            "torch.int8 1557135360 0.7194818504287396\n",
            "torch.float32 17111040 0.007906237979182607\n",
            "torch.float16 589999104 0.2726026250086937\n",
            "torch.int8 1557135360 0.719457341124805\n",
            "torch.float32 17184768 0.00794003386650126\n",
            "torch.float16 589999104 0.27260030346170605\n",
            "torch.int8 1557135360 0.71945121405973\n",
            "torch.float32 17203200 0.007948482478563937\n",
            "torch.float16 590001408 0.27260107779888304\n",
            "torch.int8 1557135360 0.7194504481839333\n",
            "torch.float32 17203200 0.007948474017183607\n",
            "torch.float16 590003712 0.2726018521344114\n",
            "torch.int8 1557135360 0.7194496823097674\n",
            "torch.float32 17203200 0.007948465555821294\n",
            "torch.float16 590006016 0.27260262646829114\n",
            "torch.int8 1557135360 0.7194489164372319\n",
            "torch.float32 17203200 0.007948457094476993\n",
            "torch.float16 590008320 0.2726034008005223\n",
            "torch.int8 1557135360 0.7194481505663269\n",
            "torch.float32 17203200 0.007948448633150708\n",
            "torch.float16 590008320 0.2720103784861686\n",
            "torch.int8 1561853952 0.7200584639613866\n",
            "torch.float32 17203200 0.007931157552444779\n",
            "torch.float16 590008320 0.2720080670517022\n",
            "torch.int8 1561853952 0.720052345195034\n",
            "torch.float32 17221632 0.007939587753263784\n",
            "torch.float16 590008320 0.27200601247626593\n",
            "torch.int8 1561853952 0.7200469063789088\n",
            "torch.float32 17238016 0.007947081144825334\n",
            "torch.float16 590008320 0.27200370111599975\n",
            "torch.int8 1561853952 0.7200407878089771\n",
            "torch.float32 17256448 0.007955511075023132\n",
            "torch.float16 590008320 0.2720016466065177\n",
            "torch.int8 1561853952 0.720035349167444\n",
            "torch.float32 17272832 0.007963004226038288\n",
            "torch.float16 590008320 0.27199933532044823\n",
            "torch.int8 1561853952 0.7200292307939239\n",
            "torch.float32 17291264 0.00797143388562791\n",
            "torch.float16 590008320 0.2719972808769172\n",
            "torch.int8 1561853952 0.7200237923269746\n",
            "torch.float32 17307648 0.007978926796108255\n",
            "torch.float16 590008320 0.27170176466094575\n",
            "torch.int8 1564213248 0.7203279773878943\n",
            "torch.float32 17307648 0.007970257951159888\n",
            "torch.float16 590008320 0.2716994584684333\n",
            "torch.int8 1564213248 0.7203218632760113\n",
            "torch.float32 17326080 0.007978678255555366\n",
            "torch.float16 590008320 0.2716984335065499\n",
            "torch.int8 1564213248 0.7203191459262682\n",
            "torch.float32 17334272 0.007982420567181915\n",
            "torch.float16 590008320 0.2716961273705862\n",
            "torch.int8 1564213248 0.7203130319643057\n",
            "torch.float32 17352704 0.007990840665108046\n",
            "torch.float16 590008320 0.2716951024338351\n",
            "torch.int8 1564213248 0.7203103146811928\n",
            "torch.float32 17360896 0.007994582884972127\n",
            "torch.float16 590008320 0.2714002421113618\n",
            "torch.int8 1566572544 0.7206138512192709\n",
            "torch.float32 17360896 0.00798590666936726\n",
            "torch.float16 590008320 0.2713979410346075\n",
            "torch.int8 1566572544 0.7206077414687085\n",
            "torch.float32 17379328 0.00799431749668395\n",
            "torch.float16 590008320 0.27139691834635254\n",
            "torch.int8 1566572544 0.7206050260572695\n",
            "torch.float32 17387520 0.007998055596377982\n",
            "torch.float16 590008320 0.271394617325959\n",
            "torch.int8 1566572544 0.7205989164563544\n",
            "torch.float32 17405952 0.008006466217686576\n",
            "torch.float16 590008320 0.2713935946627528\n",
            "torch.int8 1566572544 0.7205962011114241\n",
            "torch.float32 17414144 0.008010204225823134\n",
            "torch.float16 590008320 0.2713912936987179\n",
            "torch.int8 1566572544 0.7205900916601509\n",
            "torch.float32 17432576 0.008018614641131197\n",
            "torch.float16 590008320 0.2713902710605595\n",
            "torch.int8 1566572544 0.7205873763817269\n",
            "torch.float32 17440768 0.008022352557713648\n",
            "torch.float16 590008320 0.2708025094046274\n",
            "torch.int8 1571291136 0.7211925123938043\n",
            "torch.float32 17440768 0.008004978201568284\n",
            "torch.float16 590008320 0.27080047300079146\n",
            "torch.int8 1571291136 0.721187089108762\n",
            "torch.float32 17457152 0.008012437890446549\n",
            "torch.float16 590008320 0.27079818208308465\n",
            "torch.int8 1571291136 0.7211809880105842\n",
            "torch.float32 17475584 0.00802082990633122\n",
            "torch.float16 590008320 0.27079614574432986\n",
            "torch.int8 1571291136 0.7211755648988638\n",
            "torch.float32 17491968 0.00802828935680628\n",
            "torch.float16 590008320 0.2707938548998376\n",
            "torch.int8 1571291136 0.7211694639956686\n",
            "torch.float32 17510400 0.008036681104493773\n",
            "torch.float16 590008320 0.26818029490616624\n",
            "torch.int8 1592524800 0.7238605898123325\n",
            "torch.float32 17510400 0.00795911528150134\n",
            "torch.float16 590008320 0.26817804810616536\n",
            "torch.int8 1592524800 0.7238545253474753\n",
            "torch.float32 17528832 0.007967426546359364\n",
            "torch.float16 590008320 0.2681690612826205\n",
            "torch.int8 1592524800 0.7238302685041679\n",
            "torch.float32 17602560 0.008000670213211578\n",
            "torch.float16 590008320 0.2681668146708443\n",
            "torch.int8 1592524800 0.7238242045473585\n",
            "torch.float32 17620992 0.008008980781797162\n",
            "torch.float16 590008320 0.2681578286001508\n",
            "torch.int8 1592524800 0.7237999497361146\n",
            "torch.float32 17694720 0.008042221663734606\n",
            "torch.float16 590008320 0.2655946632150147\n",
            "torch.int8 1613758464 0.7264399860606362\n",
            "torch.float32 17694720 0.007965350724349082\n",
            "torch.float16 590008320 0.26559245953054605\n",
            "torch.int8 1613758464 0.7264339586634916\n",
            "torch.float32 17713152 0.007973581805962348\n",
            "torch.float16 590008320 0.26558364515834626\n",
            "torch.int8 1613758464 0.726409850075087\n",
            "torch.float32 17786880 0.008006504766566827\n",
            "torch.float16 590008320 0.2655814416567105\n",
            "torch.int8 1613758464 0.7264038231780168\n",
            "torch.float32 17805312 0.0080147351652728\n",
            "torch.float16 590008320 0.2655726280157966\n",
            "torch.int8 1613758464 0.7263797165897853\n",
            "torch.float32 17879040 0.00804765539441808\n",
            "torch.float16 590008320 0.2630584135958713\n",
            "torch.int8 1634992128 0.728970119325466\n",
            "torch.float32 17879040 0.007971467078662767\n",
            "torch.float16 590008320 0.26304976661626456\n",
            "torch.int8 1634992128 0.7289461573861021\n",
            "torch.float32 17952768 0.008004075997633291\n",
            "torch.float16 590008320 0.26304760496018537\n",
            "torch.int8 1634992128 0.7289401671474003\n",
            "torch.float32 17971200 0.00801222789241427\n",
            "torch.float16 590008320 0.2630389586911326\n",
            "torch.int8 1634992128 0.7289162071770767\n",
            "torch.float32 18044928 0.008044834131790654\n",
            "torch.float16 590008320 0.263036797212681\n",
            "torch.int8 1634992128 0.7289102174306046\n",
            "torch.float32 18063360 0.008052985356714382\n",
            "torch.float16 590010624 0.26303755419581676\n",
            "torch.int8 1634992128 0.7289094687192171\n",
            "torch.float32 18063360 0.008052977084966098\n",
            "torch.float16 590012928 0.26303831117739745\n",
            "torch.int8 1634992128 0.7289087200093678\n",
            "torch.float32 18063360 0.008052968813234808\n",
            "torch.float16 590015232 0.26303906815742306\n",
            "torch.int8 1634992128 0.7289079713010564\n",
            "torch.float32 18063360 0.00805296054152051\n",
            "torch.float16 590017536 0.2630398251358936\n",
            "torch.int8 1634992128 0.7289072225942832\n",
            "torch.float32 18063360 0.008052952269823204\n",
            "torch.float16 590017536 0.2624876486771272\n",
            "torch.int8 1639710720 0.729476303910193\n",
            "torch.float32 18063360 0.008036047412679735\n",
            "torch.float16 590017536 0.26248549628335854\n",
            "torch.int8 1639710720 0.7294703222182589\n",
            "torch.float32 18081792 0.008044181498382553\n",
            "torch.float16 590017536 0.26248358307408676\n",
            "torch.int8 1639710720 0.7294650052411165\n",
            "torch.float32 18098176 0.008051411684796844\n",
            "torch.float16 590017536 0.26248143074699304\n",
            "torch.int8 1639710720 0.7294590237344778\n",
            "torch.float32 18116608 0.008059545518529165\n",
            "torch.float16 590017536 0.2624795175969865\n",
            "torch.int8 1639710720 0.7294537069220386\n",
            "torch.float32 18132992 0.008066775480974883\n",
            "torch.float16 590017536 0.2624773653365646\n",
            "torch.int8 1639710720 0.7294477256006869\n",
            "torch.float32 18151424 0.008074909062748412\n",
            "torch.float16 590017536 0.2624754522458206\n",
            "torch.int8 1639710720 0.7294424089529434\n",
            "torch.float32 18167808 0.008082138801235964\n",
            "torch.float16 590017536 0.2622002580187166\n",
            "torch.int8 1642070016 0.7297260769551062\n",
            "torch.float32 18167808 0.008073665026177295\n",
            "torch.float16 590017536 0.26219811033554447\n",
            "torch.int8 1642070016 0.7297200997664199\n",
            "torch.float32 18186240 0.00808178989803566\n",
            "torch.float16 590017536 0.2621971558209835\n",
            "torch.int8 1642070016 0.7297174432695454\n",
            "torch.float32 18194432 0.008085400909471087\n",
            "torch.float16 590017536 0.2621950081886311\n",
            "torch.int8 1642070016 0.7297114662222949\n",
            "torch.float32 18212864 0.00809352558907406\n",
            "torch.float16 590017536 0.2621940536966563\n",
            "torch.int8 1642070016 0.7297088097882795\n",
            "torch.float32 18221056 0.00809713651506416\n",
            "torch.float16 590017536 0.26191944891314084\n",
            "torch.int8 1644429312 0.7299918949792965\n",
            "torch.float32 18221056 0.00808865610756267\n",
            "torch.float16 590017536 0.2619173058277031\n",
            "torch.int8 1644429312 0.7299859220169745\n",
            "torch.float32 18239488 0.008096772155322382\n",
            "torch.float16 590017536 0.2619163533565435\n",
            "torch.int8 1644429312 0.7299832673984282\n",
            "torch.float32 18247680 0.00810037924502829\n",
            "torch.float16 590017536 0.26191421032176243\n",
            "torch.int8 1644429312 0.7299772945772904\n",
            "torch.float32 18266112 0.008108495100947083\n",
            "torch.float16 590017536 0.2619132578731165\n",
            "torch.int8 1644429312 0.7299746400214917\n",
            "torch.float32 18274304 0.00811210210539187\n",
            "torch.float16 590017536 0.2619111148889902\n",
            "torch.int8 1644429312 0.7299686673415332\n",
            "torch.float32 18292736 0.008120217769476546\n",
            "torch.float16 590017536 0.2619101624628571\n",
            "torch.int8 1644429312 0.7299660128484796\n",
            "torch.float32 18300928 0.008123824688663238\n",
            "torch.float16 590017536 0.26136271368805614\n",
            "torch.int8 1649147904 0.7305304421704678\n",
            "torch.float32 18300928 0.008106844141476043\n",
            "torch.float16 590017536 0.26136081681123735\n",
            "torch.int8 1649147904 0.7305251402425776\n",
            "torch.float32 18317312 0.008114042946185044\n",
            "torch.float16 590017536 0.26135868285772734\n",
            "torch.int8 1649147904 0.7305191756656904\n",
            "torch.float32 18335744 0.008122141476582278\n",
            "torch.float16 590017536 0.26135678603941653\n",
            "torch.int8 1649147904 0.7305138739013348\n",
            "torch.float32 18352128 0.008129340059248655\n",
            "torch.float16 590017536 0.2613546521517265\n",
            "torch.int8 1649147904 0.7305079095084198\n",
            "torch.float32 18370560 0.008137438339853717\n",
            "torch.float16 590017536 0.258919339379094\n",
            "torch.int8 1670381568 0.7330190472467164\n",
            "torch.float32 18370560 0.008061613374189626\n",
            "torch.float16 590017536 0.2589172451074626\n",
            "torch.int8 1670381568 0.7330131182149198\n",
            "torch.float32 18388992 0.008069636677617611\n",
            "torch.float16 590017536 0.25890886835971677\n",
            "torch.int8 1670381568 0.7329894030468431\n",
            "torch.float32 18462720 0.00810172859344016\n",
            "torch.float16 590017536 0.2589067742574711\n",
            "torch.int8 1670381568 0.7329834744945897\n",
            "torch.float32 18481152 0.008109751247939199\n",
            "torch.float16 590017536 0.25889839818722726\n",
            "torch.int8 1670381568 0.7329597612445693\n",
            "torch.float32 18554880 0.008141840568203416\n",
            "torch.float16 590017536 0.25650843595869965\n",
            "torch.int8 1691615232 0.7354248830398032\n",
            "torch.float32 18554880 0.008066681001497144\n",
            "torch.float16 590017536 0.2565063805065675\n",
            "torch.int8 1691615232 0.7354189899367626\n",
            "torch.float32 18573312 0.008074629556669983\n",
            "torch.float16 590017536 0.256498159027443\n",
            "torch.int8 1691615232 0.7353954184690213\n",
            "torch.float32 18647040 0.008106422503535708\n",
            "torch.float16 590017536 0.256496103740009\n",
            "torch.int8 1691615232 0.73538952583818\n",
            "torch.float32 18665472 0.008114370421811044\n",
            "torch.float16 590017536 0.2564878829196375\n",
            "torch.int8 1691615232 0.7353659562591227\n",
            "torch.float32 18739200 0.008146160821239846\n",
            "torch.float16 590017536 0.2541420161406638\n",
            "torch.int8 1712848896 0.737786328733372\n",
            "torch.float32 18739200 0.008071655125964132\n",
            "torch.float16 590017536 0.254133945524674\n",
            "torch.int8 1712848896 0.7377628993523033\n",
            "torch.float32 18812928 0.008103155123022673\n",
            "torch.float16 590017536 0.2541319279507675\n",
            "torch.int8 1712848896 0.737757042239544\n",
            "torch.float32 18831360 0.008111029809688513\n",
            "torch.float16 590017536 0.2541238579754824\n",
            "torch.int8 1712848896 0.7377336147184715\n",
            "torch.float32 18905088 0.008142527306046032\n",
            "torch.float16 590017536 0.2541218405617426\n",
            "torch.int8 1712848896 0.7377277580706836\n",
            "torch.float32 18923520 0.00815040136757384\n",
            "torch.float16 590019840 0.2541225807241214\n",
            "torch.int8 1712848896 0.7377270259962483\n",
            "torch.float32 18923520 0.00815039327963027\n",
            "torch.float16 590022144 0.25412332088503126\n",
            "torch.int8 1712848896 0.7377262939232659\n",
            "torch.float32 18923520 0.008150385191702749\n",
            "torch.float16 590024448 0.25412406104447216\n",
            "torch.int8 1712848896 0.7377255618517365\n",
            "torch.float32 18923520 0.008150377103791281\n",
            "torch.float16 590026752 0.2541248012024441\n",
            "torch.int8 1712848896 0.7377248297816601\n",
            "torch.float32 18923520 0.008150369015895866\n",
            "torch.float16 590026752 0.25360939088640355\n",
            "torch.int8 1717567488 0.738256770496349\n",
            "torch.float32 18923520 0.008133838617247435\n",
            "torch.float16 590026752 0.2536073816641491\n",
            "torch.int8 1717567488 0.7382509216516844\n",
            "torch.float32 18941952 0.008141696684166606\n",
            "torch.float16 590026752 0.2536055957155382\n",
            "torch.int8 1717567488 0.7382457227564497\n",
            "torch.float32 18958336 0.008148681528012028\n",
            "torch.float16 590026752 0.2536035865534176\n",
            "torch.int8 1717567488 0.7382398740868347\n",
            "torch.float32 18976768 0.008156539359747752\n",
            "torch.float16 590026752 0.2536018006582578\n",
            "torch.int8 1717567488 0.7382346753471962\n",
            "torch.float32 18993152 0.008163523994545913\n",
            "torch.float16 590026752 0.2535997915562684\n",
            "torch.int8 1717567488 0.7382288268526229\n",
            "torch.float32 19011584 0.008171381591108749\n",
            "torch.float16 590026752 0.2535980057145573\n",
            "torch.int8 1717567488 0.7382236282685737\n",
            "torch.float32 19027968 0.008178366016869035\n",
            "torch.float16 590026752 0.2533411068257243\n",
            "torch.int8 1719926784 0.7384888119746957\n",
            "torch.float32 19027968 0.00817008119958002\n",
            "torch.float16 590026752 0.2533391018521694\n",
            "torch.int8 1719926784 0.7384829674808544\n",
            "torch.float32 19046400 0.008177930666976196\n",
            "torch.float16 590026752 0.25333821076299823\n",
            "torch.int8 1719926784 0.7384803699577299\n",
            "torch.float32 19054592 0.008181419279271832\n",
            "torch.float16 590026752 0.25333620583528255\n",
            "torch.int8 1719926784 0.7384745255975097\n",
            "torch.float32 19073024 0.008189268567207753\n",
            "torch.float16 590026752 0.253335314766484\n",
            "torch.int8 1719926784 0.7384719281337714\n",
            "torch.float32 19081216 0.008192757099744642\n",
            "torch.float16 590026752 0.2530789475533567\n",
            "torch.int8 1722286080 0.738736586154311\n",
            "torch.float32 19081216 0.008184466292332234\n",
            "torch.float16 590026752 0.25307694672716174\n",
            "torch.int8 1722286080 0.7387307457494608\n",
            "torch.float32 19099648 0.008192307523377415\n",
            "torch.float16 590026752 0.2530760574812301\n",
            "torch.int8 1722286080 0.7387281500436143\n",
            "torch.float32 19107840 0.00819579247515568\n",
            "torch.float16 590026752 0.2530740567007321\n",
            "torch.int8 1722286080 0.7387223097721535\n",
            "torch.float32 19126272 0.008203633527114418\n",
            "torch.float16 590026752 0.25307316747510983\n",
            "torch.int8 1722286080 0.7387197141255901\n",
            "torch.float32 19134464 0.008207118399300072\n",
            "torch.float16 590026752 0.25307116674030733\n",
            "torch.int8 1722286080 0.7387138739875142\n",
            "torch.float32 19152896 0.008214959272178501\n",
            "torch.float16 590026752 0.25307027753499384\n",
            "torch.int8 1722286080 0.7387112784002319\n",
            "torch.float32 19161088 0.00821844406477427\n",
            "torch.float16 590026752 0.25255913156526716\n",
            "torch.int8 1727004672 0.739239023808668\n",
            "torch.float32 19161088 0.008201844626064788\n",
            "torch.float16 590026752 0.2525573603500224\n",
            "torch.int8 1727004672 0.7392338394725468\n",
            "torch.float32 19177472 0.00820880017743071\n",
            "torch.float16 590026752 0.2525553677625673\n",
            "torch.int8 1727004672 0.7392280071813285\n",
            "torch.float32 19195904 0.00821662505610413\n",
            "torch.float16 590026752 0.2525535966001136\n",
            "torch.int8 1727004672 0.7392228229997265\n",
            "torch.float32 19212288 0.008223580400159896\n",
            "torch.float16 590026752 0.25255160407204713\n",
            "torch.int8 1727004672 0.7392169908823383\n",
            "torch.float32 19230720 0.008231405045614607\n",
            "torch.float16 590026752 0.25027690399184793\n",
            "torch.int8 1748238336 0.7415658301777137\n",
            "torch.float32 19230720 0.00815726583043833\n",
            "torch.float16 590026752 0.2502749472257695\n",
            "torch.int8 1748238336 0.741560032316072\n",
            "torch.float32 19249152 0.008165020458158504\n",
            "torch.float16 590026752 0.2502671204674217\n",
            "torch.int8 1748238336 0.7415368417760775\n",
            "torch.float32 19322880 0.0081960377565008\n",
            "torch.float16 590026752 0.25026516385432274\n",
            "torch.int8 1748238336 0.7415310443677112\n",
            "torch.float32 19341312 0.00820379177796599\n",
            "torch.float16 590026752 0.2502573377078572\n",
            "torch.int8 1748238336 0.7415078556407123\n",
            "torch.float32 19415040 0.008234806651430537\n",
            "torch.float16 590026752 0.24802359546789327\n",
            "torch.int8 1769472000 0.7438150999630675\n",
            "torch.float32 19415040 0.008161304569039213\n",
            "torch.float16 590026752 0.24802167377762166\n",
            "torch.int8 1769472000 0.7438093368733149\n",
            "torch.float32 19433472 0.008168989349063523\n",
            "torch.float16 590026752 0.24801398731431168\n",
            "torch.int8 1769472000 0.7437862854073263\n",
            "torch.float32 19507200 0.008199727278362018\n",
            "torch.float16 590026752 0.24801206577292484\n",
            "torch.int8 1769472000 0.7437805227640744\n",
            "torch.float32 19525632 0.008207411463000793\n",
            "torch.float16 590026752 0.24800437990511928\n",
            "torch.int8 1769472000 0.7437574730839852\n",
            "torch.float32 19599360 0.00823814701089553\n",
            "torch.float16 590026752 0.24581049915659658\n",
            "torch.int8 1790705664 0.7460242296104986\n",
            "torch.float32 19599360 0.008165271232904763\n",
            "torch.float16 590026752 0.24580294913371437\n",
            "torch.int8 1790705664 0.7460013156177132\n",
            "torch.float32 19673088 0.008195735248572401\n",
            "torch.float16 590026752 0.24580106170046123\n",
            "torch.int8 1790705664 0.7459955873394524\n",
            "torch.float32 19691520 0.00820335096008641\n",
            "torch.float16 590026752 0.2457935122572983\n",
            "torch.int8 1790705664 0.74597267510609\n",
            "torch.float32 19765248 0.008233812636611671\n",
            "torch.float16 590026752 0.24579162496896667\n",
            "torch.int8 1790705664 0.7459669472676596\n",
            "torch.float32 19783680 0.008241427763373764\n",
            "torch.float16 590029056 0.24579234885264784\n",
            "torch.int8 1790705664 0.7459662312940405\n",
            "torch.float32 19783680 0.008241419853311685\n",
            "torch.float16 590031360 0.24579307273493942\n",
            "torch.int8 1790705664 0.7459655153217958\n",
            "torch.float32 19783680 0.00824141194326479\n",
            "torch.float16 590033664 0.24579379661584147\n",
            "torch.int8 1790705664 0.7459647993509254\n",
            "torch.float32 19783680 0.008241404033233079\n",
            "torch.float16 590035968 0.24579452049535397\n",
            "torch.int8 1790705664 0.7459640833814295\n",
            "torch.float32 19783680 0.00824139612321655\n",
            "torch.float16 590035968 0.24531232238807496\n",
            "torch.int8 1795424256 0.7464624494065447\n",
            "torch.float32 19783680 0.008225228205380372\n",
            "torch.float16 590035968 0.24531044251133197\n",
            "torch.int8 1795424256 0.7464567291174679\n",
            "torch.float32 19802112 0.008232828371200172\n",
            "torch.float16 590035968 0.24530877153397007\n",
            "torch.int8 1795424256 0.7464516444896665\n",
            "torch.float32 19818496 0.008239583976363454\n",
            "torch.float16 590035968 0.24530689171164827\n",
            "torch.int8 1795424256 0.7464459243661883\n",
            "torch.float32 19836928 0.008247183922163477\n",
            "torch.float16 590035968 0.24530522078265973\n",
            "torch.int8 1795424256 0.7464408398855824\n",
            "torch.float32 19853312 0.00825393933175787\n",
            "torch.float16 590035968 0.24530334101475676\n",
            "torch.int8 1795424256 0.7464351199276956\n",
            "torch.float32 19871744 0.00826153905754767\n",
            "torch.float16 590035968 0.24530167013413948\n",
            "torch.int8 1795424256 0.7464300355942789\n",
            "torch.float32 19888128 0.008268294271581666\n",
            "torch.float16 590035968 0.24506130073147486\n",
            "torch.int8 1797783552 0.7466785070410674\n",
            "torch.float32 19888128 0.008260192227457675\n",
            "torch.float16 590035968 0.24505942470000575\n",
            "torch.int8 1797783552 0.7466727909513694\n",
            "torch.float32 19906560 0.008267784348624907\n",
            "torch.float16 590035968 0.24505859091746152\n",
            "torch.int8 1797783552 0.746670250495151\n",
            "torch.float32 19914752 0.008271158587387504\n",
            "torch.float16 590035968 0.24505671492748118\n",
            "torch.int8 1797783552 0.7466645345318653\n",
            "torch.float32 19933184 0.00827875054065353\n",
            "torch.float16 590035968 0.24505588116337612\n",
            "torch.int8 1797783552 0.7466619941318293\n",
            "torch.float32 19941376 0.008282124704794608\n",
            "torch.float16 590035968 0.24481599297766474\n",
            "torch.int8 1800142848 0.7469099897902519\n",
            "torch.float32 19941376 0.008274017232083337\n",
            "torch.float16 590035968 0.24481412070013778\n",
            "torch.int8 1800142848 0.7469042776520394\n",
            "torch.float32 19959808 0.008281601647822894\n",
            "torch.float16 590035968 0.24481328858598464\n",
            "torch.int8 1800142848 0.7469017389519892\n",
            "torch.float32 19968000 0.008284972462026148\n",
            "torch.float16 590035968 0.244811416349822\n",
            "torch.int8 1800142848 0.7468960269399751\n",
            "torch.float32 19986432 0.008292556710202802\n",
            "torch.float16 590035968 0.2448105842540527\n",
            "torch.int8 1800142848 0.7468934882960124\n",
            "torch.float32 19994624 0.00829592744993489\n",
            "torch.float16 590035968 0.244808712059253\n",
            "torch.int8 1800142848 0.7468877764101928\n",
            "torch.float32 20013056 0.008303511530554196\n",
            "torch.float16 590035968 0.24480787998186693\n",
            "torch.int8 1800142848 0.7468852378223155\n",
            "torch.float32 20021248 0.008306882195817584\n",
            "torch.float16 590035968 0.24432954150816746\n",
            "torch.int8 1804861440 0.7473798073967093\n",
            "torch.float32 20021248 0.008290651095123263\n",
            "torch.float16 590035968 0.24432788386721987\n",
            "torch.int8 1804861440 0.7473747368376418\n",
            "torch.float32 20037632 0.008297379295138307\n",
            "torch.float16 590035968 0.24432601904803902\n",
            "torch.int8 1804861440 0.74736903254093\n",
            "torch.float32 20056064 0.008304948411030919\n",
            "torch.float16 590035968 0.24432436145488684\n",
            "torch.int8 1804861440 0.7473639621280639\n",
            "torch.float32 20072448 0.008311676417049241\n",
            "torch.float16 590035968 0.24432249668947462\n",
            "torch.int8 1804861440 0.7473582579958251\n",
            "torch.float32 20090880 0.008319245314700258\n",
            "torch.float16 590035968 0.2421930265900502\n",
            "torch.int8 1826095104 0.7495602371125831\n",
            "torch.float32 20090880 0.008246736297366719\n",
            "torch.float16 590035968 0.2421911942167362\n",
            "torch.int8 1826095104 0.7495545661228149\n",
            "torch.float32 20109312 0.008254239660448876\n",
            "torch.float16 590035968 0.24218386500073763\n",
            "torch.int8 1826095104 0.7495318830218227\n",
            "torch.float32 20183040 0.0082842519774396\n",
            "torch.float16 590035968 0.2421820327660492\n",
            "torch.int8 1826095104 0.7495262124610851\n",
            "torch.float32 20201472 0.008291754772865687\n",
            "torch.float16 590035968 0.24217470410452135\n",
            "torch.int8 1826095104 0.7495035310761177\n",
            "torch.float32 20275200 0.008321764819360963\n",
            "torch.float16 590035968 0.24008234866183426\n",
            "torch.int8 1847328768 0.7516677853984872\n",
            "torch.float32 20275200 0.00824986593967848\n",
            "torch.float16 590035968 0.24008054808698293\n",
            "torch.int8 1847328768 0.7516621480239845\n",
            "torch.float32 20293632 0.008257303889032636\n",
            "torch.float16 590035968 0.2400733460576494\n",
            "torch.int8 1847328768 0.7516395993715338\n",
            "torch.float32 20367360 0.00828705457081682\n",
            "torch.float16 590035968 0.2400715456178309\n",
            "torch.int8 1847328768 0.7516339624198017\n",
            "torch.float32 20385792 0.008294491962367305\n",
            "torch.float16 590035968 0.24006434412859826\n",
            "torch.int8 1847328768 0.7516114154583394\n",
            "torch.float32 20459520 0.008324240413062308\n",
            "torch.float16 590035968 0.23800814141526794\n",
            "torch.int8 1868562432 0.7537389170802431\n",
            "torch.float32 20459520 0.008252941504488931\n",
            "torch.float16 590035968 0.23800106318516895\n",
            "torch.int8 1868562432 0.7537165013029594\n",
            "torch.float32 20533248 0.008282435511871614\n",
            "torch.float16 590035968 0.23799929369342576\n",
            "torch.int8 1868562432 0.7537108975669597\n",
            "torch.float32 20551680 0.008289808739614506\n",
            "torch.float16 590035968 0.23799221598956186\n",
            "torch.int8 1868562432 0.7536884834561899\n",
            "torch.float32 20625408 0.008319300554248307\n",
            "torch.float16 590035968 0.2379904466293701\n",
            "torch.int8 1868562432 0.7536828801367953\n",
            "torch.float32 20643840 0.008326673233834545\n",
            "torch.float16 590038272 0.23799115477654148\n",
            "torch.int8 1868562432 0.7536821797277292\n",
            "torch.float32 20643840 0.008326665495729331\n",
            "torch.float16 590040576 0.23799186292239666\n",
            "torch.int8 1868562432 0.7536814793199649\n",
            "torch.float32 20643840 0.0083266577576385\n",
            "torch.float16 590042880 0.23799257106693567\n",
            "torch.int8 1868562432 0.7536807789135023\n",
            "torch.float32 20643840 0.008326650019562051\n",
            "torch.float16 590045184 0.2379932792101585\n",
            "torch.int8 1868562432 0.7536800785083415\n",
            "torch.float32 20643840 0.008326642281499985\n",
            "torch.float16 590045184 0.23754118310528033\n",
            "torch.int8 1873281024 0.7541479920453533\n",
            "torch.float32 20643840 0.0083108248493663\n",
            "torch.float16 590045184 0.23753942047267465\n",
            "torch.int8 1873281024 0.7541423960227062\n",
            "torch.float32 20662272 0.008318183504619152\n",
            "torch.float16 590045184 0.23753785371009645\n",
            "torch.int8 1873281024 0.7541374218500725\n",
            "torch.float32 20678656 0.008324724439831049\n",
            "torch.float16 590045184 0.2375360911269006\n",
            "torch.int8 1873281024 0.7541318259842922\n",
            "torch.float32 20697088 0.008332082888807175\n",
            "torch.float16 590045184 0.23753452440824133\n",
            "torch.int8 1873281024 0.7541268519510929\n",
            "torch.float32 20713472 0.008338623640665836\n",
            "torch.float16 590045184 0.23753276187445327\n",
            "torch.int8 1873281024 0.7541212562421729\n",
            "torch.float32 20731904 0.008345981883373911\n",
            "torch.float16 590045184 0.2375311951997111\n",
            "torch.int8 1873281024 0.7541162823484019\n",
            "torch.float32 20748288 0.008352522451887046\n",
            "torch.float16 590045184 0.23730580958899708\n",
            "torch.int8 1875640320 0.7543495933954875\n",
            "torch.float32 20748288 0.008344597015515464\n",
            "torch.float16 590045184 0.2373040504477435\n",
            "torch.int8 1875640320 0.7543440014232906\n",
            "torch.float32 20766720 0.00835194812896594\n",
            "torch.float16 590045184 0.23730326861555798\n",
            "torch.int8 1875640320 0.754341516128926\n",
            "torch.float32 20774912 0.008355215255516057\n",
            "torch.float16 590045184 0.23730150951197634\n",
            "torch.int8 1875640320 0.7543359242764811\n",
            "torch.float32 20793344 0.008362566211542532\n",
            "torch.float16 590045184 0.23730072769653365\n",
            "torch.int8 1875640320 0.7543334390353388\n",
            "torch.float32 20801536 0.008365833268127551\n",
            "torch.float16 590045184 0.23707577903274782\n",
            "torch.int8 1877999616 0.7545663180709924\n",
            "torch.float32 20801536 0.008357902896259803\n",
            "torch.float16 590045184 0.23707402330024868\n",
            "torch.int8 1877999616 0.7545607299142739\n",
            "torch.float32 20819968 0.008365246785477417\n",
            "torch.float16 590045184 0.2370732429830408\n",
            "torch.int8 1877999616 0.7545582463156336\n",
            "torch.float32 20828160 0.008368510701325632\n",
            "torch.float16 590045184 0.2370714872881042\n",
            "torch.int8 1877999616 0.7545526582784694\n",
            "torch.float32 20846592 0.0083758544334264\n",
            "torch.float16 590045184 0.23707070698759056\n",
            "torch.int8 1877999616 0.7545501747329635\n",
            "torch.float32 20854784 0.008379118279446022\n",
            "torch.float16 590045184 0.2370689513302153\n",
            "torch.int8 1877999616 0.7545445868153497\n",
            "torch.float32 20873216 0.008386461854434984\n",
            "torch.float16 590045184 0.23706817104639527\n",
            "torch.int8 1877999616 0.7545421033229764\n",
            "torch.float32 20881408 0.008389725630628259\n",
            "torch.float16 590045184 0.23661957950065704\n",
            "torch.int8 1882718208 0.755006570302234\n",
            "torch.float32 20881408 0.008373850197109067\n",
            "torch.float16 590045184 0.23661802484871977\n",
            "torch.int8 1882718208 0.755001609713471\n",
            "torch.float32 20897792 0.008380365437809213\n",
            "torch.float16 590045184 0.23661627588970915\n",
            "torch.int8 1882718208 0.7549960291290283\n",
            "torch.float32 20916224 0.008387694981262581\n",
            "torch.float16 590045184 0.23661472128118263\n",
            "torch.int8 1882718208 0.7549910686787804\n",
            "torch.float32 20932608 0.008394210040036957\n",
            "torch.float16 590045184 0.23661297237100803\n",
            "torch.int8 1882718208 0.7549854882501639\n",
            "torch.float32 20951040 0.008401539378828121\n",
            "torch.float16 590045184 0.23461525306962042\n",
            "torch.int8 1903951872 0.7570541416056326\n",
            "torch.float32 20951040 0.008330605324747027\n",
            "torch.float16 590045184 0.2346135335921258\n",
            "torch.int8 1903951872 0.7570485932129273\n",
            "torch.float32 20969472 0.00833787319494695\n",
            "torch.float16 590045184 0.23460665593417843\n",
            "torch.int8 1903951872 0.7570264004553572\n",
            "torch.float32 21043200 0.00836694361046442\n",
            "torch.float16 590045184 0.23460493658269654\n",
            "torch.int8 1903951872 0.7570208524692685\n",
            "torch.float32 21061632 0.008374210948034942\n",
            "torch.float16 590045184 0.23459805942877243\n",
            "torch.int8 1903951872 0.7569986613380757\n",
            "torch.float32 21135360 0.008403279233151914\n",
            "torch.float16 590045184 0.23263407780630782\n",
            "torch.int8 1925185536 0.7590329925875684\n",
            "torch.float32 21135360 0.008332929606123734\n",
            "torch.float16 590045184 0.23263238724586566\n",
            "torch.int8 1925185536 0.7590274766667555\n",
            "torch.float32 21153792 0.008340136087378852\n",
            "torch.float16 590045184 0.23262562524979713\n",
            "torch.int8 1925185536 0.7590054137851684\n",
            "torch.float32 21227520 0.008368960965034457\n",
            "torch.float16 590045184 0.23262393481220237\n",
            "torch.int8 1925185536 0.7589998982651791\n",
            "torch.float32 21245952 0.008376166922618557\n",
            "torch.float16 590045184 0.23261717330749668\n",
            "torch.int8 1925185536 0.7589778369867991\n",
            "torch.float32 21319680 0.00840498970570425\n",
            "torch.float16 590045184 0.2306860818724688\n",
            "torch.int8 1946419200 0.76097870316546\n",
            "torch.float32 21319680 0.008335214962071168\n",
            "torch.float16 590045184 0.23067943254915713\n",
            "torch.int8 1946419200 0.7609567686239845\n",
            "torch.float32 21393408 0.008363798826858315\n",
            "torch.float16 590045184 0.2306777702782228\n",
            "torch.int8 1946419200 0.7609512851861905\n",
            "torch.float32 21411840 0.008370944535586722\n",
            "torch.float16 590045184 0.2306711214340444\n",
            "torch.int8 1946419200 0.760929352225262\n",
            "torch.float32 21485568 0.008399526340693627\n",
            "torch.float16 590045184 0.2306694592828869\n",
            "torch.int8 1946419200 0.7609238691825833\n",
            "torch.float32 21504000 0.008406671534529803\n",
            "torch.float16 590047488 0.23067015222967246\n",
            "torch.int8 1946419200 0.7609231838078044\n",
            "torch.float32 21504000 0.008406663962523092\n",
            "torch.float16 590049792 0.23067084517520972\n",
            "torch.int8 1946419200 0.7609224984342603\n",
            "torch.float32 21504000 0.008406656390530022\n",
            "torch.float16 590052096 0.2306715381194987\n",
            "torch.int8 1946419200 0.7609218130619507\n",
            "torch.float32 21504000 0.008406648818550591\n",
            "torch.float16 590054400 0.2306722310625394\n",
            "torch.int8 1946419200 0.7609211276908758\n",
            "torch.float32 21504000 0.008406641246584802\n",
            "torch.float16 590054400 0.23024750332949337\n",
            "torch.int8 1951137792 0.7613613342427755\n",
            "torch.float32 21504000 0.008391162427731113\n",
            "torch.float16 590054400 0.23024584730351935\n",
            "torch.int8 1951137792 0.761355858247914\n",
            "torch.float32 21522432 0.00839829444856674\n",
            "torch.float16 590054400 0.2302443753004295\n",
            "torch.int8 1951137792 0.761350990763054\n",
            "torch.float32 21538816 0.008404633936516524\n",
            "torch.float16 590054400 0.23024271931945092\n",
            "torch.int8 1951137792 0.7613455149169792\n",
            "torch.float32 21557248 0.008411765763569926\n",
            "torch.float16 590054400 0.2302412473563562\n",
            "torch.int8 1951137792 0.7613406475643715\n",
            "torch.float32 21573632 0.008418105079272355\n",
            "torch.float16 590054400 0.2302395914203712\n",
            "torch.int8 1951137792 0.7613351718670773\n",
            "torch.float32 21592064 0.008425236712551428\n",
            "torch.float16 590054400 0.23023811949727\n",
            "torch.int8 1951137792 0.7613303046467165\n",
            "torch.float32 21608448 0.00843157585601352\n",
            "torch.float16 590054400 0.2300263588710595\n",
            "torch.int8 1953497088 0.7615498201824402\n",
            "torch.float32 21608448 0.008423820946500234\n",
            "torch.float16 590054400 0.23002470602465333\n",
            "torch.int8 1953497088 0.7615443480926781\n",
            "torch.float32 21626880 0.008430945882668538\n",
            "torch.float16 590054400 0.23002397143387496\n",
            "torch.int8 1953497088 0.7615419160780259\n",
            "torch.float32 21635072 0.008434112488099112\n",
            "torch.float16 590054400 0.2300223186217782\n",
            "torch.int8 1953497088 0.7615364441018521\n",
            "torch.float32 21653504 0.008441237276369685\n",
            "torch.float16 590054400 0.23002158404624823\n",
            "torch.int8 1953497088 0.7615340121376828\n",
            "torch.float32 21661696 0.008444403816068957\n",
            "torch.float16 590054400 0.22981022136520174\n",
            "torch.int8 1955856384 0.7617531342289507\n",
            "torch.float32 21661696 0.008436644405847503\n",
            "torch.float16 590054400 0.22980857162342222\n",
            "torch.int8 1955856384 0.7617476658213067\n",
            "torch.float32 21680128 0.00844376255527111\n",
            "torch.float16 590054400 0.22980783841245647\n",
            "torch.int8 1955856384 0.7617452354431107\n",
            "torch.float32 21688320 0.008446926144432865\n",
            "torch.float16 590054400 0.22980618870488972\n",
            "torch.int8 1955856384 0.7617397671488718\n",
            "torch.float32 21706752 0.008454044146238453\n",
            "torch.float16 590054400 0.2298054555091294\n",
            "torch.int8 1955856384 0.7617373368210774\n",
            "torch.float32 21714944 0.00845720766979322\n",
            "torch.float16 590054400 0.22980380583577437\n",
            "torch.int8 1955856384 0.7617318686402402\n",
            "torch.float32 21733376 0.00846432552398538\n",
            "torch.float16 590054400 0.22980307265521902\n",
            "torch.int8 1955856384 0.7617294383628458\n",
            "torch.float32 21741568 0.008467488981935199\n",
            "torch.float16 590054400 0.22938153666223343\n",
            "torch.int8 1960574976 0.7621665065736336\n",
            "torch.float32 21741568 0.008451956764133003\n",
            "torch.float16 590054400 0.22938007568975244\n",
            "torch.int8 1960574976 0.7621616521973474\n",
            "torch.float32 21757952 0.008458272112900098\n",
            "torch.float16 590054400 0.2293784321179564\n",
            "torch.int8 1960574976 0.7621561910979394\n",
            "torch.float32 21776384 0.008465376784104231\n",
            "torch.float16 590054400 0.2293769711850218\n",
            "torch.int8 1960574976 0.7621513368530543\n",
            "torch.float32 21792768 0.00847169196192396\n",
            "torch.float16 590054400 0.22937532765771457\n",
            "torch.int8 1960574976 0.7621458759014692\n",
            "torch.float32 21811200 0.00847879644081621\n",
            "torch.float16 590054400 0.22749749791245952\n",
            "torch.int8 1981808640 0.7640931191112111\n",
            "torch.float32 21811200 0.008409382976329363\n",
            "torch.float16 590054400 0.22749588120824207\n",
            "torch.int8 1981808640 0.7640876891061363\n",
            "torch.float32 21829632 0.008416429685621597\n",
            "torch.float16 590054400 0.22748941462114708\n",
            "torch.int8 1981808640 0.7640659698575786\n",
            "torch.float32 21903360 0.00844461552127439\n",
            "torch.float16 590054400 0.22748779803181454\n",
            "torch.int8 1981808640 0.7640605402383662\n",
            "torch.float32 21921792 0.00845166172981923\n",
            "torch.float16 590054400 0.22748133190423458\n",
            "torch.int8 1981808640 0.7640388225331762\n",
            "torch.float32 21995520 0.008479845562589195\n",
            "torch.float16 590054400 0.2256342604611714\n",
            "torch.int8 2003042304 0.7659547474529143\n",
            "torch.float32 21995520 0.00841099208591429\n",
            "torch.float16 590054400 0.22563267013049867\n",
            "torch.int8 2003042304 0.765949348798799\n",
            "torch.float32 22013952 0.008417981070702348\n",
            "torch.float16 590054400 0.22562630903198297\n",
            "torch.int8 2003042304 0.7659277549433394\n",
            "torch.float32 22087680 0.00844593602467764\n",
            "torch.float16 590054400 0.22562471881339546\n",
            "torch.int8 2003042304 0.7659223566697169\n",
            "torch.float32 22106112 0.008452924516887641\n",
            "torch.float16 590054400 0.22561835816319697\n",
            "torch.int8 2003042304 0.7659007643361482\n",
            "torch.float32 22179840 0.00848087750065486\n",
            "torch.float16 590054400 0.22380129544334387\n",
            "torch.int8 2024275968 0.7677861295047184\n",
            "torch.float32 22179840 0.008412575051937748\n",
            "torch.float16 590054400 0.2237950371855713\n",
            "torch.int8 2024275968 0.7677646595507438\n",
            "torch.float32 22253568 0.008440303263684907\n",
            "torch.float16 590054400 0.2237934726758161\n",
            "torch.int8 2024275968 0.7677592922498657\n",
            "torch.float32 22272000 0.008447235074318191\n",
            "torch.float16 590054400 0.22378721485553302\n",
            "torch.int8 2024275968 0.7677378237967687\n",
            "torch.float32 22345728 0.00847496134769828\n",
            "torch.float16 590054400 0.22378565045514442\n",
            "torch.int8 2024275968 0.7677324568710904\n",
            "torch.float32 22364160 0.008481892673765203\n",
            "torch.float16 590056704 0.22378632872723078\n",
            "torch.int8 2024275968 0.7677317860106565\n",
            "torch.float32 22364160 0.008481885262112682\n",
            "torch.float16 590059008 0.22378700699813178\n",
            "torch.int8 2024275968 0.7677311151513951\n",
            "torch.float32 22364160 0.008481877850473114\n",
            "torch.float16 590061312 0.2237876852678474\n",
            "torch.int8 2024275968 0.7677304442933061\n",
            "torch.float32 22364160 0.0084818704388465\n",
            "torch.float16 590063616 0.22378836353637763\n",
            "torch.int8 2024275968 0.7677297734363895\n",
            "torch.float32 22364160 0.008481863027232838\n",
            "torch.float16 590065920 0.22378904180372253\n",
            "torch.int8 2024275968 0.7677291025806453\n",
            "torch.float32 22364160 0.00848185561563213\n",
            "Training...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "Gemma2ForCausalLM.forward() got an unexpected keyword argument 'decoder_input_ids'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-45-c9c911cc6968>\u001b[0m in \u001b[0;36m<cell line: 32>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Training...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdo_train\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m   \u001b[0mtrain_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m   \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_result\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m   \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"train\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1930\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1931\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1932\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   1933\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1934\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2266\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2267\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccelerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccumulate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2268\u001b[0;31m                     \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2269\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2270\u001b[0m                 if (\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   3305\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3306\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss_context_manager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3307\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3309\u001b[0m         \u001b[0;32mdel\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mcompute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m   3336\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3337\u001b[0m             \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3338\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3339\u001b[0m         \u001b[0;31m# Save past state if it exists\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3340\u001b[0m         \u001b[0;31m# TODO: this needs to be fixed and made cleaner later.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1553\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1555\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1560\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1561\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1564\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/utils/operations.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    817\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodel_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m     \u001b[0;31m# To act like a decorator so that it can be popped when doing `extract_model_from_parallel`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/utils/operations.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    805\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    806\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 807\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mconvert_to_fp32\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    808\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    809\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getstate__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/amp/autocast_mode.py\u001b[0m in \u001b[0;36mdecorate_autocast\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_autocast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mautocast_instance\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0mdecorate_autocast\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__script_unsupported\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"@autocast() decorator is not supported in script mode\"\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/peft/peft_model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict, task_ids, **kwargs)\u001b[0m\n\u001b[1;32m   1575\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_enable_peft_forward_hooks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1576\u001b[0m                 \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspecial_peft_forward_args\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1577\u001b[0;31m                 return self.base_model(\n\u001b[0m\u001b[1;32m   1578\u001b[0m                     \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1579\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1553\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1555\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1560\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1561\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1564\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/peft/tuners/tuners_utils.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_pre_injection_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mPeftConfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madapter_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/utils/operations.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    817\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodel_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m     \u001b[0;31m# To act like a decorator so that it can be popped when doing `extract_model_from_parallel`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/utils/operations.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    805\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    806\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 807\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mconvert_to_fp32\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    808\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    809\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getstate__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/amp/autocast_mode.py\u001b[0m in \u001b[0;36mdecorate_autocast\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_autocast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mautocast_instance\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0mdecorate_autocast\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__script_unsupported\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"@autocast() decorator is not supported in script mode\"\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/utils/operations.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    817\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodel_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m     \u001b[0;31m# To act like a decorator so that it can be popped when doing `extract_model_from_parallel`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/utils/operations.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    805\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    806\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 807\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mconvert_to_fp32\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    808\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    809\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getstate__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/amp/autocast_mode.py\u001b[0m in \u001b[0;36mdecorate_autocast\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_autocast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mautocast_instance\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0mdecorate_autocast\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__script_unsupported\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"@autocast() decorator is not supported in script mode\"\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/utils/operations.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    817\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodel_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m     \u001b[0;31m# To act like a decorator so that it can be popped when doing `extract_model_from_parallel`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/utils/operations.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    805\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    806\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 807\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mconvert_to_fp32\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    808\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    809\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getstate__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/amp/autocast_mode.py\u001b[0m in \u001b[0;36mdecorate_autocast\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_autocast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mautocast_instance\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0mdecorate_autocast\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__script_unsupported\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"@autocast() decorator is not supported in script mode\"\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/peft/peft_model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict, task_ids, **kwargs)\u001b[0m\n\u001b[1;32m   1575\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_enable_peft_forward_hooks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1576\u001b[0m                 \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspecial_peft_forward_args\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1577\u001b[0;31m                 return self.base_model(\n\u001b[0m\u001b[1;32m   1578\u001b[0m                     \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1579\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1553\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1555\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1560\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1561\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1564\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/peft/tuners/tuners_utils.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_pre_injection_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mPeftConfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madapter_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/peft/peft_model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, inputs_embeds, decoder_input_ids, decoder_attention_mask, decoder_inputs_embeds, labels, output_attentions, output_hidden_states, return_dict, task_ids, **kwargs)\u001b[0m\n\u001b[1;32m   1783\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_enable_peft_forward_hooks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspecial_peft_forward_args\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1785\u001b[0;31m                 return self.base_model(\n\u001b[0m\u001b[1;32m   1786\u001b[0m                     \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1787\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1553\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1555\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1560\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1561\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1564\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/peft/tuners/tuners_utils.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_pre_injection_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mPeftConfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madapter_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py\u001b[0m in \u001b[0;36mnew_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    167\u001b[0m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_old_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_old_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hf_hook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Gemma2ForCausalLM.forward() got an unexpected keyword argument 'decoder_input_ids'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iG_t1SQgGvrR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}